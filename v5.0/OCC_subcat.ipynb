{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cf08e90-f3f8-4fe9-a27e-820859a602fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== LOADING ALL DATASETS ===\n",
      "Loading client... ✓ (45,688 rows, 49 columns)\n",
      "Loading emfc2fna... ✓ (51,772 rows, 31 columns)\n",
      "Loading emfc2personalinformation... ✓ (51,837 rows, 37 columns)\n",
      "Loading emfc2... ✓ (51,769 rows, 8 columns)\n",
      "Loading EMFC2Assets... ✓ (50,500 rows, 39 columns)\n",
      "Loading emfc2portofolioinsurance... ✓ (27,437 rows, 25 columns)\n",
      "Loading emfc2productsolution... ✓ (61,733 rows, 25 columns)\n",
      "Loading emfc2productsolutiondetail... ✓ (154,286 rows, 5 columns)\n",
      "Loading EMFC2ProductIntegrationApplication... ✓ (560 rows, 14 columns)\n",
      "Loading EMFC2ProductIntegrationLog... ✓ (977 rows, 21 columns)\n",
      "Loading ProductMainPlan... ✓ (1,532 rows, 22 columns)\n",
      "Loading ProductType... ✓ (4 rows, 8 columns)\n",
      "Loading ProductCategory... ✓ (10 rows, 6 columns)\n",
      "Loading productsubcategory... ✓ (39 rows, 13 columns)\n",
      "\n",
      "Successfully loaded 14 datasets\n",
      "Available datasets: ['client', 'emfc2fna', 'emfc2personalinformation', 'emfc2', 'EMFC2Assets', 'emfc2portofolioinsurance', 'emfc2productsolution', 'emfc2productsolutiondetail', 'EMFC2ProductIntegrationApplication', 'EMFC2ProductIntegrationLog', 'ProductMainPlan', 'ProductType', 'ProductCategory', 'productsubcategory']\n"
     ]
    }
   ],
   "source": [
    "# Data gathering\n",
    "import pandas as pd\n",
    "import msoffcrypto\n",
    "import io\n",
    "\n",
    "def load_encrypted_excel(file_path: str, password: str) -> pd.DataFrame:\n",
    "    with open(file_path, 'rb') as f:\n",
    "        office_file = msoffcrypto.OfficeFile(f)\n",
    "        office_file.load_key(password=password)\n",
    "        decrypted = io.BytesIO()\n",
    "        office_file.decrypt(decrypted)\n",
    "        decrypted.seek(0)\n",
    "        return pd.read_excel(decrypted)\n",
    "\n",
    "# File configurations\n",
    "files = [\n",
    "    # Core Client & FNA Process Tables\n",
    "    {\"name\": \"client\", \"path\": \"client.xlsx\", \"password\": \"_XlN@a9)EVy1\"},\n",
    "    {\"name\": \"emfc2fna\", \"path\": \"emfc2fna.xlsx\", \"password\": \"dQq9T%pC^?22\"},\n",
    "    {\"name\": \"emfc2personalinformation\", \"path\": \"emfc2personalinformation.xlsx\", \"password\": \"ZqYmaFgC@Zv3\"},\n",
    "    {\"name\": \"emfc2\", \"path\": \"emfc2.xlsx\", \"password\": \"79GYEd%l(2Bf\"},\n",
    "    {\"name\": \"EMFC2Assets\", \"path\": \"EMFC2Assets.xlsx\", \"password\": \"!suNZ=%YA13k\"},\n",
    "    {\"name\": \"emfc2portofolioinsurance\", \"path\": \"emfc2portofolioinsurance.xlsx\", \"password\": \"BcxM>wz*(hxF\"},\n",
    "\n",
    "    # Product & Solution Workflow\n",
    "    {\"name\": \"emfc2productsolution\", \"path\": \"emfc2productsolution.xlsx\", \"password\": \"@OFn7oA5!Joe\"},\n",
    "    {\"name\": \"EMFC2ProductIntegrationApplication\", \"path\": \"EMFC2ProductIntegrationApplication.xlsx\", \"password\": \"(FZsw7#vz-bN\"},\n",
    "    {\"name\": \"EMFC2ProductIntegrationLog\", \"path\": \"EMFC2ProductIntegrationLog.xlsx\", \"password\": \"?wcAx*P4n=9&\"},\n",
    "\n",
    "    # Product & Category Lookup Tables\n",
    "    {\"name\": \"ProductMainPlan\", \"path\": \"ProductMainPlan.xlsx\", \"password\": \")XQ4ZDssowrA\"},\n",
    "    {\"name\": \"ProductType\", \"path\": \"ProductType.xlsx\", \"password\": \"#9zCw?^-xTO?\"},\n",
    "    {\"name\": \"ProductCategory\", \"path\": \"ProductCategory.xlsx\", \"password\": \"#F)cdAEOVJ@4\"},\n",
    "    {\"name\": \"productsubcategory\", \"path\": \"productsubcategory.xlsx\", \"password\": \"y-^t$N9>%S%C\"}\n",
    "]\n",
    "\n",
    "# Load all datasets into memory\n",
    "datasets = {}\n",
    "\n",
    "print(\"=== LOADING ALL DATASETS ===\")\n",
    "for file in files:\n",
    "    print(f\"Loading {file['name']}...\", end=\" \")\n",
    "    try:\n",
    "        datasets[file['name']] = load_encrypted_excel(file[\"path\"], file[\"password\"])\n",
    "        shape = datasets[file['name']].shape\n",
    "        print(f\"✓ ({shape[0]:,} rows, {shape[1]} columns)\")\n",
    "    except Exception as e:\n",
    "        print(f\"✗ Error: {e}\")\n",
    "\n",
    "print(f\"\\nSuccessfully loaded {len(datasets)} datasets\")\n",
    "print(\"Available datasets:\", list(datasets.keys()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7b7afcee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\n=== COLUMN HEADERS FOR EACH DATASET ===\\n\n",
      "📄 Dataset: client\n",
      "🧾 Columns (49):\n",
      "  - # (int64)\n",
      "  - ClientId (object)\n",
      "  - ClientName (object)\n",
      "  - ClientMobileNumber (object)\n",
      "  - ClientMNVerified (bool)\n",
      "  - ClientMNVeriCode (float64)\n",
      "  - ClientMNVeriCodeTime (datetime64[ns])\n",
      "  - ClientEmail (object)\n",
      "  - ClientContactPreferences (object)\n",
      "  - ClientGender (object)\n",
      "  - ClientDOB (datetime64[ns])\n",
      "  - ClientCPFContributionCategoryId (object)\n",
      "  - IDNumber (object)\n",
      "  - Nationality (object)\n",
      "  - SpokenLanguage (object)\n",
      "  - WrittenLanguage (object)\n",
      "  - Education (object)\n",
      "  - EmploymentStatus (object)\n",
      "  - Occupation (object)\n",
      "  - MaritalStatus (object)\n",
      "  - PrimaryAddress (object)\n",
      "  - CorrespondingAddress (object)\n",
      "  - IncomeRange (object)\n",
      "  - AccompaniedbyTrustedIndividual (float64)\n",
      "  - ClientInvitedDate (datetime64[ns])\n",
      "  - ClientStatus (object)\n",
      "  - RiskProfile (object)\n",
      "  - RiskProfileSubmissionDate (datetime64[ns])\n",
      "  - CKAProfile (object)\n",
      "  - CARProfile (object)\n",
      "  - CKACARSubmissionDate (datetime64[ns])\n",
      "  - UserId (object)\n",
      "  - ClientEmailVerified (bool)\n",
      "  - ClientEmailVeriCode (float64)\n",
      "  - ClientEmailVeriCodeTime (datetime64[ns])\n",
      "  - ClientResidentialStatus (object)\n",
      "  - CountryOfBirth (object)\n",
      "  - Race (object)\n",
      "  - UEN (object)\n",
      "  - ClientSource (object)\n",
      "  - LastModifiedTime (datetime64[ns])\n",
      "  - AccreditedInvestor (float64)\n",
      "  - ClientType (object)\n",
      "  - ManuallyRetrieved (bool)\n",
      "  - MyInfoRetrieved (bool)\n",
      "  - NextReviewDate (datetime64[ns])\n",
      "  - AITransactionCapability (bool)\n",
      "  - TempContactPreferences (object)\n",
      "  - SelectedClient (float64)\n",
      "----------------------------------------\n",
      "📄 Dataset: emfc2fna\n",
      "🧾 Columns (31):\n",
      "  - # (int64)\n",
      "  - EMFC2FNAId (object)\n",
      "  - EMFC2Id (object)\n",
      "  - TradeType (object)\n",
      "  - CoBroke (bool)\n",
      "  - CoBrokeUserId (object)\n",
      "  - CoBrokeSFACode (object)\n",
      "  - JointApplicant (bool)\n",
      "  - JointApplicantClientId (object)\n",
      "  - StartTime (datetime64[ns])\n",
      "  - SubmissionTime (datetime64[ns])\n",
      "  - FNAStatus (object)\n",
      "  - NonFaceToFace (bool)\n",
      "  - SignRemotely (bool)\n",
      "  - NFTFCommunicationMode (object)\n",
      "  - NFTFCommunicationPlatform (object)\n",
      "  - UserId (object)\n",
      "  - RenewalFNA (bool)\n",
      "  - MaterialChanges (bool)\n",
      "  - FundSwitch (bool)\n",
      "  - NonResident (float64)\n",
      "  - RequirementNeeded (float64)\n",
      "  - RequirementCompleted (float64)\n",
      "  - FNAIdentifier (int64)\n",
      "  - RequirementResolvementTime (datetime64[ns])\n",
      "  - HasIntroducer (float64)\n",
      "  - AITransaction (float64)\n",
      "  - SelfSubmission (float64)\n",
      "  - AdviserId (object)\n",
      "  - ManagerAdviserId (object)\n",
      "  - DirectorAdviserId (object)\n",
      "----------------------------------------\n",
      "📄 Dataset: emfc2personalinformation\n",
      "🧾 Columns (37):\n",
      "  - # (int64)\n",
      "  - PersonalInformationId (object)\n",
      "  - EMFC2FNAId (object)\n",
      "  - ClientId (object)\n",
      "  - ClientName (object)\n",
      "  - ClientMobileNumber (object)\n",
      "  - ClientEmail (object)\n",
      "  - ClientContactPreferences (object)\n",
      "  - ClientGender (object)\n",
      "  - ClientDOB (datetime64[ns])\n",
      "  - ClientAge (float64)\n",
      "  - ClientCPFContributionCategoryId (object)\n",
      "  - IDNumber (object)\n",
      "  - Nationality (object)\n",
      "  - SpokenLanguage (object)\n",
      "  - WrittenLanguage (object)\n",
      "  - Education (object)\n",
      "  - EmploymentStatus (object)\n",
      "  - Occupation (object)\n",
      "  - MaritalStatus (object)\n",
      "  - PrimaryAddress (object)\n",
      "  - CorrespondingAddress (object)\n",
      "  - IncomeRange (object)\n",
      "  - AccompaniedbyTrustedIndividual (bool)\n",
      "  - RiskProfile (object)\n",
      "  - RiskProfileSubmissionDate (datetime64[ns])\n",
      "  - CKAProfile (object)\n",
      "  - CARProfile (object)\n",
      "  - CKACARSubmissionDate (datetime64[ns])\n",
      "  - ClientStatus (object)\n",
      "  - ClientResidentialStatus (object)\n",
      "  - CountryOfBirth (object)\n",
      "  - Race (object)\n",
      "  - ClientRetrieval (bool)\n",
      "  - ManuallyRetrieved (bool)\n",
      "  - MyInfoRetrieved (bool)\n",
      "  - SelectedClient (bool)\n",
      "----------------------------------------\n",
      "📄 Dataset: emfc2\n",
      "🧾 Columns (8):\n",
      "  - # (int64)\n",
      "  - EMFC2Id (object)\n",
      "  - ClientId (object)\n",
      "  - ClientAge (float64)\n",
      "  - EMFCStartDate (datetime64[ns])\n",
      "  - EMFCSubmitDate (datetime64[ns])\n",
      "  - EMFCStatus (object)\n",
      "  - UserId (object)\n",
      "----------------------------------------\n",
      "📄 Dataset: EMFC2Assets\n",
      "🧾 Columns (39):\n",
      "  - # (int64)\n",
      "  - EMFC2AssetsId (int64)\n",
      "  - EMFC2Id (object)\n",
      "  - AssetsDisclose (bool)\n",
      "  - ReasonAssetsDisclose (object)\n",
      "  - SavingsAccounts (int64)\n",
      "  - FixedDepositsAccount (float64)\n",
      "  - OtherCashAsset (float64)\n",
      "  - HomeAsset (float64)\n",
      "  - MotorAsset (float64)\n",
      "  - InsuranceCashValues (float64)\n",
      "  - OtherUseAsset (float64)\n",
      "  - StocksPortofolio (float64)\n",
      "  - BondPortofolio (float64)\n",
      "  - UTFEquityAsset (float64)\n",
      "  - UTFFixedIncomeAsset (float64)\n",
      "  - UTFMoneyAsset (float64)\n",
      "  - UTFOther (float64)\n",
      "  - ETFs (float64)\n",
      "  - NetBusinessInterests (float64)\n",
      "  - InvestmentProperties (float64)\n",
      "  - OtherInvestedAsset (float64)\n",
      "  - CPFOABalance (float64)\n",
      "  - CPFSABalance (float64)\n",
      "  - CPFMABalance (float64)\n",
      "  - CPFISOAEquityAsset (float64)\n",
      "  - CPFISOAFixedIncomeAsset (float64)\n",
      "  - CPFISOACashAsset (float64)\n",
      "  - CPFISOAOthersAsset (float64)\n",
      "  - CPFISSAEquityAsset (float64)\n",
      "  - CPFISSAFixedIncomeAsset (float64)\n",
      "  - CPFISSACashAsset (float64)\n",
      "  - CPFISSAOthersAsset (float64)\n",
      "  - SRSEquityAsset (float64)\n",
      "  - SRSFixedIncomeAsset (float64)\n",
      "  - SRSCashAsset (float64)\n",
      "  - SRSOther (float64)\n",
      "  - Completion (bool)\n",
      "  - CPFRABalance (float64)\n",
      "----------------------------------------\n",
      "📄 Dataset: emfc2portofolioinsurance\n",
      "🧾 Columns (25):\n",
      "  - # (int64)\n",
      "  - EMFC2PortofolioInsuranceId (int64)\n",
      "  - EMFC2Id (object)\n",
      "  - InsurerCompanyName (object)\n",
      "  - PolicyNumber (object)\n",
      "  - PolicyOwner (object)\n",
      "  - PlanType (object)\n",
      "  - PlanName (object)\n",
      "  - MainPurposeofPlan (object)\n",
      "  - SumAssuredforLossofLife (float64)\n",
      "  - SumAssuredforTPD (float64)\n",
      "  - SumAssuredforCI (float64)\n",
      "  - SumAssuredforEarlyCI (int64)\n",
      "  - SumAssuredforAccidentalDeath (float64)\n",
      "  - SumAssuredforHospitalIncome (int64)\n",
      "  - SumAssuredforDisabilityIncome (int64)\n",
      "  - SumAssuredforLongTermCare (int64)\n",
      "  - SumAssuredforHnS (int64)\n",
      "  - AnnualCashPremium (float64)\n",
      "  - BeneficiaryInformation (object)\n",
      "  - Remarks (object)\n",
      "  - AnnualCPFPremium (float64)\n",
      "  - CoverageStartAge (float64)\n",
      "  - CoverageEndAge (float64)\n",
      "  - PaymentEndAge (float64)\n",
      "----------------------------------------\n",
      "📄 Dataset: emfc2productsolution\n",
      "🧾 Columns (25):\n",
      "  - # (int64)\n",
      "  - EMFC2ProductSolutionId (object)\n",
      "  - EMFC2FNAId (object)\n",
      "  - PersonalInformationId (object)\n",
      "  - ProductId (object)\n",
      "  - SubProductId (object)\n",
      "  - ActivityType (object)\n",
      "  - Replacement (bool)\n",
      "  - ReasonforReplacement (object)\n",
      "  - RecommendedProductRejected (bool)\n",
      "  - ReasonforRejected (object)\n",
      "  - InvestmentGoalId (object)\n",
      "  - InvestmentHorizonId (object)\n",
      "  - InvestmentRiskId (object)\n",
      "  - ReasonBudgetLessThanPremium (object)\n",
      "  - ThirdParty (bool)\n",
      "  - DateCreated (datetime64[ns])\n",
      "  - AIProduct (bool)\n",
      "  - FNAProviderSubmissionTypeId (object)\n",
      "  - Status (object)\n",
      "  - StatusRemarks (object)\n",
      "  - StatusTimeStamp (datetime64[ns])\n",
      "  - CurrencyCode (object)\n",
      "  - CurrencyToSGDExchangeRate (float64)\n",
      "  - TargetedPremium (float64)\n",
      "----------------------------------------\n",
      "📄 Dataset: emfc2productsolutiondetail\n",
      "🧾 Columns (5):\n",
      "  - # (int64)\n",
      "  - EMFC2ProductSolutionDetailId (int64)\n",
      "  - EMFC2ProductSolutionId (object)\n",
      "  - ProductComponentCode (object)\n",
      "  - ProductComponentValue (object)\n",
      "----------------------------------------\n",
      "📄 Dataset: EMFC2ProductIntegrationApplication\n",
      "🧾 Columns (14):\n",
      "  - # (int64)\n",
      "  - integrationapplicationId (int64)\n",
      "  - applicationId (object)\n",
      "  - efnaTransNo (object)\n",
      "  - quotationId (object)\n",
      "  - solutionId (object)\n",
      "  - efnaQuotationId (object)\n",
      "  - module (object)\n",
      "  - caseId (object)\n",
      "  - BISignDate (object)\n",
      "  - AppFormSignDate (object)\n",
      "  - eReferenceNumber (object)\n",
      "  - userId (object)\n",
      "  - isDone (bool)\n",
      "----------------------------------------\n",
      "📄 Dataset: EMFC2ProductIntegrationLog\n",
      "🧾 Columns (21):\n",
      "  - # (int64)\n",
      "  - integrationLogsId (int64)\n",
      "  - EMFC2ProductSolutionId (object)\n",
      "  - efnaTransNo (object)\n",
      "  - efnaQuotationId (object)\n",
      "  - ProviderId (object)\n",
      "  - ProductId (object)\n",
      "  - ProductName (object)\n",
      "  - jsonResponse (object)\n",
      "  - userId (object)\n",
      "  - TransactionDate (datetime64[ns])\n",
      "  - Remarks (object)\n",
      "  - isDone (bool)\n",
      "  - givenName (object)\n",
      "  - familyName (object)\n",
      "  - needs (object)\n",
      "  - producttype (object)\n",
      "  - personalinfoid (float64)\n",
      "  - ClientDOB (object)\n",
      "  - jsonRequest (object)\n",
      "  - AgentCode (float64)\n",
      "----------------------------------------\n",
      "📄 Dataset: ProductMainPlan\n",
      "🧾 Columns (22):\n",
      "  - # (int64)\n",
      "  - ProductId (object)\n",
      "  - ProviderId (object)\n",
      "  - ProductSubCategoryId (object)\n",
      "  - MainPlan (object)\n",
      "  - DateInclusion (datetime64[ns])\n",
      "  - DateCeased (datetime64[ns])\n",
      "  - Benefit (object)\n",
      "  - Limitation (object)\n",
      "  - RiskProfile (bool)\n",
      "  - CKA (bool)\n",
      "  - CAR (bool)\n",
      "  - Star (bool)\n",
      "  - Objective (bool)\n",
      "  - Horizon (bool)\n",
      "  - Fund (bool)\n",
      "  - Rider (bool)\n",
      "  - Status (bool)\n",
      "  - UserId (object)\n",
      "  - CreatedDate (datetime64[ns])\n",
      "  - ProductCode (object)\n",
      "  - UlRequirement (bool)\n",
      "----------------------------------------\n",
      "📄 Dataset: ProductType\n",
      "🧾 Columns (8):\n",
      "  - # (int64)\n",
      "  - ProductTypeId (object)\n",
      "  - TypeName (object)\n",
      "  - TypeIcon (object)\n",
      "  - InvestmentType (bool)\n",
      "  - MFC (bool)\n",
      "  - SIMApp (bool)\n",
      "  - Active (bool)\n",
      "----------------------------------------\n",
      "📄 Dataset: ProductCategory\n",
      "🧾 Columns (6):\n",
      "  - # (int64)\n",
      "  - ProductCategoryId (object)\n",
      "  - CategoryName (object)\n",
      "  - ProductTypeId (object)\n",
      "  - CategoryIcon (object)\n",
      "  - Active (bool)\n",
      "----------------------------------------\n",
      "📄 Dataset: productsubcategory\n",
      "🧾 Columns (13):\n",
      "  - # (int64)\n",
      "  - ProductSubCategoryId (object)\n",
      "  - ProductCategoryId (object)\n",
      "  - SubCategoryName (object)\n",
      "  - SubCategoryShortName (object)\n",
      "  - SubCategoryIcon (object)\n",
      "  - IncludeFund (bool)\n",
      "  - IncludeRider (bool)\n",
      "  - IncludeStar (bool)\n",
      "  - IncludeObjective (bool)\n",
      "  - IncludeHorizon (bool)\n",
      "  - IncludeRiskProfile (bool)\n",
      "  - Active (bool)\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "print(\"\\\\n=== COLUMN HEADERS FOR EACH DATASET ===\\\\n\")\n",
    "for name, df in datasets.items():\n",
    "    print(f\"📄 Dataset: {name}\")\n",
    "    print(f\"🧾 Columns ({len(df.columns)}):\")\n",
    "    for col in df.columns:\n",
    "        print(f\"  - {col} ({df[col].dtype})\")\n",
    "    print(\"-\" * 40)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e460d3fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== ENHANCED UNIFIED DATASET CREATION ===\n",
      "=== LOADING DATASETS ===\n",
      "Loading client... ✓ (45,688 rows, 49 columns)\n",
      "Loading emfc2personalinformation... ✓ (52,305 rows, 37 columns)\n",
      "Loading emfc2... ✓ (51,769 rows, 8 columns)\n",
      "Loading EMFC2Assets... ✓ (50,500 rows, 39 columns)\n",
      "Loading emfc2portofolioinsurance... ✓ (27,437 rows, 25 columns)\n",
      "Loading emfc2fna... ✓ (51,772 rows, 31 columns)\n",
      "Loading emfc2productsolution... ✓ (43,501 rows, 25 columns)\n",
      "\n",
      "🚀 STARTING ENHANCED DATA UNIFICATION...\n",
      "\n",
      "🚀 Starting enhanced dataset creation...\n",
      "\n",
      "⏰ Creating temporal features...\n",
      "   ✅ Created temporal features\n",
      "\n",
      "🎯 Creating behavioral features...\n",
      "   ✅ Created behavioral features\n",
      "\n",
      "💼 Creating business-driven features...\n",
      "   ✅ Created business-driven features\n",
      "\n",
      "🔗 Creating interaction features...\n",
      "   ✅ Created interaction features\n",
      "\n",
      "📊 Creating portfolio sequence features...\n",
      "   ✅ Created portfolio sequence features\n",
      "\n",
      "🔧 Creating original derived features...\n",
      "   ✅ Created original derived features\n",
      "\n",
      "✅ ENHANCED UNIFICATION COMPLETE!\n",
      "   Final records: 65,010\n",
      "   Final columns: 130\n",
      "   Processing time: 2110.4 seconds\n",
      "\n",
      "💾 Saved: enhanced_unified_client_dataset.xlsx\n",
      "\n",
      "📊 ENHANCEMENT SUMMARY:\n",
      "✨ New Enhanced Features Added: 17\n",
      "    1. Client_Tenure_Years: 100.0% coverage\n",
      "    2. Days_Since_Last_FNA: 79.9% coverage\n",
      "    3. FNA_Frequency: 79.9% coverage\n",
      "    4. Years_With_Insurance: 10.5% coverage\n",
      "    5. Engagement_Score: 100.0% coverage\n",
      "    7. Digital_Adoption_Score: 100.0% coverage\n",
      "    8. Protection_Gap_Score: 100.0% coverage\n",
      "   13. Family_Protection_Priority: 100.0% coverage\n",
      "   16. Insurance_Evolution_Stage: 100.0% coverage\n",
      "   17. Product_Diversity_Score: 100.0% coverage\n",
      "\n",
      "🎉 ENHANCED PHASE 1 COMPLETE!\n",
      "✅ Total features: 130\n",
      "✅ Enhanced features provide better signals for ML modeling\n",
      "✅ Ready for Phase 2: Target Variable Engineering\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "=== ENHANCED PHASE 1: DATA UNIFICATION WITH ADVANCED FEATURE ENGINEERING ===\n",
    "Enhanced version with temporal features, behavioral scoring, and improved portfolio analysis\n",
    "\"\"\"\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "import msoffcrypto\n",
    "import io\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"=== ENHANCED UNIFIED DATASET CREATION ===\")\n",
    "\n",
    "def load_encrypted_excel(file_path: str, password: str) -> pd.DataFrame:\n",
    "    \"\"\"Load password-protected Excel files\"\"\"\n",
    "    with open(file_path, 'rb') as f:\n",
    "        office_file = msoffcrypto.OfficeFile(f)\n",
    "        office_file.load_key(password=password)\n",
    "        decrypted = io.BytesIO()\n",
    "        office_file.decrypt(decrypted)\n",
    "        decrypted.seek(0)\n",
    "        return pd.read_excel(decrypted)\n",
    "\n",
    "# Load all datasets\n",
    "files = [\n",
    "    {\"name\": \"client\", \"path\": \"client.xlsx\", \"password\": \"_XlN@a9)EVy1\"},\n",
    "    {\"name\": \"emfc2personalinformation\", \"path\": \"emfc2personalinformation.xlsx\", \"password\": \"ZqYmaFgC@Zv3\"},\n",
    "    {\"name\": \"emfc2\", \"path\": \"emfc2.xlsx\", \"password\": \"79GYEd%l(2Bf\"},\n",
    "    {\"name\": \"EMFC2Assets\", \"path\": \"EMFC2Assets.xlsx\", \"password\": \"!suNZ=%YA13k\"},\n",
    "    {\"name\": \"emfc2portofolioinsurance\", \"path\": \"emfc2portofolioinsurance.xlsx\", \"password\": \"BcxM>wz*(hxF\"},\n",
    "    {\"name\": \"emfc2fna\", \"path\": \"emfc2fna.xlsx\", \"password\": \"dQq9T%pC^?22\"},  # Added FNA table\n",
    "    {\"name\": \"emfc2productsolution\", \"path\": \"emfc2productsolution.xlsx\", \"password\": \"@OFn7oA5!Joe\"}  # Added for temporal features\n",
    "]\n",
    "\n",
    "datasets = {}\n",
    "print(\"=== LOADING DATASETS ===\")\n",
    "for file in files:\n",
    "    print(f\"Loading {file['name']}...\", end=\" \")\n",
    "    try:\n",
    "        datasets[file['name']] = load_encrypted_excel(file[\"path\"], file[\"password\"])\n",
    "        shape = datasets[file['name']].shape\n",
    "        print(f\"✓ ({shape[0]:,} rows, {shape[1]} columns)\")\n",
    "    except Exception as e:\n",
    "        print(f\"✗ Error: {e}\")\n",
    "\n",
    "def create_enhanced_unified_dataset(datasets):\n",
    "    \"\"\"Create unified dataset with enhanced feature engineering\"\"\"\n",
    "    \n",
    "    print(\"\\n🚀 Starting enhanced dataset creation...\")\n",
    "    start_time = datetime.now()\n",
    "    \n",
    "    # Get base datasets\n",
    "    client_df = datasets['client'].copy()\n",
    "    personal_df = datasets['emfc2personalinformation'].copy()\n",
    "    emfc2_df = datasets['emfc2'].copy()\n",
    "    assets_df = datasets['EMFC2Assets'].copy()\n",
    "    portfolio_df = datasets['emfc2portofolioinsurance'].copy()\n",
    "    \n",
    "    # Start with client as base\n",
    "    unified_df = client_df.copy()\n",
    "    \n",
    "    # Add unique columns from personal info\n",
    "    personal_unique_cols = ['PersonalInformationId', 'EMFC2FNAId', 'ClientAge', 'ClientRetrieval']\n",
    "    personal_merge_cols = ['ClientId'] + personal_unique_cols\n",
    "    available_personal_cols = [col for col in personal_merge_cols if col in personal_df.columns]\n",
    "    \n",
    "    unified_df = unified_df.merge(\n",
    "        personal_df[available_personal_cols],\n",
    "        on='ClientId',\n",
    "        how='left',\n",
    "        suffixes=('', '_personal')\n",
    "    )\n",
    "    \n",
    "    # Smart replacement for overlapping columns\n",
    "    overlap_cols = [\n",
    "        'ClientGender', 'IncomeRange', 'MaritalStatus', 'Education', \n",
    "        'EmploymentStatus', 'RiskProfile', 'CKAProfile', 'CARProfile',\n",
    "        'Nationality', 'SpokenLanguage', 'WrittenLanguage'\n",
    "    ]\n",
    "    \n",
    "    for col in overlap_cols:\n",
    "        if col in personal_df.columns and col in unified_df.columns:\n",
    "            personal_updates = personal_df[['ClientId', col]].dropna()\n",
    "            \n",
    "            if len(personal_updates) > 0:\n",
    "                for _, row in personal_updates.iterrows():\n",
    "                    client_id = row['ClientId']\n",
    "                    new_value = row[col]\n",
    "                    \n",
    "                    mask = unified_df['ClientId'] == client_id\n",
    "                    if mask.any():\n",
    "                        null_mask = mask & unified_df[col].isnull()\n",
    "                        if null_mask.any():\n",
    "                            unified_df.loc[null_mask, col] = new_value\n",
    "    \n",
    "    # Add EMFC2Id bridge with temporal features\n",
    "    emfc2_bridge = emfc2_df.sort_values(['ClientId', 'EMFCSubmitDate'], na_position='last')\n",
    "    emfc2_bridge = emfc2_bridge.groupby('ClientId').agg({\n",
    "        'EMFC2Id': 'last',\n",
    "        'EMFCStartDate': ['last', 'count'],  # Added count for frequency\n",
    "        'EMFCSubmitDate': ['last', 'first'],  # Added first for duration\n",
    "        'EMFCStatus': 'last'\n",
    "    }).reset_index()\n",
    "    \n",
    "    # Flatten column names\n",
    "    emfc2_bridge.columns = ['ClientId', 'EMFC2Id', 'EMFCStartDate', 'EMFC_Count', \n",
    "                           'EMFCSubmitDate', 'First_EMFC_Date', 'EMFCStatus']\n",
    "    \n",
    "    unified_df = unified_df.merge(emfc2_bridge, on='ClientId', how='left')\n",
    "    \n",
    "    # Add financial assets\n",
    "    asset_cols = [\n",
    "        'EMFC2Id', 'SavingsAccounts', 'FixedDepositsAccount', 'HomeAsset', \n",
    "        'MotorAsset', 'InsuranceCashValues', 'StocksPortofolio', 'BondPortofolio',\n",
    "        'UTFEquityAsset', 'ETFs', 'InvestmentProperties', 'CPFOABalance', \n",
    "        'CPFSABalance', 'CPFMABalance', 'SRSEquityAsset'\n",
    "    ]\n",
    "    \n",
    "    available_asset_cols = [col for col in asset_cols if col in assets_df.columns]\n",
    "    unified_df = unified_df.merge(assets_df[available_asset_cols], on='EMFC2Id', how='left')\n",
    "    \n",
    "    # Enhanced portfolio analysis\n",
    "    portfolio_analysis = portfolio_df.groupby('EMFC2Id').agg({\n",
    "        'PolicyNumber': 'count',\n",
    "        'PlanType': lambda x: list(x.dropna().unique()),\n",
    "        'SumAssuredforLossofLife': 'sum',\n",
    "        'SumAssuredforTPD': 'sum',\n",
    "        'SumAssuredforCI': 'sum',\n",
    "        'SumAssuredforHospitalIncome': 'sum',\n",
    "        'SumAssuredforLongTermCare': 'sum',\n",
    "        'AnnualCashPremium': 'sum',\n",
    "        'InsurerCompanyName': lambda x: list(x.dropna().unique()),\n",
    "        'CoverageStartAge': ['min', 'mean'],  # Added for temporal analysis\n",
    "        'PaymentEndAge': 'max'  # Added for premium duration\n",
    "    }).reset_index()\n",
    "    \n",
    "    # Flatten and rename columns\n",
    "    portfolio_analysis.columns = [\n",
    "        'EMFC2Id', 'Total_Policies', 'Plan_Types', 'Total_Life_Coverage',\n",
    "        'Total_TPD_Coverage', 'Total_CI_Coverage', 'Total_Hospital_Income',\n",
    "        'Total_LTC_Coverage', 'Total_Annual_Premium', 'Insurance_Companies',\n",
    "        'First_Policy_Age', 'Avg_Policy_Start_Age', 'Max_Payment_End_Age'\n",
    "    ]\n",
    "    \n",
    "    unified_df = unified_df.merge(portfolio_analysis, on='EMFC2Id', how='left')\n",
    "    \n",
    "    # Create all enhanced features\n",
    "    unified_df = create_temporal_features(unified_df)\n",
    "    unified_df = create_behavioral_features(unified_df)\n",
    "    unified_df = create_business_driven_features(unified_df)\n",
    "    unified_df = create_interaction_features(unified_df)\n",
    "    unified_df = create_portfolio_sequence_features(unified_df, portfolio_df)\n",
    "    \n",
    "    # Original derived features (kept for compatibility)\n",
    "    unified_df = create_original_derived_features(unified_df)\n",
    "    \n",
    "    end_time = datetime.now()\n",
    "    processing_time = (end_time - start_time).total_seconds()\n",
    "    \n",
    "    print(f\"\\n✅ ENHANCED UNIFICATION COMPLETE!\")\n",
    "    print(f\"   Final records: {len(unified_df):,}\")\n",
    "    print(f\"   Final columns: {len(unified_df.columns)}\")\n",
    "    print(f\"   Processing time: {processing_time:.1f} seconds\")\n",
    "    \n",
    "    return unified_df\n",
    "\n",
    "def create_temporal_features(df):\n",
    "    \"\"\"Create time-based features\"\"\"\n",
    "    print(\"\\n⏰ Creating temporal features...\")\n",
    "    \n",
    "    # Current date for calculations\n",
    "    current_date = pd.Timestamp.now()\n",
    "    \n",
    "    # Client relationship duration\n",
    "    if 'ClientInvitedDate' in df.columns:\n",
    "        df['Client_Tenure_Days'] = (current_date - pd.to_datetime(df['ClientInvitedDate'])).dt.days\n",
    "        df['Client_Tenure_Years'] = df['Client_Tenure_Days'] / 365.25\n",
    "    \n",
    "    # Time since last FNA\n",
    "    if 'EMFCSubmitDate' in df.columns:\n",
    "        df['Days_Since_Last_FNA'] = (current_date - pd.to_datetime(df['EMFCSubmitDate'])).dt.days\n",
    "        df['Months_Since_Last_FNA'] = df['Days_Since_Last_FNA'] / 30.44\n",
    "    \n",
    "    # Client engagement frequency\n",
    "    if 'EMFC_Count' in df.columns and 'Client_Tenure_Days' in df.columns:\n",
    "        df['FNA_Frequency'] = df['EMFC_Count'] / (df['Client_Tenure_Days'] / 365.25 + 1)\n",
    "    \n",
    "    # Insurance holding duration\n",
    "    if 'First_Policy_Age' in df.columns and 'ClientAge' in df.columns:\n",
    "        df['Years_With_Insurance'] = df['ClientAge'] - df['First_Policy_Age']\n",
    "        df['Years_With_Insurance'] = df['Years_With_Insurance'].clip(lower=0)\n",
    "    \n",
    "    print(\"   ✅ Created temporal features\")\n",
    "    return df\n",
    "\n",
    "def create_behavioral_features(df):\n",
    "    \"\"\"Create behavioral and engagement features\"\"\"\n",
    "    print(\"\\n🎯 Creating behavioral features...\")\n",
    "    \n",
    "    # Client engagement score\n",
    "    df['Engagement_Score'] = 0\n",
    "    if 'ClientEmailVerified' in df.columns:\n",
    "        df['Engagement_Score'] += df['ClientEmailVerified'].astype(float) * 0.25\n",
    "    if 'ClientMNVerified' in df.columns:\n",
    "        df['Engagement_Score'] += df['ClientMNVerified'].astype(float) * 0.25\n",
    "    if 'RiskProfile' in df.columns:\n",
    "        df['Engagement_Score'] += df['RiskProfile'].notna().astype(float) * 0.25\n",
    "    if 'EMFC_Count' in df.columns:\n",
    "        df['Engagement_Score'] += (df['EMFC_Count'] > 1).astype(float) * 0.25\n",
    "    \n",
    "    # Financial maturity index\n",
    "    if 'Total_Investments' in df.columns and 'Total_Liquid_Assets' in df.columns:\n",
    "        df['Financial_Maturity_Index'] = np.where(\n",
    "            df['Total_Liquid_Assets'] > 0,\n",
    "            df['Total_Investments'] / (df['Total_Liquid_Assets'] + df['Total_Investments'] + 1),\n",
    "            0\n",
    "        )\n",
    "    \n",
    "    # Digital adoption score\n",
    "    contact_prefs = df.get('ClientContactPreferences', pd.Series(['[]'] * len(df)))\n",
    "    df['Digital_Adoption_Score'] = contact_prefs.apply(\n",
    "        lambda x: 1 if 'Email' in str(x) or 'Text' in str(x) else 0\n",
    "    )\n",
    "    \n",
    "    # Protection gap score\n",
    "    df['Protection_Gap_Score'] = 0\n",
    "    if 'Life_Coverage_Gap' in df.columns:\n",
    "        df['Protection_Gap_Score'] += df['Life_Coverage_Gap'] * 0.4\n",
    "    if 'CI_Coverage_Gap' in df.columns:\n",
    "        df['Protection_Gap_Score'] += df['CI_Coverage_Gap'] * 0.3\n",
    "    if 'Has_Hospital_Coverage' in df.columns:\n",
    "        df['Protection_Gap_Score'] += (df['Has_Hospital_Coverage'] == 0).astype(float) * 0.3\n",
    "    \n",
    "    print(\"   ✅ Created behavioral features\")\n",
    "    return df\n",
    "\n",
    "def create_business_driven_features(df):\n",
    "    \"\"\"Create features based on business logic and insurance needs\"\"\"\n",
    "    print(\"\\n💼 Creating business-driven features...\")\n",
    "    \n",
    "    # Underinsured indicator\n",
    "    if 'Life_Coverage_Multiple' in df.columns and 'Income_Numeric' in df.columns:\n",
    "        df['Underinsured'] = (\n",
    "            (df['Life_Coverage_Multiple'] < 8) & \n",
    "            (df['Income_Numeric'] > 50000)\n",
    "        ).astype(int)\n",
    "    \n",
    "    # Retirement planning need\n",
    "    if 'ClientAge' in df.columns and 'Total_CPF' in df.columns and 'Income_Numeric' in df.columns:\n",
    "        df['Retirement_Planning_Need'] = (\n",
    "            (df['ClientAge'] > 40) & \n",
    "            (df['Total_CPF'] < df['Income_Numeric'] * 3)\n",
    "        ).astype(int)\n",
    "    \n",
    "    # Wealth accumulation opportunity\n",
    "    if 'Age_Group' in df.columns and 'Investment_Ratio' in df.columns:\n",
    "        df['Wealth_Accumulation_Opp'] = (\n",
    "            (df['ClientAge'].between(35, 55)) &\n",
    "            (df['Investment_Ratio'] < 0.3) &\n",
    "            (df['Income_Numeric'] > 75000)\n",
    "        ).astype(int)\n",
    "    \n",
    "    # Critical illness vulnerability\n",
    "    if 'ClientAge' in df.columns and 'Has_CI_Coverage' in df.columns:\n",
    "        df['CI_Vulnerability'] = (\n",
    "            (df['ClientAge'] > 35) &\n",
    "            (df['Has_CI_Coverage'] == 0)\n",
    "        ).astype(int)\n",
    "    \n",
    "    # Family protection priority\n",
    "    if 'MaritalStatus' in df.columns and 'ClientAge' in df.columns:\n",
    "        df['Family_Protection_Priority'] = (\n",
    "            (df['MaritalStatus'].isin(['Married', 'Divorced'])) &\n",
    "            (df['ClientAge'].between(25, 50)) &\n",
    "            (df.get('Life_Coverage_Multiple', 0) < 10)\n",
    "        ).astype(int)\n",
    "    \n",
    "    # Premium affordability ratio\n",
    "    if 'Total_Annual_Premium' in df.columns and 'Income_Numeric' in df.columns:\n",
    "        df['Premium_Affordability_Buffer'] = np.where(\n",
    "            df['Income_Numeric'] > 0,\n",
    "            1 - (df['Total_Annual_Premium'] / (df['Income_Numeric'] * 0.15)),  # 15% income rule\n",
    "            1\n",
    "        ).clip(0, 1)\n",
    "    \n",
    "    print(\"   ✅ Created business-driven features\")\n",
    "    return df\n",
    "\n",
    "def create_interaction_features(df):\n",
    "    \"\"\"Create interaction features between key variables\"\"\"\n",
    "    print(\"\\n🔗 Creating interaction features...\")\n",
    "    \n",
    "    # Age × Income interaction\n",
    "    if 'ClientAge' in df.columns and 'Income_Numeric' in df.columns:\n",
    "        df['Age_Income_Interaction'] = df['ClientAge'] * np.log1p(df['Income_Numeric'])\n",
    "    \n",
    "    # Insurance × Tenure interaction\n",
    "    if 'Has_Insurance' in df.columns and 'Years_With_Insurance' in df.columns:\n",
    "        df['Insurance_Tenure_Interaction'] = df['Has_Insurance'] * df['Years_With_Insurance']\n",
    "    \n",
    "    # Sophistication × Gap interaction\n",
    "    if 'Financial_Sophistication' in df.columns and 'Protection_Gap_Score' in df.columns:\n",
    "        df['Sophistication_Gap_Interaction'] = (\n",
    "            df['Financial_Sophistication'].map({'High': 2, 'Medium': 1, 'Low': 0}).fillna(0) * \n",
    "            df['Protection_Gap_Score']\n",
    "        )\n",
    "    \n",
    "    # Wealth × Age interaction\n",
    "    if 'Estimated_Net_Worth' in df.columns and 'Age_Group' in df.columns:\n",
    "        age_numeric = df['ClientAge'].fillna(40)\n",
    "        df['Wealth_Age_Ratio'] = np.log1p(df['Estimated_Net_Worth']) / (age_numeric / 10)\n",
    "    \n",
    "    print(\"   ✅ Created interaction features\")\n",
    "    return df\n",
    "\n",
    "def create_portfolio_sequence_features(df, portfolio_df):\n",
    "    \"\"\"Create features based on product sequence patterns\"\"\"\n",
    "    print(\"\\n📊 Creating portfolio sequence features...\")\n",
    "    \n",
    "    # Product affinity mapping\n",
    "    product_sequences = {\n",
    "        'Term': ['Whole Life', 'Investment-Linked', 'Universal Life'],\n",
    "        'Integrated Shield': ['Critical Illness', 'Hospital Income', 'Disability Income'],\n",
    "        'Whole Life': ['Investment-Linked', 'Endowment', 'Annuity'],\n",
    "        'Endowment': ['Investment-Linked', 'Annuity'],\n",
    "        'Critical Illness': ['Early Critical Illness', 'Long Term Care']\n",
    "    }\n",
    "    \n",
    "    # Create affinity scores\n",
    "    if 'Plan_Types' in df.columns:\n",
    "        for current_product, next_products in product_sequences.items():\n",
    "            for next_product in next_products:\n",
    "                feature_name = f'Affinity_{current_product.replace(\" \", \"_\")}_to_{next_product.replace(\" \", \"_\")}'\n",
    "                df[feature_name] = df['Plan_Types'].apply(\n",
    "                    lambda x: 1 if isinstance(x, list) and any(current_product in str(p) for p in x) else 0\n",
    "                )\n",
    "    \n",
    "    # Product diversity score\n",
    "    if 'Plan_Types' in df.columns:\n",
    "        df['Product_Diversity_Score'] = df['Plan_Types'].apply(\n",
    "            lambda x: len(set(x)) if isinstance(x, list) else 0\n",
    "        )\n",
    "    \n",
    "    # Insurance evolution stage\n",
    "    def get_insurance_evolution_stage(plan_types):\n",
    "        if not isinstance(plan_types, list) or len(plan_types) == 0:\n",
    "            return 0  # No insurance\n",
    "        \n",
    "        plan_str = ' '.join(str(p) for p in plan_types)\n",
    "        \n",
    "        if 'Investment-Linked' in plan_str or 'Annuity' in plan_str:\n",
    "            return 4  # Advanced\n",
    "        elif 'Whole Life' in plan_str or 'Endowment' in plan_str:\n",
    "            return 3  # Intermediate\n",
    "        elif 'Critical Illness' in plan_str or 'Disability' in plan_str:\n",
    "            return 2  # Developing\n",
    "        else:\n",
    "            return 1  # Basic\n",
    "    \n",
    "    if 'Plan_Types' in df.columns:\n",
    "        df['Insurance_Evolution_Stage'] = df['Plan_Types'].apply(get_insurance_evolution_stage)\n",
    "    \n",
    "    print(\"   ✅ Created portfolio sequence features\")\n",
    "    return df\n",
    "\n",
    "def create_original_derived_features(df):\n",
    "    \"\"\"Keep original derived features for compatibility\"\"\"\n",
    "    print(\"\\n🔧 Creating original derived features...\")\n",
    "    \n",
    "    # Income features\n",
    "    if 'IncomeRange' in df.columns:\n",
    "        income_mapping = {\n",
    "            'No Income': 0,\n",
    "            'Below S$30,000': 15000,\n",
    "            'S$30,000 - S$49,999': 40000,\n",
    "            'S$50,000 - S$99,999': 75000,\n",
    "            'S$100,000 and above': 150000\n",
    "        }\n",
    "        df['Income_Numeric'] = df['IncomeRange'].map(income_mapping).fillna(0)\n",
    "        \n",
    "        income_category_mapping = {\n",
    "            'No Income': 'Low', 'Below S$30,000': 'Low',\n",
    "            'S$30,000 - S$49,999': 'Medium', 'S$50,000 - S$99,999': 'Medium',\n",
    "            'S$100,000 and above': 'High'\n",
    "        }\n",
    "        df['Income_Category'] = df['IncomeRange'].map(income_category_mapping).fillna('Unknown')\n",
    "    \n",
    "    # Asset aggregations\n",
    "    liquid_assets = ['SavingsAccounts', 'FixedDepositsAccount']\n",
    "    investment_assets = ['StocksPortofolio', 'BondPortofolio', 'UTFEquityAsset', 'ETFs']\n",
    "    cpf_assets = ['CPFOABalance', 'CPFSABalance', 'CPFMABalance']\n",
    "    \n",
    "    df['Total_Liquid_Assets'] = df[liquid_assets].fillna(0).sum(axis=1)\n",
    "    df['Total_Investments'] = df[investment_assets].fillna(0).sum(axis=1)\n",
    "    df['Total_CPF'] = df[cpf_assets].fillna(0).sum(axis=1)\n",
    "    \n",
    "    # Net worth calculation\n",
    "    wealth_components = ['Total_Liquid_Assets', 'Total_Investments', 'Total_CPF', 'InvestmentProperties']\n",
    "    df['Estimated_Net_Worth'] = df[[c for c in wealth_components if c in df.columns]].fillna(0).sum(axis=1)\n",
    "    \n",
    "    # Investment ratio\n",
    "    total_financial = df['Total_Liquid_Assets'] + df['Total_Investments']\n",
    "    df['Investment_Ratio'] = np.where(\n",
    "        total_financial > 0,\n",
    "        df['Total_Investments'] / total_financial,\n",
    "        0\n",
    "    )\n",
    "    \n",
    "    # Insurance features\n",
    "    df['Has_Insurance'] = (df['Total_Policies'].fillna(0) > 0).astype(int)\n",
    "    \n",
    "    if 'Income_Numeric' in df.columns:\n",
    "        df['Life_Coverage_Multiple'] = np.where(\n",
    "            df['Income_Numeric'] > 0,\n",
    "            df['Total_Life_Coverage'].fillna(0) / df['Income_Numeric'],\n",
    "            0\n",
    "        )\n",
    "        \n",
    "        df['Premium_to_Income_Ratio'] = np.where(\n",
    "            df['Income_Numeric'] > 0,\n",
    "            df['Total_Annual_Premium'].fillna(0) / df['Income_Numeric'],\n",
    "            0\n",
    "        )\n",
    "    \n",
    "    # Coverage indicators\n",
    "    df['Has_Life_Coverage'] = (df['Total_Life_Coverage'].fillna(0) > 0).astype(int)\n",
    "    df['Has_CI_Coverage'] = (df['Total_CI_Coverage'].fillna(0) > 0).astype(int)\n",
    "    df['Has_Hospital_Coverage'] = (df['Total_Hospital_Income'].fillna(0) > 0).astype(int)\n",
    "    df['Has_LTC_Coverage'] = (df['Total_LTC_Coverage'].fillna(0) > 0).astype(int)\n",
    "    \n",
    "    # Coverage gaps\n",
    "    df['Life_Coverage_Gap'] = ((df['Has_Insurance'] == 1) & (df['Has_Life_Coverage'] == 0)).astype(int)\n",
    "    df['CI_Coverage_Gap'] = ((df['Has_Insurance'] == 1) & (df['Has_CI_Coverage'] == 0)).astype(int)\n",
    "    \n",
    "    # Insurance sophistication\n",
    "    def calculate_insurance_sophistication(row):\n",
    "        policies = row.get('Total_Policies', 0)\n",
    "        if policies == 0:\n",
    "            return 'No_Insurance'\n",
    "        \n",
    "        coverage_types = (row.get('Has_Life_Coverage', 0) + row.get('Has_CI_Coverage', 0) + \n",
    "                         row.get('Has_Hospital_Coverage', 0) + row.get('Has_LTC_Coverage', 0))\n",
    "        \n",
    "        if coverage_types >= 3:\n",
    "            return 'Comprehensive'\n",
    "        elif coverage_types >= 2:\n",
    "            return 'Moderate'\n",
    "        else:\n",
    "            return 'Basic'\n",
    "    \n",
    "    df['Insurance_Sophistication'] = df.apply(calculate_insurance_sophistication, axis=1)\n",
    "    \n",
    "    # Age groups\n",
    "    if 'ClientAge' in df.columns:\n",
    "        df['Age_Group'] = pd.cut(\n",
    "            df['ClientAge'], \n",
    "            bins=[0, 25, 35, 45, 55, 65, 100], \n",
    "            labels=['Under_25', '25-35', '35-45', '45-55', '55-65', 'Over_65'],\n",
    "            include_lowest=True\n",
    "        )\n",
    "    \n",
    "    # Life stage\n",
    "    if 'Age_Group' in df.columns and 'MaritalStatus' in df.columns:\n",
    "        def determine_life_stage(row):\n",
    "            age_group = row['Age_Group']\n",
    "            marital_status = row['MaritalStatus']\n",
    "            \n",
    "            if pd.isna(age_group) or pd.isna(marital_status):\n",
    "                return 'Unknown'\n",
    "            \n",
    "            age_str = str(age_group)\n",
    "            marital_str = str(marital_status).lower()\n",
    "            \n",
    "            if age_str in ['Under_25', '25-35']:\n",
    "                return 'Young_Single' if 'single' in marital_str else 'Young_Family'\n",
    "            elif age_str in ['35-45', '45-55']:\n",
    "                return 'Mid_Career_Single' if 'single' in marital_str else 'Mid_Career_Family'\n",
    "            else:\n",
    "                return 'Pre_Retirement'\n",
    "        \n",
    "        df['Life_Stage'] = df.apply(determine_life_stage, axis=1)\n",
    "    \n",
    "    # Financial sophistication\n",
    "    if 'Education' in df.columns and 'Investment_Ratio' in df.columns:\n",
    "        def calculate_financial_sophistication(row):\n",
    "            score = 0\n",
    "            education = str(row.get('Education', '')).lower()\n",
    "            if 'university' in education or 'degree' in education:\n",
    "                score += 2\n",
    "            elif 'diploma' in education:\n",
    "                score += 1\n",
    "            \n",
    "            inv_ratio = row.get('Investment_Ratio', 0)\n",
    "            if inv_ratio > 0.3:\n",
    "                score += 2\n",
    "            elif inv_ratio > 0.1:\n",
    "                score += 1\n",
    "            \n",
    "            return 'High' if score >= 3 else 'Medium' if score >= 1 else 'Low'\n",
    "        \n",
    "        df['Financial_Sophistication'] = df.apply(calculate_financial_sophistication, axis=1)\n",
    "    \n",
    "    print(\"   ✅ Created original derived features\")\n",
    "    return df\n",
    "\n",
    "# Execute the enhanced unification\n",
    "print(\"\\n🚀 STARTING ENHANCED DATA UNIFICATION...\")\n",
    "enhanced_unified_dataset = create_enhanced_unified_dataset(datasets)\n",
    "\n",
    "# Save the enhanced dataset\n",
    "enhanced_unified_dataset.to_excel('enhanced_unified_client_dataset.xlsx', index=False)\n",
    "print(f\"\\n💾 Saved: enhanced_unified_client_dataset.xlsx\")\n",
    "\n",
    "# Display enhancement summary\n",
    "print(\"\\n📊 ENHANCEMENT SUMMARY:\")\n",
    "new_features = [\n",
    "    'Client_Tenure_Years', 'Days_Since_Last_FNA', 'FNA_Frequency',\n",
    "    'Years_With_Insurance', 'Engagement_Score', 'Financial_Maturity_Index',\n",
    "    'Digital_Adoption_Score', 'Protection_Gap_Score', 'Underinsured',\n",
    "    'Retirement_Planning_Need', 'Wealth_Accumulation_Opp', 'CI_Vulnerability',\n",
    "    'Family_Protection_Priority', 'Premium_Affordability_Buffer',\n",
    "    'Age_Income_Interaction', 'Insurance_Evolution_Stage', 'Product_Diversity_Score'\n",
    "]\n",
    "\n",
    "print(f\"✨ New Enhanced Features Added: {len(new_features)}\")\n",
    "for i, feat in enumerate(new_features, 1):\n",
    "    if feat in enhanced_unified_dataset.columns:\n",
    "        non_null = enhanced_unified_dataset[feat].notna().sum()\n",
    "        coverage = (non_null / len(enhanced_unified_dataset)) * 100\n",
    "        print(f\"   {i:2d}. {feat}: {coverage:.1f}% coverage\")\n",
    "\n",
    "print(f\"\\n🎉 ENHANCED PHASE 1 COMPLETE!\")\n",
    "print(f\"✅ Total features: {len(enhanced_unified_dataset.columns)}\")\n",
    "print(f\"✅ Enhanced features provide better signals for ML modeling\")\n",
    "print(f\"✅ Ready for Phase 2: Target Variable Engineering\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "34b40b51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== ENHANCED PHASE 2: TARGET VARIABLE ENGINEERING ===\n",
      "Goal: Create high-quality ML training data with validated targets\n",
      "\n",
      "=== STEP 1: LOAD ENHANCED DATASETS ===\n",
      "✅ Loaded enhanced client dataset: 65,010 unique clients\n",
      "   Total features: 130\n",
      "✅ Loaded emfc2productsolution: 43,501 records\n",
      "✅ Loaded ProductMainPlan: 1,532 records\n",
      "✅ Loaded ProductType: 4 records\n",
      "✅ Loaded ProductCategory: 10 records\n",
      "✅ Loaded productsubcategory: 39 records\n",
      "✅ Loaded emfc2fna: 51,772 records\n",
      "\n",
      "=== STEP 2: BUILD VALIDATED PRODUCT HIERARCHY ===\n",
      "\n",
      "🔍 Validating product data quality...\n",
      "   Main plans with null SubCategoryId: 2\n",
      "   Subcategories with null CategoryId: 0\n",
      "   Categories with null TypeId: 0\n",
      "✅ Products with complete hierarchy: 1,485 / 1,532\n",
      "\n",
      "=== STEP 3: ANALYZE RECOMMENDATIONS WITH BUSINESS LOGIC ===\n",
      "\n",
      "🔧 Applying business logic filters...\n",
      "📊 RECOMMENDATION FILTERING:\n",
      "   Total recommendations: 43,501\n",
      "   Approved: 43,500\n",
      "   Rejection rate: 0.0%\n",
      "   Recommendations with FNA context: 43,311\n",
      "\n",
      "🎯 SUBCATEGORY DISTRIBUTION (TOP 20):\n",
      "    1. Long Term Care Plans: 12,994 (29.9%)\n",
      "    2. SHIELD: 10,526 (24.2%)\n",
      "    3. Term: 5,407 (12.4%)\n",
      "    4. Investment Linked Plan - Accumulation: 3,272 (7.5%)\n",
      "    5. Whole Life: 2,772 (6.4%)\n",
      "    6. Accident and Health Plans: 1,526 (3.5%)\n",
      "    7. Critical Illness Plans: 1,329 (3.1%)\n",
      "    8. ENDOW (NP): 891 (2.0%)\n",
      "    9. Retirement: 683 (1.6%)\n",
      "   10. Add Rider to existing shield plans: 638 (1.5%)\n",
      "   11. Endowment: 431 (1.0%)\n",
      "   12. Policy Servicing (Non Shield Plans): 334 (0.8%)\n",
      "   13. Whole Life Income: 241 (0.6%)\n",
      "   14. Universal Life (Protection): 123 (0.3%)\n",
      "   15. Discretionary Managed Account: 60 (0.1%)\n",
      "   16. Bond: 13 (0.0%)\n",
      "   17. ILP: 4 (0.0%)\n",
      "   18. Universal Life (Accumulation): 2 (0.0%)\n",
      "\n",
      "=== STEP 4: ENHANCED CLIENT-RECOMMENDATION JOIN ===\n",
      "\n",
      "🔗 JOIN RESULTS:\n",
      "   PersonalInformationId: 43,424 records \n",
      "   EMFC2FNAId: 43,643 records ✅ BEST\n",
      "\n",
      "✅ Final training data: 43,643 records\n",
      "   Unique clients: 25,115\n",
      "   Coverage: 38.6% of all clients\n",
      "\n",
      "=== STEP 5: DATA QUALITY VALIDATION ===\n",
      "\n",
      "🔍 Checking for data leakage...\n",
      "   Temporally valid records: 43,478 / 43,643\n",
      "   ⚠️  Removing 165 records with temporal issues\n",
      "   Records with valid subcategories: 41,197\n",
      "\n",
      "=== STEP 6: PREPARE TARGET VARIABLE ===\n",
      "\n",
      "📊 TARGET DISTRIBUTION ANALYSIS:\n",
      "   Total subcategories: 18\n",
      "   Imbalance ratio: 6465:1\n",
      "\n",
      "=== STEP 7: ENHANCED FEATURE QUALITY ASSESSMENT ===\n",
      "\n",
      "📊 FEATURE QUALITY BY CATEGORY:\n",
      "\n",
      "   Demographics:\n",
      "      Available: 4/4 features\n",
      "      Avg completeness: 100.0%\n",
      "\n",
      "   Socioeconomic:\n",
      "      Available: 4/4 features\n",
      "      Avg completeness: 100.0%\n",
      "\n",
      "   Financial:\n",
      "      Available: 4/4 features\n",
      "      Avg completeness: 100.0%\n",
      "\n",
      "   Insurance:\n",
      "      Available: 4/4 features\n",
      "      Avg completeness: 100.0%\n",
      "\n",
      "   Behavioral:\n",
      "      Available: 2/3 features\n",
      "      Avg completeness: 100.0%\n",
      "      Missing: ['Financial_Maturity_Index']\n",
      "\n",
      "   Temporal:\n",
      "      Available: 3/3 features\n",
      "      Avg completeness: 70.8%\n",
      "\n",
      "   Business:\n",
      "      Available: 1/3 features\n",
      "      Avg completeness: 100.0%\n",
      "      Missing: ['Underinsured', 'Retirement_Planning_Need']\n",
      "\n",
      "=== STEP 8: HANDLE CLASS IMBALANCE ===\n",
      "\n",
      "📊 Removing 3 small subcategories (<50 samples)\n",
      "   Records after filtering: 41,178\n",
      "   Remaining subcategories: 15\n",
      "\n",
      "🔧 Creating balanced dataset (max 5000 per class)...\n",
      "\n",
      "=== STEP 9: CREATE TEMPORAL VALIDATION SPLIT ===\n",
      "📅 Temporal split created:\n",
      "   Train: 32,942 records (up to 2024-11-21)\n",
      "   Test: 8,236 records (from 2024-11-21)\n",
      "\n",
      "=== STEP 10: SAVE ENHANCED ML DATASETS ===\n",
      "💾 Saved: ML_TRAINING_ENHANCED_FULL_V3.xlsx (41,178 records)\n",
      "💾 Saved: ML_TRAINING_ENHANCED_BALANCED_V3.xlsx (27,365 records)\n",
      "💾 Saved: ML_TRAINING_TEMPORAL_TRAIN_V3.xlsx (32,942 records)\n",
      "💾 Saved: ML_TRAINING_TEMPORAL_TEST_V3.xlsx (8,236 records)\n",
      "\n",
      "=== STEP 11: ENHANCED SUMMARY REPORT ===\n",
      "💾 Saved: ENHANCED_PHASE_2_REPORT.txt\n",
      "\n",
      "🎉 ENHANCED PHASE 2 COMPLETE!\n",
      "✅ Created high-quality ML training data with 27 features\n",
      "✅ Enhanced features capture behavioral, temporal, and business signals\n",
      "✅ Multiple validation datasets created for robust model evaluation\n",
      "✅ Ready for Phase 3: Advanced Model Training\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "=== ENHANCED PHASE 2: TARGET VARIABLE ENGINEERING WITH VALIDATION ===\n",
    "Enhanced version with better data quality checks and business logic validation\n",
    "\"\"\"\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "print(\"=== ENHANCED PHASE 2: TARGET VARIABLE ENGINEERING ===\")\n",
    "print(\"Goal: Create high-quality ML training data with validated targets\")\n",
    "\n",
    "# Step 1: Load datasets\n",
    "print(\"\\n=== STEP 1: LOAD ENHANCED DATASETS ===\")\n",
    "\n",
    "def load_encrypted_excel(file_path: str, password: str) -> pd.DataFrame:\n",
    "    import msoffcrypto\n",
    "    import io\n",
    "    with open(file_path, 'rb') as f:\n",
    "        office_file = msoffcrypto.OfficeFile(f)\n",
    "        office_file.load_key(password=password)\n",
    "        decrypted = io.BytesIO()\n",
    "        office_file.decrypt(decrypted)\n",
    "        decrypted.seek(0)\n",
    "        return pd.read_excel(decrypted)\n",
    "\n",
    "# Load enhanced client dataset\n",
    "clients_df = pd.read_excel('enhanced_unified_client_dataset.xlsx')\n",
    "print(f\"✅ Loaded enhanced client dataset: {len(clients_df):,} unique clients\")\n",
    "print(f\"   Total features: {len(clients_df.columns)}\")\n",
    "\n",
    "# Load product tables\n",
    "product_tables = {\n",
    "    'emfc2productsolution': '@OFn7oA5!Joe',\n",
    "    'ProductMainPlan': ')XQ4ZDssowrA', \n",
    "    'ProductType': '#9zCw?^-xTO?',\n",
    "    'ProductCategory': '#F)cdAEOVJ@4',\n",
    "    'productsubcategory': 'y-^t$N9>%S%C',\n",
    "    'emfc2fna': 'dQq9T%pC^?22'  # For additional validation\n",
    "}\n",
    "\n",
    "product_datasets = {}\n",
    "for table_name, password in product_tables.items():\n",
    "    try:\n",
    "        product_datasets[table_name] = load_encrypted_excel(f\"{table_name}.xlsx\", password)\n",
    "        print(f\"✅ Loaded {table_name}: {len(product_datasets[table_name]):,} records\")\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Failed to load {table_name}: {e}\")\n",
    "\n",
    "# Step 2: Enhanced Product Hierarchy with Validation\n",
    "print(\"\\n=== STEP 2: BUILD VALIDATED PRODUCT HIERARCHY ===\")\n",
    "\n",
    "# Build and validate product hierarchy\n",
    "main_plans = product_datasets['ProductMainPlan']\n",
    "subcategories = product_datasets['productsubcategory']\n",
    "categories = product_datasets['ProductCategory']\n",
    "product_types = product_datasets['ProductType']\n",
    "\n",
    "# Validate data quality\n",
    "print(\"\\n🔍 Validating product data quality...\")\n",
    "print(f\"   Main plans with null SubCategoryId: {main_plans['ProductSubCategoryId'].isnull().sum()}\")\n",
    "print(f\"   Subcategories with null CategoryId: {subcategories['ProductCategoryId'].isnull().sum()}\")\n",
    "print(f\"   Categories with null TypeId: {categories['ProductTypeId'].isnull().sum()}\")\n",
    "\n",
    "# Build complete hierarchy with validation\n",
    "complete_product_hierarchy = (\n",
    "    main_plans\n",
    "    .merge(subcategories[['ProductSubCategoryId', 'SubCategoryName', 'ProductCategoryId']], \n",
    "           on='ProductSubCategoryId', how='left')\n",
    "    .merge(categories[['ProductCategoryId', 'CategoryName', 'ProductTypeId']], \n",
    "           on='ProductCategoryId', how='left')\n",
    "    .merge(product_types[['ProductTypeId', 'TypeName', 'InvestmentType']], \n",
    "           on='ProductTypeId', how='left')\n",
    ")\n",
    "\n",
    "# Check hierarchy completeness\n",
    "hierarchy_complete = complete_product_hierarchy[['SubCategoryName', 'CategoryName', 'TypeName']].notna().all(axis=1)\n",
    "print(f\"✅ Products with complete hierarchy: {hierarchy_complete.sum():,} / {len(complete_product_hierarchy):,}\")\n",
    "\n",
    "# Step 3: Analyze and Filter Recommendations\n",
    "print(\"\\n=== STEP 3: ANALYZE RECOMMENDATIONS WITH BUSINESS LOGIC ===\")\n",
    "\n",
    "raw_recommendations = product_datasets['emfc2productsolution']\n",
    "fna_data = product_datasets['emfc2fna']\n",
    "\n",
    "# Apply business filters\n",
    "print(\"\\n🔧 Applying business logic filters...\")\n",
    "recommendations = raw_recommendations[\n",
    "    (raw_recommendations['Status'] == 'Approved') &\n",
    "    (raw_recommendations['RecommendedProductRejected'] != True)\n",
    "].copy()\n",
    "\n",
    "print(f\"📊 RECOMMENDATION FILTERING:\")\n",
    "print(f\"   Total recommendations: {len(raw_recommendations):,}\")\n",
    "print(f\"   Approved: {len(recommendations):,}\")\n",
    "print(f\"   Rejection rate: {(1 - len(recommendations)/len(raw_recommendations))*100:.1f}%\")\n",
    "\n",
    "# Add temporal context from FNA\n",
    "if 'EMFC2FNAId' in recommendations.columns and 'EMFC2FNAId' in fna_data.columns:\n",
    "    recommendations = recommendations.merge(\n",
    "        fna_data[['EMFC2FNAId', 'SubmissionTime', 'FNAStatus']], \n",
    "        on='EMFC2FNAId', \n",
    "        how='left'\n",
    "    )\n",
    "    print(f\"   Recommendations with FNA context: {recommendations['SubmissionTime'].notna().sum():,}\")\n",
    "\n",
    "# Join with product hierarchy\n",
    "recommendations_with_products = recommendations.merge(\n",
    "    complete_product_hierarchy[['ProductId', 'SubCategoryName', 'CategoryName', 'TypeName', 'InvestmentType']], \n",
    "    on='ProductId', \n",
    "    how='left'\n",
    ")\n",
    "\n",
    "# Analyze subcategory distribution\n",
    "subcat_distribution = recommendations_with_products['SubCategoryName'].value_counts()\n",
    "print(f\"\\n🎯 SUBCATEGORY DISTRIBUTION (TOP 20):\")\n",
    "for i, (subcat, count) in enumerate(subcat_distribution.head(20).items(), 1):\n",
    "    pct = (count / len(recommendations_with_products)) * 100\n",
    "    print(f\"   {i:2d}. {subcat}: {count:,} ({pct:.1f}%)\")\n",
    "\n",
    "# Step 4: Enhanced Client-Recommendation Join\n",
    "print(\"\\n=== STEP 4: ENHANCED CLIENT-RECOMMENDATION JOIN ===\")\n",
    "\n",
    "# Try multiple join strategies\n",
    "join_results = {}\n",
    "\n",
    "# Strategy 1: PersonalInformationId\n",
    "if 'PersonalInformationId' in clients_df.columns:\n",
    "    join1 = clients_df.merge(recommendations_with_products, on='PersonalInformationId', how='inner')\n",
    "    join_results['PersonalInformationId'] = len(join1)\n",
    "\n",
    "# Strategy 2: EMFC2FNAId\n",
    "if 'EMFC2FNAId' in clients_df.columns:\n",
    "    join2 = clients_df.merge(recommendations_with_products, on='EMFC2FNAId', how='inner')\n",
    "    join_results['EMFC2FNAId'] = len(join2)\n",
    "\n",
    "# Strategy 3: EMFC2Id (if available)\n",
    "if 'EMFC2Id' in clients_df.columns and 'EMFC2Id' in recommendations_with_products.columns:\n",
    "    join3 = clients_df.merge(recommendations_with_products, on='EMFC2Id', how='inner')\n",
    "    join_results['EMFC2Id'] = len(join3)\n",
    "\n",
    "print(\"\\n🔗 JOIN RESULTS:\")\n",
    "best_join_key = max(join_results, key=join_results.get)\n",
    "for key, count in join_results.items():\n",
    "    print(f\"   {key}: {count:,} records {'✅ BEST' if key == best_join_key else ''}\")\n",
    "\n",
    "# Use best join method\n",
    "ml_training_data = clients_df.merge(\n",
    "    recommendations_with_products, \n",
    "    on=best_join_key, \n",
    "    how='inner'\n",
    ")\n",
    "\n",
    "print(f\"\\n✅ Final training data: {len(ml_training_data):,} records\")\n",
    "print(f\"   Unique clients: {ml_training_data['ClientId'].nunique():,}\")\n",
    "print(f\"   Coverage: {ml_training_data['ClientId'].nunique() / len(clients_df) * 100:.1f}% of all clients\")\n",
    "\n",
    "# Step 5: Data Quality Validation\n",
    "print(\"\\n=== STEP 5: DATA QUALITY VALIDATION ===\")\n",
    "\n",
    "# Check for data leakage\n",
    "print(\"\\n🔍 Checking for data leakage...\")\n",
    "if 'DateCreated' in ml_training_data.columns and 'SubmissionTime' in ml_training_data.columns:\n",
    "    ml_training_data['DateCreated'] = pd.to_datetime(ml_training_data['DateCreated'])\n",
    "    ml_training_data['SubmissionTime'] = pd.to_datetime(ml_training_data['SubmissionTime'])\n",
    "    \n",
    "    # Ensure recommendation comes after client data\n",
    "    valid_temporal = ml_training_data['DateCreated'] <= ml_training_data['SubmissionTime']\n",
    "    print(f\"   Temporally valid records: {valid_temporal.sum():,} / {len(ml_training_data):,}\")\n",
    "    \n",
    "    if not valid_temporal.all():\n",
    "        print(f\"   ⚠️  Removing {(~valid_temporal).sum()} records with temporal issues\")\n",
    "        ml_training_data = ml_training_data[valid_temporal]\n",
    "\n",
    "# Remove invalid subcategories\n",
    "ml_training_data = ml_training_data[ml_training_data['SubCategoryName'].notna()]\n",
    "print(f\"   Records with valid subcategories: {len(ml_training_data):,}\")\n",
    "\n",
    "# Step 6: Prepare Target Variable with Business Logic\n",
    "print(\"\\n=== STEP 6: PREPARE TARGET VARIABLE ===\")\n",
    "\n",
    "def clean_subcategory_name(subcategory):\n",
    "    \"\"\"Clean and standardize subcategory names\"\"\"\n",
    "    if pd.isna(subcategory):\n",
    "        return 'Unknown'\n",
    "    \n",
    "    subcategory = str(subcategory).strip()\n",
    "    subcategory = (subcategory.replace(' ', '_')\n",
    "                             .replace('-', '_')\n",
    "                             .replace('(', '')\n",
    "                             .replace(')', '')\n",
    "                             .replace('/', '_')\n",
    "                             .replace('&', 'and')\n",
    "                             .replace(',', ''))\n",
    "    return subcategory\n",
    "\n",
    "ml_training_data['Target_SubCategory'] = ml_training_data['SubCategoryName'].apply(clean_subcategory_name)\n",
    "\n",
    "# Analyze class distribution\n",
    "target_dist = ml_training_data['Target_SubCategory'].value_counts()\n",
    "print(f\"\\n📊 TARGET DISTRIBUTION ANALYSIS:\")\n",
    "print(f\"   Total subcategories: {target_dist.nunique()}\")\n",
    "print(f\"   Imbalance ratio: {target_dist.max() / target_dist.min():.0f}:1\")\n",
    "\n",
    "# Step 7: Feature Quality Assessment\n",
    "print(\"\\n=== STEP 7: ENHANCED FEATURE QUALITY ASSESSMENT ===\")\n",
    "\n",
    "# Define comprehensive feature sets\n",
    "feature_categories = {\n",
    "    'Demographics': ['ClientAge', 'ClientGender', 'Nationality', 'MaritalStatus'],\n",
    "    'Socioeconomic': ['IncomeRange', 'Income_Numeric', 'Education', 'EmploymentStatus'],\n",
    "    'Financial': ['Total_Liquid_Assets', 'Total_Investments', 'Estimated_Net_Worth', 'Investment_Ratio'],\n",
    "    'Insurance': ['Has_Insurance', 'Life_Coverage_Multiple', 'Insurance_Sophistication', 'Insurance_Evolution_Stage'],\n",
    "    'Behavioral': ['Engagement_Score', 'Financial_Maturity_Index', 'Digital_Adoption_Score'],\n",
    "    'Temporal': ['Client_Tenure_Years', 'Days_Since_Last_FNA', 'Years_With_Insurance'],\n",
    "    'Business': ['Underinsured', 'Retirement_Planning_Need', 'Protection_Gap_Score']\n",
    "}\n",
    "\n",
    "print(\"\\n📊 FEATURE QUALITY BY CATEGORY:\")\n",
    "feature_quality_summary = {}\n",
    "\n",
    "for category, features in feature_categories.items():\n",
    "    available = [f for f in features if f in ml_training_data.columns]\n",
    "    missing = [f for f in features if f not in ml_training_data.columns]\n",
    "    \n",
    "    if available:\n",
    "        avg_completeness = (ml_training_data[available].notna().sum() / len(ml_training_data)).mean()\n",
    "        feature_quality_summary[category] = {\n",
    "            'available': len(available),\n",
    "            'missing': len(missing),\n",
    "            'avg_completeness': avg_completeness\n",
    "        }\n",
    "        \n",
    "        print(f\"\\n   {category}:\")\n",
    "        print(f\"      Available: {len(available)}/{len(features)} features\")\n",
    "        print(f\"      Avg completeness: {avg_completeness*100:.1f}%\")\n",
    "        if missing:\n",
    "            print(f\"      Missing: {missing}\")\n",
    "\n",
    "# Step 8: Handle Class Imbalance\n",
    "print(\"\\n=== STEP 8: HANDLE CLASS IMBALANCE ===\")\n",
    "\n",
    "# Set thresholds\n",
    "min_samples_threshold = 50\n",
    "max_samples_per_class = 5000  # Increased for better representation\n",
    "\n",
    "# Filter small classes\n",
    "small_classes = target_dist[target_dist < min_samples_threshold]\n",
    "if len(small_classes) > 0:\n",
    "    print(f\"\\n📊 Removing {len(small_classes)} small subcategories (<{min_samples_threshold} samples)\")\n",
    "    ml_training_clean = ml_training_data[\n",
    "        ~ml_training_data['Target_SubCategory'].isin(small_classes.index)\n",
    "    ].copy()\n",
    "else:\n",
    "    ml_training_clean = ml_training_data.copy()\n",
    "\n",
    "print(f\"   Records after filtering: {len(ml_training_clean):,}\")\n",
    "print(f\"   Remaining subcategories: {ml_training_clean['Target_SubCategory'].nunique()}\")\n",
    "\n",
    "# Create balanced dataset\n",
    "print(f\"\\n🔧 Creating balanced dataset (max {max_samples_per_class} per class)...\")\n",
    "balanced_samples = []\n",
    "\n",
    "for subcategory in ml_training_clean['Target_SubCategory'].value_counts().index:\n",
    "    subcat_data = ml_training_clean[ml_training_clean['Target_SubCategory'] == subcategory]\n",
    "    \n",
    "    if len(subcat_data) > max_samples_per_class:\n",
    "        # Stratified sampling to maintain temporal distribution\n",
    "        sampled = subcat_data.sample(n=max_samples_per_class, random_state=42)\n",
    "    else:\n",
    "        sampled = subcat_data.copy()\n",
    "    \n",
    "    balanced_samples.append(sampled)\n",
    "\n",
    "balanced_training_data = pd.concat(balanced_samples, ignore_index=True)\n",
    "\n",
    "# Step 9: Create Temporal Validation Split\n",
    "print(\"\\n=== STEP 9: CREATE TEMPORAL VALIDATION SPLIT ===\")\n",
    "\n",
    "if 'DateCreated' in ml_training_clean.columns:\n",
    "    # Sort by date\n",
    "    ml_training_clean = ml_training_clean.sort_values('DateCreated')\n",
    "    \n",
    "    # Use last 20% as temporal holdout\n",
    "    split_idx = int(len(ml_training_clean) * 0.8)\n",
    "    temporal_train = ml_training_clean.iloc[:split_idx]\n",
    "    temporal_test = ml_training_clean.iloc[split_idx:]\n",
    "    \n",
    "    print(f\"📅 Temporal split created:\")\n",
    "    print(f\"   Train: {len(temporal_train):,} records (up to {temporal_train['DateCreated'].max().date()})\")\n",
    "    print(f\"   Test: {len(temporal_test):,} records (from {temporal_test['DateCreated'].min().date()})\")\n",
    "\n",
    "# Step 10: Save Enhanced Datasets\n",
    "print(\"\\n=== STEP 10: SAVE ENHANCED ML DATASETS ===\")\n",
    "\n",
    "# Define final feature set\n",
    "all_features = []\n",
    "for features in feature_categories.values():\n",
    "    all_features.extend(features)\n",
    "\n",
    "# Add additional important features\n",
    "additional_features = [\n",
    "    'Age_Income_Interaction', 'Product_Diversity_Score', 'Premium_Affordability_Buffer',\n",
    "    'Family_Protection_Priority', 'CI_Vulnerability', 'Wealth_Accumulation_Opp',\n",
    "    'Target_SubCategory', 'DateCreated', 'ClientId'\n",
    "]\n",
    "\n",
    "final_features = list(set(all_features + additional_features))\n",
    "available_final_features = [f for f in final_features if f in ml_training_clean.columns]\n",
    "\n",
    "# Save datasets\n",
    "datasets_to_save = {\n",
    "    'ML_TRAINING_ENHANCED_FULL_V3.xlsx': ml_training_clean[available_final_features],\n",
    "    'ML_TRAINING_ENHANCED_BALANCED_V3.xlsx': balanced_training_data[available_final_features],\n",
    "}\n",
    "\n",
    "if 'DateCreated' in ml_training_clean.columns:\n",
    "    datasets_to_save['ML_TRAINING_TEMPORAL_TRAIN_V3.xlsx'] = temporal_train[available_final_features]\n",
    "    datasets_to_save['ML_TRAINING_TEMPORAL_TEST_V3.xlsx'] = temporal_test[available_final_features]\n",
    "\n",
    "for filename, dataset in datasets_to_save.items():\n",
    "    dataset.to_excel(filename, index=False)\n",
    "    print(f\"💾 Saved: {filename} ({len(dataset):,} records)\")\n",
    "\n",
    "# Step 11: Generate Enhanced Summary Report\n",
    "print(\"\\n=== STEP 11: ENHANCED SUMMARY REPORT ===\")\n",
    "\n",
    "# Calculate enhanced metrics\n",
    "enhancement_metrics = {\n",
    "    'Original_Features': len(clients_df.columns),\n",
    "    'Enhanced_Features': len(available_final_features),\n",
    "    'New_Behavioral_Features': sum(1 for f in ['Engagement_Score', 'Financial_Maturity_Index', 'Digital_Adoption_Score'] if f in available_final_features),\n",
    "    'New_Temporal_Features': sum(1 for f in ['Client_Tenure_Years', 'Days_Since_Last_FNA', 'Years_With_Insurance'] if f in available_final_features),\n",
    "    'New_Business_Features': sum(1 for f in ['Underinsured', 'Retirement_Planning_Need', 'Protection_Gap_Score'] if f in available_final_features),\n",
    "    'Feature_Completeness': f\"{(ml_training_clean[available_final_features].notna().sum().sum() / (len(ml_training_clean) * len(available_final_features)) * 100):.1f}%\"\n",
    "}\n",
    "\n",
    "with open('ENHANCED_PHASE_2_REPORT.txt', 'w') as f:\n",
    "    f.write(\"=== ENHANCED PHASE 2: TARGET VARIABLE ENGINEERING REPORT ===\\n\")\n",
    "    f.write(f\"Generated: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\\n\\n\")\n",
    "    \n",
    "    f.write(\"ENHANCEMENT METRICS:\\n\")\n",
    "    for key, value in enhancement_metrics.items():\n",
    "        f.write(f\"  {key}: {value}\\n\")\n",
    "    \n",
    "    f.write(\"\\nFEATURE QUALITY BY CATEGORY:\\n\")\n",
    "    for category, metrics in feature_quality_summary.items():\n",
    "        f.write(f\"  {category}: {metrics['available']} features, {metrics['avg_completeness']*100:.1f}% complete\\n\")\n",
    "    \n",
    "    f.write(f\"\\nTARGET DISTRIBUTION (Top 10):\\n\")\n",
    "    for i, (subcat, count) in enumerate(target_dist.head(10).items(), 1):\n",
    "        f.write(f\"  {i}. {subcat}: {count:,} ({count/len(ml_training_clean)*100:.1f}%)\\n\")\n",
    "\n",
    "print(f\"💾 Saved: ENHANCED_PHASE_2_REPORT.txt\")\n",
    "\n",
    "print(f\"\\n🎉 ENHANCED PHASE 2 COMPLETE!\")\n",
    "print(f\"✅ Created high-quality ML training data with {len(available_final_features)} features\")\n",
    "print(f\"✅ Enhanced features capture behavioral, temporal, and business signals\")\n",
    "print(f\"✅ Multiple validation datasets created for robust model evaluation\")\n",
    "print(f\"✅ Ready for Phase 3: Advanced Model Training\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dc922d6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== ENHANCED PHASE 3: ADVANCED ML MODEL TRAINING ===\n",
      "Goal: Build high-performance ensemble model with business metrics\n",
      "\n",
      "=== STEP 1: LOAD ENHANCED TRAINING DATA ===\n",
      "✅ Full dataset: 41,178 records, 15 classes\n",
      "✅ Balanced dataset: 27,365 records, 15 classes\n",
      "✅ Temporal train: 32,942 records\n",
      "✅ Temporal test: 8,236 records\n",
      "\n",
      "=== STEP 2: ENHANCED FEATURE PREPARATION ===\n",
      "🔧 Preparing features for 41,178 records...\n",
      "   📊 Features identified:\n",
      "      Numerical: 14\n",
      "      Categorical: 7\n",
      "      Binary: 3\n",
      "      Using target encoding for Nationality (73 unique values)\n",
      "   ✅ Feature matrix: (41178, 24)\n",
      "🔧 Preparing features for 27,365 records...\n",
      "   📊 Features identified:\n",
      "      Numerical: 14\n",
      "      Categorical: 7\n",
      "      Binary: 3\n",
      "      Using target encoding for Nationality (69 unique values)\n",
      "   ✅ Feature matrix: (27365, 24)\n",
      "🔧 Preparing features for 32,942 records...\n",
      "   📊 Features identified:\n",
      "      Numerical: 14\n",
      "      Categorical: 7\n",
      "      Binary: 3\n",
      "      Using target encoding for Nationality (72 unique values)\n",
      "   ✅ Feature matrix: (32942, 24)\n",
      "🔧 Preparing features for 8,236 records...\n",
      "   📊 Features identified:\n",
      "      Numerical: 14\n",
      "      Categorical: 7\n",
      "      Binary: 3\n",
      "      Using target encoding for Nationality (45 unique values)\n",
      "   ✅ Feature matrix: (8236, 24)\n",
      "\n",
      "=== STEP 3: ADVANCED FEATURE SELECTION ===\n",
      "🔍 Selecting top 40 diverse features...\n",
      "   ✅ Selected 18 features with MI scores and low correlation\n",
      "\n",
      "   🏆 TOP 10 SELECTED FEATURES:\n",
      "       1. Engagement_Score: MI=0.2862\n",
      "       2. Client_Tenure_Years: MI=0.2087\n",
      "       3. Days_Since_Last_FNA: MI=0.1909\n",
      "       4. Estimated_Net_Worth: MI=0.1617\n",
      "       5. Education: MI=0.1497\n",
      "       6. Total_Liquid_Assets: MI=0.1424\n",
      "       7. IncomeRange: MI=0.1182\n",
      "       8. Income_Numeric: MI=0.1129\n",
      "       9. ClientAge: MI=0.1007\n",
      "      10. Nationality_encoded: MI=0.0648\n",
      "\n",
      "=== STEP 4: CREATE TRAIN-TEST SPLITS ===\n",
      "📅 Using temporal splits...\n",
      "✅ Training set: 32,942 samples\n",
      "✅ Test set: 8,236 samples\n",
      "\n",
      "=== STEP 5: BUILD ADVANCED ENSEMBLE MODEL ===\n",
      "\n",
      "🔧 Training stacking ensemble...\n",
      "✅ Stacking ensemble trained\n",
      "\n",
      "=== STEP 6: COMPREHENSIVE MODEL EVALUATION ===\n",
      "\n",
      "📊 Evaluating Stacking Ensemble...\n",
      "   ✅ Accuracy Metrics:\n",
      "      Top-1: 0.2390 (23.9%)\n",
      "      Top-3: 0.6281 (62.8%)\n",
      "      Top-5: 0.7479 (74.8%)\n",
      "   📈 Business Metrics:\n",
      "      Coverage (Top-3): 0.6281 (62.8%)\n",
      "      Avg Confidence: 0.3320\n",
      "      Recommendation Diversity: 0.0006\n",
      "\n",
      "=== STEP 7: MODEL COMPARISON ===\n",
      "\n",
      "📊 Evaluating RF...\n",
      "   ✅ Accuracy Metrics:\n",
      "      Top-1: 0.2759 (27.6%)\n",
      "      Top-3: 0.7441 (74.4%)\n",
      "      Top-5: 0.8505 (85.1%)\n",
      "   📈 Business Metrics:\n",
      "      Coverage (Top-3): 0.7441 (74.4%)\n",
      "      Avg Confidence: 0.3862\n",
      "      Recommendation Diversity: 0.0006\n",
      "\n",
      "📊 Evaluating XGB...\n",
      "   ✅ Accuracy Metrics:\n",
      "      Top-1: 0.3520 (35.2%)\n",
      "      Top-3: 0.7929 (79.3%)\n",
      "      Top-5: 0.8940 (89.4%)\n",
      "   📈 Business Metrics:\n",
      "      Coverage (Top-3): 0.7929 (79.3%)\n",
      "      Avg Confidence: 0.5868\n",
      "      Recommendation Diversity: 0.0006\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000722 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1090\n",
      "[LightGBM] [Info] Number of data points in the train set: 32942, number of used features: 18\n",
      "[LightGBM] [Info] Start training from score -2.708050\n",
      "[LightGBM] [Info] Start training from score -2.708050\n",
      "[LightGBM] [Info] Start training from score -2.708050\n",
      "[LightGBM] [Info] Start training from score -2.708050\n",
      "[LightGBM] [Info] Start training from score -2.708050\n",
      "[LightGBM] [Info] Start training from score -2.708050\n",
      "[LightGBM] [Info] Start training from score -2.708050\n",
      "[LightGBM] [Info] Start training from score -2.708050\n",
      "[LightGBM] [Info] Start training from score -2.708050\n",
      "[LightGBM] [Info] Start training from score -2.708050\n",
      "[LightGBM] [Info] Start training from score -2.708050\n",
      "[LightGBM] [Info] Start training from score -2.708050\n",
      "[LightGBM] [Info] Start training from score -2.708050\n",
      "[LightGBM] [Info] Start training from score -2.708050\n",
      "[LightGBM] [Info] Start training from score -2.708050\n",
      "\n",
      "📊 Evaluating LGBM...\n",
      "   ✅ Accuracy Metrics:\n",
      "      Top-1: 0.2991 (29.9%)\n",
      "      Top-3: 0.7685 (76.8%)\n",
      "      Top-5: 0.8809 (88.1%)\n",
      "   📈 Business Metrics:\n",
      "      Coverage (Top-3): 0.7685 (76.8%)\n",
      "      Avg Confidence: 0.6022\n",
      "      Recommendation Diversity: 0.0006\n",
      "\n",
      "📊 MODEL PERFORMANCE COMPARISON:\n",
      "                   accuracy  top1_accuracy  top3_accuracy  top5_accuracy  \\\n",
      "Stacking Ensemble    0.2390         0.2390         0.6281         0.7479   \n",
      "RF                   0.2759         0.2759         0.7441         0.8505   \n",
      "XGB                  0.3520         0.3520         0.7929         0.8940   \n",
      "LGBM                 0.2991         0.2991         0.7685         0.8809   \n",
      "\n",
      "                   coverage  avg_confidence  diversity_score  \n",
      "Stacking Ensemble    0.6281          0.3320           0.0006  \n",
      "RF                   0.7441          0.3862           0.0006  \n",
      "XGB                  0.7929          0.5868           0.0006  \n",
      "LGBM                 0.7685          0.6022           0.0006  \n",
      "\n",
      "🏆 Best model: XGB with 79.3% Top-3 accuracy\n",
      "\n",
      "=== STEP 8: TRAIN FINAL MODEL ON FULL DATASET ===\n",
      "🔧 Training final ensemble on full dataset...\n",
      "\n",
      "📊 Evaluating Final Ensemble (Full Data)...\n",
      "   ✅ Accuracy Metrics:\n",
      "      Top-1: 0.4608 (46.1%)\n",
      "      Top-3: 0.6950 (69.5%)\n",
      "      Top-5: 0.8077 (80.8%)\n",
      "   📈 Business Metrics:\n",
      "      Coverage (Top-3): 0.6950 (69.5%)\n",
      "      Avg Confidence: 0.4756\n",
      "      Recommendation Diversity: 0.0006\n",
      "\n",
      "=== STEP 9: PROBABILITY CALIBRATION ===\n",
      "🎯 Calibrating probabilities for reliable confidence scores...\n",
      "\n",
      "📊 Evaluating Calibrated Ensemble...\n",
      "   ✅ Accuracy Metrics:\n",
      "      Top-1: 0.4933 (49.3%)\n",
      "      Top-3: 0.7972 (79.7%)\n",
      "      Top-5: 0.8959 (89.6%)\n",
      "   📈 Business Metrics:\n",
      "      Coverage (Top-3): 0.7972 (79.7%)\n",
      "      Avg Confidence: 0.4185\n",
      "      Recommendation Diversity: 0.0006\n",
      "\n",
      "=== STEP 10: FEATURE IMPORTANCE ANALYSIS ===\n",
      "\n",
      "🏆 TOP 15 MOST IMPORTANT FEATURES:\n",
      "    1. Client_Tenure_Years           : 0.1481\n",
      "    2. Days_Since_Last_FNA           : 0.1425\n",
      "    3. ClientAge                     : 0.1347\n",
      "    4. Engagement_Score              : 0.0862\n",
      "    5. Nationality_encoded           : 0.0754\n",
      "    6. Estimated_Net_Worth           : 0.0636\n",
      "    7. Total_Liquid_Assets           : 0.0611\n",
      "    8. Education                     : 0.0437\n",
      "    9. Income_Numeric                : 0.0423\n",
      "   10. MaritalStatus                 : 0.0311\n",
      "   11. Digital_Adoption_Score        : 0.0296\n",
      "   12. Years_With_Insurance          : 0.0283\n",
      "   13. IncomeRange                   : 0.0282\n",
      "   14. ClientGender                  : 0.0245\n",
      "   15. Family_Protection_Priority    : 0.0186\n",
      "\n",
      "=== STEP 11: CREATE ENHANCED PREDICTION FUNCTION ===\n",
      "\n",
      "🧪 Testing enhanced prediction function...\n",
      "   Sample prediction:\n",
      "   1. SHIELD: 0.396 (39.6%) - Comprehensive financial planning\n",
      "   2. Long_Term_Care_Plans: 0.347 (34.7%) - Basic protection need identified\n",
      "   3. Accident_and_Health_Plans: 0.114 (11.4%) - Comprehensive financial planning\n",
      "\n",
      "=== STEP 12: SAVE ENHANCED MODEL PACKAGE ===\n",
      "💾 Saved: ENHANCED_SUBCATEGORY_MODEL_V3.pkl\n",
      "💾 Saved: ENHANCED_MODEL_REPORT_V3.txt\n",
      "💾 Saved: ENHANCED_FEATURE_IMPORTANCE_V3.xlsx\n",
      "\n",
      "=== ENHANCED PHASE 3 COMPLETE ===\n",
      "\n",
      "🎉 FINAL MODEL PERFORMANCE:\n",
      "✅ Top-1 Accuracy: 49.3%\n",
      "✅ Top-3 Accuracy: 79.7%\n",
      "✅ Top-5 Accuracy: 89.6%\n",
      "✅ Business Coverage: 79.7%\n",
      "✅ Model Type: Calibrated Stacking Ensemble\n",
      "\n",
      "🚀 KEY IMPROVEMENTS:\n",
      "   • Advanced ensemble architecture (3 base models + meta-learner)\n",
      "   • Enhanced feature engineering (behavioral, temporal, business)\n",
      "   • Probability calibration for reliable confidence scores\n",
      "   • Business logic integration in predictions\n",
      "   • Comprehensive evaluation metrics\n",
      "\n",
      "📁 DELIVERABLES:\n",
      "   • ENHANCED_SUBCATEGORY_MODEL_V3.pkl (production-ready model)\n",
      "   • ENHANCED_MODEL_REPORT_V3.txt (detailed performance)\n",
      "   • ENHANCED_FEATURE_IMPORTANCE_V3.xlsx (feature analysis)\n",
      "   • feature_importance_enhanced.png (visualization)\n",
      "\n",
      "💡 USAGE EXAMPLE:\n",
      "\n",
      "# Load model\n",
      "import joblib\n",
      "model_package = joblib.load('ENHANCED_SUBCATEGORY_MODEL_V3.pkl')\n",
      "\n",
      "# Prepare client data\n",
      "client_data = pd.DataFrame({...})  # Client features\n",
      "\n",
      "# Make predictions with business logic\n",
      "predictions = predict_with_business_logic(\n",
      "    client_data,\n",
      "    model_package['model'],\n",
      "    model_package['target_encoder'],\n",
      "    model_package['selected_features'],\n",
      "    model_package['scaler'],\n",
      "    k=3\n",
      ")\n",
      "\n",
      "# Display recommendations\n",
      "for i, (subcategory, confidence, reasoning) in enumerate(predictions, 1):\n",
      "    print(f\"{i}. {subcategory}: {confidence:.1%} - {reasoning}\")\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "=== ENHANCED PHASE 3: ADVANCED ML MODEL TRAINING ===\n",
    "Enhanced version with ensemble methods, better feature selection, and business metrics\n",
    "\"\"\"\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, StackingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, StratifiedKFold\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "from sklearn.feature_selection import mutual_info_classif, SelectKBest\n",
    "from xgboost import XGBClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import joblib\n",
    "from datetime import datetime\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"=== ENHANCED PHASE 3: ADVANCED ML MODEL TRAINING ===\")\n",
    "print(\"Goal: Build high-performance ensemble model with business metrics\")\n",
    "\n",
    "# Step 1: Load Enhanced Training Data\n",
    "print(\"\\n=== STEP 1: LOAD ENHANCED TRAINING DATA ===\")\n",
    "\n",
    "# Load datasets\n",
    "try:\n",
    "    full_data = pd.read_excel('ML_TRAINING_ENHANCED_FULL_V3.xlsx')\n",
    "    balanced_data = pd.read_excel('ML_TRAINING_ENHANCED_BALANCED_V3.xlsx')\n",
    "    \n",
    "    # Try to load temporal splits if available\n",
    "    try:\n",
    "        temporal_train = pd.read_excel('ML_TRAINING_TEMPORAL_TRAIN_V3.xlsx')\n",
    "        temporal_test = pd.read_excel('ML_TRAINING_TEMPORAL_TEST_V3.xlsx')\n",
    "        has_temporal = True\n",
    "    except:\n",
    "        has_temporal = False\n",
    "        print(\"   ℹ️  Temporal splits not available\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"   ⚠️  Using fallback to original datasets: {e}\")\n",
    "    full_data = pd.read_excel('ML_TRAINING_SUBCATEGORIES_FULL_V2.xlsx')\n",
    "    balanced_data = pd.read_excel('ML_TRAINING_SUBCATEGORIES_BALANCED_V2.xlsx')\n",
    "    has_temporal = False\n",
    "\n",
    "print(f\"✅ Full dataset: {len(full_data):,} records, {full_data['Target_SubCategory'].nunique()} classes\")\n",
    "print(f\"✅ Balanced dataset: {len(balanced_data):,} records, {balanced_data['Target_SubCategory'].nunique()} classes\")\n",
    "if has_temporal:\n",
    "    print(f\"✅ Temporal train: {len(temporal_train):,} records\")\n",
    "    print(f\"✅ Temporal test: {len(temporal_test):,} records\")\n",
    "\n",
    "# Step 2: Enhanced Feature Engineering\n",
    "print(\"\\n=== STEP 2: ENHANCED FEATURE PREPARATION ===\")\n",
    "\n",
    "def prepare_enhanced_features(df):\n",
    "    \"\"\"Prepare features with enhanced encoding and scaling\"\"\"\n",
    "    \n",
    "    print(f\"🔧 Preparing features for {len(df):,} records...\")\n",
    "    \n",
    "    # Separate feature types\n",
    "    categorical_features = []\n",
    "    numerical_features = []\n",
    "    binary_features = []\n",
    "    \n",
    "    # Identify feature types\n",
    "    for col in df.columns:\n",
    "        if col in ['Target_SubCategory', 'ClientId', 'DateCreated']:\n",
    "            continue\n",
    "            \n",
    "        if df[col].dtype == 'object':\n",
    "            categorical_features.append(col)\n",
    "        elif df[col].nunique() == 2:\n",
    "            binary_features.append(col)\n",
    "        else:\n",
    "            numerical_features.append(col)\n",
    "    \n",
    "    print(f\"   📊 Features identified:\")\n",
    "    print(f\"      Numerical: {len(numerical_features)}\")\n",
    "    print(f\"      Categorical: {len(categorical_features)}\")\n",
    "    print(f\"      Binary: {len(binary_features)}\")\n",
    "    \n",
    "    X = pd.DataFrame(index=df.index)\n",
    "    encoders = {}\n",
    "    \n",
    "    # Process numerical features with outlier handling\n",
    "    for feature in numerical_features:\n",
    "        if feature in df.columns:\n",
    "            # Handle outliers using IQR method\n",
    "            Q1 = df[feature].quantile(0.25)\n",
    "            Q3 = df[feature].quantile(0.75)\n",
    "            IQR = Q3 - Q1\n",
    "            lower_bound = Q1 - 1.5 * IQR\n",
    "            upper_bound = Q3 + 1.5 * IQR\n",
    "            \n",
    "            # Clip outliers\n",
    "            X[feature] = df[feature].clip(lower=lower_bound, upper=upper_bound)\n",
    "            \n",
    "            # Fill missing with median\n",
    "            median_val = X[feature].median()\n",
    "            X[feature] = X[feature].fillna(median_val)\n",
    "    \n",
    "    # Process binary features\n",
    "    for feature in binary_features:\n",
    "        if feature in df.columns:\n",
    "            X[feature] = df[feature].fillna(0).astype(int)\n",
    "    \n",
    "    # Process categorical features with target encoding for high cardinality\n",
    "    for feature in categorical_features:\n",
    "        if feature in df.columns:\n",
    "            if df[feature].nunique() > 20:\n",
    "                # Use target encoding for high cardinality\n",
    "                print(f\"      Using target encoding for {feature} ({df[feature].nunique()} unique values)\")\n",
    "                # Simple mean encoding (in production, use cross-validation)\n",
    "                target_mean = df.groupby(feature)['Target_SubCategory'].apply(\n",
    "                    lambda x: x.value_counts().index[0] if len(x) > 0 else 'Unknown'\n",
    "                )\n",
    "                X[feature + '_encoded'] = df[feature].map(target_mean).fillna('Unknown')\n",
    "                \n",
    "                # Convert to numeric\n",
    "                le = LabelEncoder()\n",
    "                X[feature + '_encoded'] = le.fit_transform(X[feature + '_encoded'].astype(str))\n",
    "                encoders[feature] = {'type': 'target', 'mapping': target_mean, 'le': le}\n",
    "            else:\n",
    "                # Use label encoding for low cardinality\n",
    "                le = LabelEncoder()\n",
    "                X[feature] = le.fit_transform(df[feature].fillna('Unknown').astype(str))\n",
    "                encoders[feature] = {'type': 'label', 'le': le}\n",
    "    \n",
    "    # Encode target variable\n",
    "    target_encoder = LabelEncoder()\n",
    "    y = target_encoder.fit_transform(df['Target_SubCategory'])\n",
    "    \n",
    "    print(f\"   ✅ Feature matrix: {X.shape}\")\n",
    "    \n",
    "    return X, y, encoders, target_encoder\n",
    "\n",
    "# Prepare features for all datasets\n",
    "X_full, y_full, encoders_full, target_encoder_full = prepare_enhanced_features(full_data)\n",
    "X_balanced, y_balanced, encoders_balanced, target_encoder_balanced = prepare_enhanced_features(balanced_data)\n",
    "\n",
    "if has_temporal:\n",
    "    X_temporal_train, y_temporal_train, _, _ = prepare_enhanced_features(temporal_train)\n",
    "    X_temporal_test, y_temporal_test, _, _ = prepare_enhanced_features(temporal_test)\n",
    "\n",
    "# Step 3: Advanced Feature Selection\n",
    "print(\"\\n=== STEP 3: ADVANCED FEATURE SELECTION ===\")\n",
    "\n",
    "def select_diverse_features(X, y, n_features=30, correlation_threshold=0.85):\n",
    "    \"\"\"Select features using mutual information and correlation filtering\"\"\"\n",
    "    \n",
    "    print(f\"🔍 Selecting top {n_features} diverse features...\")\n",
    "    \n",
    "    # Calculate mutual information\n",
    "    mi_scores = mutual_info_classif(X, y, random_state=42)\n",
    "    mi_df = pd.DataFrame({\n",
    "        'feature': X.columns,\n",
    "        'mi_score': mi_scores\n",
    "    }).sort_values('mi_score', ascending=False)\n",
    "    \n",
    "    # Calculate correlation matrix\n",
    "    corr_matrix = X.corr().abs()\n",
    "    \n",
    "    # Select features iteratively to avoid high correlation\n",
    "    selected_features = []\n",
    "    for _, row in mi_df.iterrows():\n",
    "        feature = row['feature']\n",
    "        \n",
    "        # Check correlation with already selected features\n",
    "        if selected_features:\n",
    "            max_corr = corr_matrix.loc[feature, selected_features].max()\n",
    "            if max_corr < correlation_threshold:\n",
    "                selected_features.append(feature)\n",
    "        else:\n",
    "            selected_features.append(feature)\n",
    "        \n",
    "        if len(selected_features) >= n_features:\n",
    "            break\n",
    "    \n",
    "    print(f\"   ✅ Selected {len(selected_features)} features with MI scores and low correlation\")\n",
    "    \n",
    "    # Show top features\n",
    "    print(f\"\\n   🏆 TOP 10 SELECTED FEATURES:\")\n",
    "    for i, feat in enumerate(selected_features[:10], 1):\n",
    "        mi_score = mi_df[mi_df['feature'] == feat]['mi_score'].values[0]\n",
    "        print(f\"      {i:2d}. {feat}: MI={mi_score:.4f}\")\n",
    "    \n",
    "    return selected_features\n",
    "\n",
    "# Select features using balanced dataset\n",
    "selected_features = select_diverse_features(X_balanced, y_balanced, n_features=40)\n",
    "\n",
    "# Step 4: Create Train-Test Splits\n",
    "print(\"\\n=== STEP 4: CREATE TRAIN-TEST SPLITS ===\")\n",
    "\n",
    "if has_temporal:\n",
    "    print(\"📅 Using temporal splits...\")\n",
    "    X_train = X_temporal_train[selected_features]\n",
    "    X_test = X_temporal_test[selected_features]\n",
    "    y_train = y_temporal_train\n",
    "    y_test = y_temporal_test\n",
    "else:\n",
    "    print(\"📊 Creating stratified random splits...\")\n",
    "    # Use balanced dataset for development\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X_balanced[selected_features], y_balanced,\n",
    "        test_size=0.2,\n",
    "        random_state=42,\n",
    "        stratify=y_balanced\n",
    "    )\n",
    "\n",
    "# Also prepare full dataset split for final training\n",
    "X_full_train, X_full_test, y_full_train, y_full_test = train_test_split(\n",
    "    X_full[selected_features], y_full,\n",
    "    test_size=0.2,\n",
    "    random_state=42,\n",
    "    stratify=y_full\n",
    ")\n",
    "\n",
    "print(f\"✅ Training set: {len(X_train):,} samples\")\n",
    "print(f\"✅ Test set: {len(X_test):,} samples\")\n",
    "\n",
    "# Step 5: Build Ensemble Model\n",
    "print(\"\\n=== STEP 5: BUILD ADVANCED ENSEMBLE MODEL ===\")\n",
    "\n",
    "# Scale features for some models\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Define base models with optimized parameters\n",
    "base_models = [\n",
    "    ('rf', RandomForestClassifier(\n",
    "        n_estimators=300,\n",
    "        max_depth=20,\n",
    "        min_samples_split=20,\n",
    "        min_samples_leaf=10,\n",
    "        max_features='sqrt',\n",
    "        class_weight='balanced',\n",
    "        random_state=42,\n",
    "        n_jobs=-1\n",
    "    )),\n",
    "    ('xgb', XGBClassifier(\n",
    "        n_estimators=200,\n",
    "        max_depth=10,\n",
    "        learning_rate=0.1,\n",
    "        subsample=0.8,\n",
    "        colsample_bytree=0.8,\n",
    "        scale_pos_weight=1,\n",
    "        random_state=42,\n",
    "        n_jobs=-1\n",
    "    )),\n",
    "    ('lgbm', LGBMClassifier(\n",
    "        n_estimators=200,\n",
    "        max_depth=15,\n",
    "        learning_rate=0.1,\n",
    "        num_leaves=31,\n",
    "        min_child_samples=20,\n",
    "        subsample=0.8,\n",
    "        colsample_bytree=0.8,\n",
    "        class_weight='balanced',\n",
    "        random_state=42,\n",
    "        n_jobs=-1\n",
    "    ))\n",
    "]\n",
    "\n",
    "# Create stacking ensemble\n",
    "print(\"\\n🔧 Training stacking ensemble...\")\n",
    "meta_learner = LogisticRegression(\n",
    "    multi_class='multinomial',\n",
    "    max_iter=1000,\n",
    "    class_weight='balanced',\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "stacking_model = StackingClassifier(\n",
    "    estimators=base_models,\n",
    "    final_estimator=meta_learner,\n",
    "    cv=5,  # 5-fold cross-validation for meta-learner\n",
    "    stack_method='predict_proba',\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "# Train ensemble\n",
    "stacking_model.fit(X_train, y_train)\n",
    "print(\"✅ Stacking ensemble trained\")\n",
    "\n",
    "# Step 6: Model Evaluation with Business Metrics\n",
    "print(\"\\n=== STEP 6: COMPREHENSIVE MODEL EVALUATION ===\")\n",
    "\n",
    "def evaluate_model_comprehensive(model, X_test, y_test, target_encoder, model_name=\"Model\"):\n",
    "    \"\"\"Comprehensive evaluation including business metrics\"\"\"\n",
    "    \n",
    "    print(f\"\\n📊 Evaluating {model_name}...\")\n",
    "    \n",
    "    # Make predictions\n",
    "    y_pred = model.predict(X_test)\n",
    "    y_pred_proba = model.predict_proba(X_test)\n",
    "    \n",
    "    # Basic metrics\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    \n",
    "    # Top-k accuracy\n",
    "    def top_k_accuracy(y_true, y_pred_proba, k):\n",
    "        top_k_preds = np.argsort(y_pred_proba, axis=1)[:, -k:]\n",
    "        return np.mean([y_true[i] in top_k_preds[i] for i in range(len(y_true))])\n",
    "    \n",
    "    top1_acc = accuracy\n",
    "    top3_acc = top_k_accuracy(y_test, y_pred_proba, k=3)\n",
    "    top5_acc = top_k_accuracy(y_test, y_pred_proba, k=5)\n",
    "    \n",
    "    # Business metrics\n",
    "    # 1. Coverage metric: What percentage of clients get at least one correct recommendation in top-3\n",
    "    coverage = top3_acc\n",
    "    \n",
    "    # 2. Confidence calibration: How well-calibrated are the probabilities\n",
    "    top_confidences = np.max(y_pred_proba, axis=1)\n",
    "    avg_confidence = np.mean(top_confidences)\n",
    "    \n",
    "    # 3. Diversity metric: How diverse are the recommendations\n",
    "    top3_preds = np.argsort(y_pred_proba, axis=1)[:, -3:]\n",
    "    unique_recommendations = len(np.unique(top3_preds))\n",
    "    diversity_score = unique_recommendations / (3 * len(y_test))\n",
    "    \n",
    "    results = {\n",
    "        'accuracy': accuracy,\n",
    "        'top1_accuracy': top1_acc,\n",
    "        'top3_accuracy': top3_acc,\n",
    "        'top5_accuracy': top5_acc,\n",
    "        'coverage': coverage,\n",
    "        'avg_confidence': avg_confidence,\n",
    "        'diversity_score': diversity_score\n",
    "    }\n",
    "    \n",
    "    print(f\"   ✅ Accuracy Metrics:\")\n",
    "    print(f\"      Top-1: {top1_acc:.4f} ({top1_acc*100:.1f}%)\")\n",
    "    print(f\"      Top-3: {top3_acc:.4f} ({top3_acc*100:.1f}%)\")\n",
    "    print(f\"      Top-5: {top5_acc:.4f} ({top5_acc*100:.1f}%)\")\n",
    "    \n",
    "    print(f\"   📈 Business Metrics:\")\n",
    "    print(f\"      Coverage (Top-3): {coverage:.4f} ({coverage*100:.1f}%)\")\n",
    "    print(f\"      Avg Confidence: {avg_confidence:.4f}\")\n",
    "    print(f\"      Recommendation Diversity: {diversity_score:.4f}\")\n",
    "    \n",
    "    return results, y_pred_proba\n",
    "\n",
    "# Evaluate stacking ensemble\n",
    "ensemble_results, ensemble_proba = evaluate_model_comprehensive(\n",
    "    stacking_model, X_test, y_test, target_encoder_balanced, \"Stacking Ensemble\"\n",
    ")\n",
    "\n",
    "# Step 7: Compare with Individual Models\n",
    "print(\"\\n=== STEP 7: MODEL COMPARISON ===\")\n",
    "\n",
    "model_comparison = {'Stacking Ensemble': ensemble_results}\n",
    "\n",
    "# Train and evaluate individual models for comparison\n",
    "for name, model in base_models:\n",
    "    model.fit(X_train, y_train)\n",
    "    results, _ = evaluate_model_comprehensive(model, X_test, y_test, target_encoder_balanced, name.upper())\n",
    "    model_comparison[name.upper()] = results\n",
    "\n",
    "# Create comparison DataFrame\n",
    "comparison_df = pd.DataFrame(model_comparison).T\n",
    "print(\"\\n📊 MODEL PERFORMANCE COMPARISON:\")\n",
    "print(comparison_df.round(4))\n",
    "\n",
    "# Select best model\n",
    "best_model_name = comparison_df['top3_accuracy'].idxmax()\n",
    "print(f\"\\n🏆 Best model: {best_model_name} with {comparison_df.loc[best_model_name, 'top3_accuracy']*100:.1f}% Top-3 accuracy\")\n",
    "\n",
    "# Step 8: Train Final Model on Full Dataset\n",
    "print(\"\\n=== STEP 8: TRAIN FINAL MODEL ON FULL DATASET ===\")\n",
    "\n",
    "print(\"🔧 Training final ensemble on full dataset...\")\n",
    "final_model = StackingClassifier(\n",
    "    estimators=base_models,\n",
    "    final_estimator=meta_learner,\n",
    "    cv=5,\n",
    "    stack_method='predict_proba',\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "# Scale full dataset\n",
    "X_full_train_scaled = scaler.fit_transform(X_full_train)\n",
    "X_full_test_scaled = scaler.transform(X_full_test)\n",
    "\n",
    "# Train on full dataset\n",
    "final_model.fit(X_full_train, y_full_train)\n",
    "\n",
    "# Evaluate on full test set\n",
    "final_results, final_proba = evaluate_model_comprehensive(\n",
    "    final_model, X_full_test, y_full_test, target_encoder_full, \"Final Ensemble (Full Data)\"\n",
    ")\n",
    "\n",
    "# Step 9: Probability Calibration\n",
    "print(\"\\n=== STEP 9: PROBABILITY CALIBRATION ===\")\n",
    "\n",
    "print(\"🎯 Calibrating probabilities for reliable confidence scores...\")\n",
    "calibrated_model = CalibratedClassifierCV(\n",
    "    final_model, \n",
    "    method='sigmoid', \n",
    "    cv=3\n",
    ")\n",
    "calibrated_model.fit(X_full_train, y_full_train)\n",
    "\n",
    "# Evaluate calibrated model\n",
    "cal_results, cal_proba = evaluate_model_comprehensive(\n",
    "    calibrated_model, X_full_test, y_full_test, target_encoder_full, \"Calibrated Ensemble\"\n",
    ")\n",
    "\n",
    "# Step 10: Feature Importance Analysis\n",
    "print(\"\\n=== STEP 10: FEATURE IMPORTANCE ANALYSIS ===\")\n",
    "\n",
    "# Get feature importance from Random Forest in the ensemble\n",
    "rf_model = dict(base_models)['rf']\n",
    "rf_model.fit(X_full_train, y_full_train)\n",
    "\n",
    "feature_importance = pd.DataFrame({\n",
    "    'Feature': selected_features,\n",
    "    'Importance': rf_model.feature_importances_\n",
    "}).sort_values('Importance', ascending=False)\n",
    "\n",
    "print(\"\\n🏆 TOP 15 MOST IMPORTANT FEATURES:\")\n",
    "for i, (_, row) in enumerate(feature_importance.head(15).iterrows(), 1):\n",
    "    print(f\"   {i:2d}. {row['Feature']:<30}: {row['Importance']:.4f}\")\n",
    "\n",
    "# Visualize feature importance\n",
    "plt.figure(figsize=(10, 8))\n",
    "top_features_plot = feature_importance.head(20)\n",
    "plt.barh(range(len(top_features_plot)), top_features_plot['Importance'])\n",
    "plt.yticks(range(len(top_features_plot)), top_features_plot['Feature'])\n",
    "plt.xlabel('Importance Score')\n",
    "plt.title('Top 20 Feature Importances')\n",
    "plt.tight_layout()\n",
    "plt.savefig('feature_importance_enhanced.png', dpi=300, bbox_inches='tight')\n",
    "plt.close()\n",
    "\n",
    "# Step 11: Create Enhanced Prediction Function\n",
    "print(\"\\n=== STEP 11: CREATE ENHANCED PREDICTION FUNCTION ===\")\n",
    "\n",
    "def predict_with_business_logic(client_features, model, target_encoder, selected_features, \n",
    "                               scaler=None, k=3, min_confidence=0.1):\n",
    "    \"\"\"\n",
    "    Enhanced prediction function with business logic\n",
    "    \n",
    "    Returns:\n",
    "        List of tuples: [(subcategory, confidence, reasoning), ...]\n",
    "    \"\"\"\n",
    "    \n",
    "    # Ensure features are in correct order\n",
    "    X_pred = client_features[selected_features]\n",
    "    \n",
    "    # Scale if scaler provided\n",
    "    if scaler:\n",
    "        X_pred = scaler.transform(X_pred)\n",
    "    \n",
    "    # Get prediction probabilities\n",
    "    pred_proba = model.predict_proba(X_pred)[0]\n",
    "    \n",
    "    # Get top-k predictions with minimum confidence threshold\n",
    "    valid_indices = np.where(pred_proba >= min_confidence)[0]\n",
    "    if len(valid_indices) < k:\n",
    "        # If not enough predictions meet threshold, take top k anyway\n",
    "        top_k_indices = np.argsort(pred_proba)[-k:][::-1]\n",
    "    else:\n",
    "        # Take top k from valid predictions\n",
    "        valid_proba = pred_proba[valid_indices]\n",
    "        valid_top_k = np.argsort(valid_proba)[-k:][::-1]\n",
    "        top_k_indices = valid_indices[valid_top_k]\n",
    "    \n",
    "    # Get subcategories and confidences\n",
    "    top_k_subcategories = target_encoder.inverse_transform(top_k_indices)\n",
    "    top_k_confidences = pred_proba[top_k_indices]\n",
    "    \n",
    "    # Add business reasoning\n",
    "    predictions_with_reasoning = []\n",
    "    for subcat, conf in zip(top_k_subcategories, top_k_confidences):\n",
    "        # Add simple reasoning based on subcategory\n",
    "        if 'Term' in subcat:\n",
    "            reasoning = \"Basic protection need identified\"\n",
    "        elif 'Investment' in subcat:\n",
    "            reasoning = \"Wealth accumulation opportunity\"\n",
    "        elif 'Shield' in subcat or 'Hospital' in subcat:\n",
    "            reasoning = \"Healthcare coverage gap\"\n",
    "        elif 'Critical' in subcat:\n",
    "            reasoning = \"Critical illness protection recommended\"\n",
    "        elif 'Whole_Life' in subcat:\n",
    "            reasoning = \"Long-term protection and savings\"\n",
    "        elif 'Retirement' in subcat or 'Annuity' in subcat:\n",
    "            reasoning = \"Retirement planning need\"\n",
    "        else:\n",
    "            reasoning = \"Comprehensive financial planning\"\n",
    "        \n",
    "        predictions_with_reasoning.append((subcat, conf, reasoning))\n",
    "    \n",
    "    return predictions_with_reasoning\n",
    "\n",
    "# Test enhanced prediction function\n",
    "print(\"\\n🧪 Testing enhanced prediction function...\")\n",
    "test_sample = X_full_test.iloc[[0]]\n",
    "predictions = predict_with_business_logic(\n",
    "    test_sample, calibrated_model, target_encoder_full, \n",
    "    selected_features, scaler, k=3\n",
    ")\n",
    "\n",
    "print(\"   Sample prediction:\")\n",
    "for i, (subcat, conf, reasoning) in enumerate(predictions, 1):\n",
    "    print(f\"   {i}. {subcat}: {conf:.3f} ({conf*100:.1f}%) - {reasoning}\")\n",
    "\n",
    "# Step 12: Save Enhanced Model Package\n",
    "print(\"\\n=== STEP 12: SAVE ENHANCED MODEL PACKAGE ===\")\n",
    "\n",
    "# Create comprehensive model package\n",
    "model_package = {\n",
    "    'model': calibrated_model,\n",
    "    'target_encoder': target_encoder_full,\n",
    "    'feature_encoders': encoders_full,\n",
    "    'selected_features': selected_features,\n",
    "    'scaler': scaler,\n",
    "    'model_performance': cal_results,\n",
    "    'comparison_results': comparison_df.to_dict(),\n",
    "    'feature_importance': feature_importance.to_dict(),\n",
    "    'training_date': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),\n",
    "    'model_type': 'Calibrated Stacking Ensemble',\n",
    "    'base_models': [name for name, _ in base_models]\n",
    "}\n",
    "\n",
    "# Save model package\n",
    "joblib.dump(model_package, 'ENHANCED_SUBCATEGORY_MODEL_V3.pkl')\n",
    "print(\"💾 Saved: ENHANCED_SUBCATEGORY_MODEL_V3.pkl\")\n",
    "\n",
    "# Save detailed performance report\n",
    "with open('ENHANCED_MODEL_REPORT_V3.txt', 'w') as f:\n",
    "    f.write(\"=== ENHANCED ML MODEL PERFORMANCE REPORT ===\\n\")\n",
    "    f.write(f\"Generated: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\\n\\n\")\n",
    "    \n",
    "    f.write(\"MODEL ARCHITECTURE:\\n\")\n",
    "    f.write(f\"  Type: Calibrated Stacking Ensemble\\n\")\n",
    "    f.write(f\"  Base Models: {', '.join([name.upper() for name, _ in base_models])}\\n\")\n",
    "    f.write(f\"  Meta Learner: Multinomial Logistic Regression\\n\")\n",
    "    f.write(f\"  Calibration: Platt Scaling (Sigmoid)\\n\\n\")\n",
    "    \n",
    "    f.write(\"PERFORMANCE METRICS:\\n\")\n",
    "    for metric, value in cal_results.items():\n",
    "        f.write(f\"  {metric}: {value:.4f} ({value*100:.1f}%)\\n\")\n",
    "    \n",
    "    f.write(\"\\nMODEL COMPARISON:\\n\")\n",
    "    f.write(comparison_df.round(4).to_string())\n",
    "    \n",
    "    f.write(\"\\n\\nTOP 15 FEATURES:\\n\")\n",
    "    for i, (_, row) in enumerate(feature_importance.head(15).iterrows(), 1):\n",
    "        f.write(f\"  {i:2d}. {row['Feature']}: {row['Importance']:.4f}\\n\")\n",
    "    \n",
    "    f.write(f\"\\nTRAINING DETAILS:\\n\")\n",
    "    f.write(f\"  Training samples: {len(X_full_train):,}\\n\")\n",
    "    f.write(f\"  Test samples: {len(X_full_test):,}\\n\")\n",
    "    f.write(f\"  Number of features: {len(selected_features)}\\n\")\n",
    "    f.write(f\"  Number of classes: {len(target_encoder_full.classes_)}\\n\")\n",
    "\n",
    "print(\"💾 Saved: ENHANCED_MODEL_REPORT_V3.txt\")\n",
    "\n",
    "# Save feature importance\n",
    "feature_importance.to_excel('ENHANCED_FEATURE_IMPORTANCE_V3.xlsx', index=False)\n",
    "print(\"💾 Saved: ENHANCED_FEATURE_IMPORTANCE_V3.xlsx\")\n",
    "\n",
    "# Step 13: Final Summary\n",
    "print(\"\\n=== ENHANCED PHASE 3 COMPLETE ===\")\n",
    "print(f\"\\n🎉 FINAL MODEL PERFORMANCE:\")\n",
    "print(f\"✅ Top-1 Accuracy: {cal_results['top1_accuracy']*100:.1f}%\")\n",
    "print(f\"✅ Top-3 Accuracy: {cal_results['top3_accuracy']*100:.1f}%\")\n",
    "print(f\"✅ Top-5 Accuracy: {cal_results['top5_accuracy']*100:.1f}%\")\n",
    "print(f\"✅ Business Coverage: {cal_results['coverage']*100:.1f}%\")\n",
    "print(f\"✅ Model Type: Calibrated Stacking Ensemble\")\n",
    "\n",
    "print(f\"\\n🚀 KEY IMPROVEMENTS:\")\n",
    "print(f\"   • Advanced ensemble architecture (3 base models + meta-learner)\")\n",
    "print(f\"   • Enhanced feature engineering (behavioral, temporal, business)\")\n",
    "print(f\"   • Probability calibration for reliable confidence scores\")\n",
    "print(f\"   • Business logic integration in predictions\")\n",
    "print(f\"   • Comprehensive evaluation metrics\")\n",
    "\n",
    "print(f\"\\n📁 DELIVERABLES:\")\n",
    "print(f\"   • ENHANCED_SUBCATEGORY_MODEL_V3.pkl (production-ready model)\")\n",
    "print(f\"   • ENHANCED_MODEL_REPORT_V3.txt (detailed performance)\")\n",
    "print(f\"   • ENHANCED_FEATURE_IMPORTANCE_V3.xlsx (feature analysis)\")\n",
    "print(f\"   • feature_importance_enhanced.png (visualization)\")\n",
    "\n",
    "print(f\"\\n💡 USAGE EXAMPLE:\")\n",
    "print(\"\"\"\n",
    "# Load model\n",
    "import joblib\n",
    "model_package = joblib.load('ENHANCED_SUBCATEGORY_MODEL_V3.pkl')\n",
    "\n",
    "# Prepare client data\n",
    "client_data = pd.DataFrame({...})  # Client features\n",
    "\n",
    "# Make predictions with business logic\n",
    "predictions = predict_with_business_logic(\n",
    "    client_data,\n",
    "    model_package['model'],\n",
    "    model_package['target_encoder'],\n",
    "    model_package['selected_features'],\n",
    "    model_package['scaler'],\n",
    "    k=3\n",
    ")\n",
    "\n",
    "# Display recommendations\n",
    "for i, (subcategory, confidence, reasoning) in enumerate(predictions, 1):\n",
    "    print(f\"{i}. {subcategory}: {confidence:.1%} - {reasoning}\")\n",
    "\"\"\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
