{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5cf08e90-f3f8-4fe9-a27e-820859a602fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== LOADING ALL DATASETS ===\n",
      "Loading client... âœ“ (45,688 rows, 49 columns)\n",
      "Loading provider... âœ“ (128 rows, 21 columns)\n",
      "Loading emfc2fna... âœ“ (51,772 rows, 31 columns)\n",
      "Loading emfc2personalinformation... âœ“ (52,305 rows, 37 columns)\n",
      "Loading emfc2... âœ“ (51,769 rows, 8 columns)\n",
      "Loading EMFC2Assets... âœ“ (50,500 rows, 39 columns)\n",
      "Loading emfc2portofolioinsurance... âœ“ (27,437 rows, 25 columns)\n",
      "Loading emfc2productsolution... âœ“ (43,501 rows, 25 columns)\n",
      "Loading EMFC2ProductIntegrationApplication... âœ“ (560 rows, 14 columns)\n",
      "Loading EMFC2ProductIntegrationLog... âœ“ (977 rows, 21 columns)\n",
      "Loading ProductMainPlan... âœ“ (1,532 rows, 22 columns)\n",
      "Loading ProductType... âœ“ (4 rows, 8 columns)\n",
      "Loading ProductCategory... âœ“ (10 rows, 6 columns)\n",
      "Loading productsubcategory... âœ“ (39 rows, 13 columns)\n",
      "\n",
      "Successfully loaded 14 datasets\n",
      "Available datasets: ['client', 'provider', 'emfc2fna', 'emfc2personalinformation', 'emfc2', 'EMFC2Assets', 'emfc2portofolioinsurance', 'emfc2productsolution', 'EMFC2ProductIntegrationApplication', 'EMFC2ProductIntegrationLog', 'ProductMainPlan', 'ProductType', 'ProductCategory', 'productsubcategory']\n"
     ]
    }
   ],
   "source": [
    "# Data gathering\n",
    "import pandas as pd\n",
    "import msoffcrypto\n",
    "import io\n",
    "\n",
    "def load_encrypted_excel(file_path: str, password: str) -> pd.DataFrame:\n",
    "    with open(file_path, 'rb') as f:\n",
    "        office_file = msoffcrypto.OfficeFile(f)\n",
    "        office_file.load_key(password=password)\n",
    "        decrypted = io.BytesIO()\n",
    "        office_file.decrypt(decrypted)\n",
    "        decrypted.seek(0)\n",
    "        return pd.read_excel(decrypted)\n",
    "\n",
    "# File configurations\n",
    "files = [\n",
    "    # Core Client & FNA Process Tables\n",
    "    {\"name\": \"client\", \"path\": \"client.xlsx\", \"password\": \"_XlN@a9)EVy1\"},\n",
    "    {\"name\": \"provider\", \"path\": \"provider.xlsx\", \"password\": \"unT4d4GO#dX(\"},\n",
    "    {\"name\": \"emfc2fna\", \"path\": \"emfc2fna.xlsx\", \"password\": \"dQq9T%pC^?22\"},\n",
    "    {\"name\": \"emfc2personalinformation\", \"path\": \"emfc2personalinformation.xlsx\", \"password\": \"ZqYmaFgC@Zv3\"},\n",
    "    {\"name\": \"emfc2\", \"path\": \"emfc2.xlsx\", \"password\": \"79GYEd%l(2Bf\"},\n",
    "    {\"name\": \"EMFC2Assets\", \"path\": \"EMFC2Assets.xlsx\", \"password\": \"!suNZ=%YA13k\"},\n",
    "    {\"name\": \"emfc2portofolioinsurance\", \"path\": \"emfc2portofolioinsurance.xlsx\", \"password\": \"BcxM>wz*(hxF\"},\n",
    "\n",
    "    # Product & Solution Workflow\n",
    "    {\"name\": \"emfc2productsolution\", \"path\": \"emfc2productsolution.xlsx\", \"password\": \"@OFn7oA5!Joe\"},\n",
    "    {\"name\": \"EMFC2ProductIntegrationApplication\", \"path\": \"EMFC2ProductIntegrationApplication.xlsx\", \"password\": \"(FZsw7#vz-bN\"},\n",
    "    {\"name\": \"EMFC2ProductIntegrationLog\", \"path\": \"EMFC2ProductIntegrationLog.xlsx\", \"password\": \"?wcAx*P4n=9&\"},\n",
    "\n",
    "    # Product & Category Lookup Tables\n",
    "    {\"name\": \"ProductMainPlan\", \"path\": \"ProductMainPlan.xlsx\", \"password\": \")XQ4ZDssowrA\"},\n",
    "    {\"name\": \"ProductType\", \"path\": \"ProductType.xlsx\", \"password\": \"#9zCw?^-xTO?\"},\n",
    "    {\"name\": \"ProductCategory\", \"path\": \"ProductCategory.xlsx\", \"password\": \"#F)cdAEOVJ@4\"},\n",
    "    {\"name\": \"productsubcategory\", \"path\": \"productsubcategory.xlsx\", \"password\": \"y-^t$N9>%S%C\"}\n",
    "]\n",
    "\n",
    "# Load all datasets into memory\n",
    "datasets = {}\n",
    "\n",
    "print(\"=== LOADING ALL DATASETS ===\")\n",
    "for file in files:\n",
    "    print(f\"Loading {file['name']}...\", end=\" \")\n",
    "    try:\n",
    "        datasets[file['name']] = load_encrypted_excel(file[\"path\"], file[\"password\"])\n",
    "        shape = datasets[file['name']].shape\n",
    "        print(f\"âœ“ ({shape[0]:,} rows, {shape[1]} columns)\")\n",
    "    except Exception as e:\n",
    "        print(f\"âœ— Error: {e}\")\n",
    "\n",
    "print(f\"\\nSuccessfully loaded {len(datasets)} datasets\")\n",
    "print(\"Available datasets:\", list(datasets.keys()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7b7afcee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\n=== COLUMN HEADERS FOR EACH DATASET ===\\n\n",
      "ðŸ“„ Dataset: client\n",
      "ðŸ§¾ Columns (49):\n",
      "  - # (int64)\n",
      "  - ClientId (object)\n",
      "  - ClientName (object)\n",
      "  - ClientMobileNumber (object)\n",
      "  - ClientMNVerified (bool)\n",
      "  - ClientMNVeriCode (float64)\n",
      "  - ClientMNVeriCodeTime (datetime64[ns])\n",
      "  - ClientEmail (object)\n",
      "  - ClientContactPreferences (object)\n",
      "  - ClientGender (object)\n",
      "  - ClientDOB (datetime64[ns])\n",
      "  - ClientCPFContributionCategoryId (object)\n",
      "  - IDNumber (object)\n",
      "  - Nationality (object)\n",
      "  - SpokenLanguage (object)\n",
      "  - WrittenLanguage (object)\n",
      "  - Education (object)\n",
      "  - EmploymentStatus (object)\n",
      "  - Occupation (object)\n",
      "  - MaritalStatus (object)\n",
      "  - PrimaryAddress (object)\n",
      "  - CorrespondingAddress (object)\n",
      "  - IncomeRange (object)\n",
      "  - AccompaniedbyTrustedIndividual (float64)\n",
      "  - ClientInvitedDate (datetime64[ns])\n",
      "  - ClientStatus (object)\n",
      "  - RiskProfile (object)\n",
      "  - RiskProfileSubmissionDate (datetime64[ns])\n",
      "  - CKAProfile (object)\n",
      "  - CARProfile (object)\n",
      "  - CKACARSubmissionDate (datetime64[ns])\n",
      "  - UserId (object)\n",
      "  - ClientEmailVerified (bool)\n",
      "  - ClientEmailVeriCode (float64)\n",
      "  - ClientEmailVeriCodeTime (datetime64[ns])\n",
      "  - ClientResidentialStatus (object)\n",
      "  - CountryOfBirth (object)\n",
      "  - Race (object)\n",
      "  - UEN (object)\n",
      "  - ClientSource (object)\n",
      "  - LastModifiedTime (datetime64[ns])\n",
      "  - AccreditedInvestor (float64)\n",
      "  - ClientType (object)\n",
      "  - ManuallyRetrieved (bool)\n",
      "  - MyInfoRetrieved (bool)\n",
      "  - NextReviewDate (datetime64[ns])\n",
      "  - AITransactionCapability (bool)\n",
      "  - TempContactPreferences (object)\n",
      "  - SelectedClient (float64)\n",
      "----------------------------------------\n",
      "ðŸ“„ Dataset: provider\n",
      "ðŸ§¾ Columns (21):\n",
      "  - # (int64)\n",
      "  - ProviderId (object)\n",
      "  - ProviderName (object)\n",
      "  - ProviderAlias (object)\n",
      "  - ProviderMainColor (object)\n",
      "  - CompanyRegisteredName (object)\n",
      "  - CompanyLogo (object)\n",
      "  - CompanyURL (object)\n",
      "  - CompanyCode (object)\n",
      "  - CompanyAgentLoginURL (object)\n",
      "  - CompanyUserGuide (object)\n",
      "  - CompanyClientLoginURL (object)\n",
      "  - CompanyAgencyLoginURL (object)\n",
      "  - IsActive (bool)\n",
      "  - ProviderCode (object)\n",
      "  - ProviderShortName (object)\n",
      "  - IntegratedFlowActivated (bool)\n",
      "  - UpdatedBy (object)\n",
      "  - UpdatedDate (datetime64[ns])\n",
      "  - ProviderLabelIds (float64)\n",
      "  - HasProfessionalService (bool)\n",
      "----------------------------------------\n",
      "ðŸ“„ Dataset: emfc2fna\n",
      "ðŸ§¾ Columns (31):\n",
      "  - # (int64)\n",
      "  - EMFC2FNAId (object)\n",
      "  - EMFC2Id (object)\n",
      "  - TradeType (object)\n",
      "  - CoBroke (bool)\n",
      "  - CoBrokeUserId (object)\n",
      "  - CoBrokeSFACode (object)\n",
      "  - JointApplicant (bool)\n",
      "  - JointApplicantClientId (object)\n",
      "  - StartTime (datetime64[ns])\n",
      "  - SubmissionTime (datetime64[ns])\n",
      "  - FNAStatus (object)\n",
      "  - NonFaceToFace (bool)\n",
      "  - SignRemotely (bool)\n",
      "  - NFTFCommunicationMode (object)\n",
      "  - NFTFCommunicationPlatform (object)\n",
      "  - UserId (object)\n",
      "  - RenewalFNA (bool)\n",
      "  - MaterialChanges (bool)\n",
      "  - FundSwitch (bool)\n",
      "  - NonResident (float64)\n",
      "  - RequirementNeeded (float64)\n",
      "  - RequirementCompleted (float64)\n",
      "  - FNAIdentifier (int64)\n",
      "  - RequirementResolvementTime (datetime64[ns])\n",
      "  - HasIntroducer (float64)\n",
      "  - AITransaction (float64)\n",
      "  - SelfSubmission (float64)\n",
      "  - AdviserId (object)\n",
      "  - ManagerAdviserId (object)\n",
      "  - DirectorAdviserId (object)\n",
      "----------------------------------------\n",
      "ðŸ“„ Dataset: emfc2personalinformation\n",
      "ðŸ§¾ Columns (37):\n",
      "  - # (int64)\n",
      "  - PersonalInformationId (object)\n",
      "  - EMFC2FNAId (object)\n",
      "  - ClientId (object)\n",
      "  - ClientName (object)\n",
      "  - ClientMobileNumber (object)\n",
      "  - ClientEmail (object)\n",
      "  - ClientContactPreferences (object)\n",
      "  - ClientGender (object)\n",
      "  - ClientDOB (datetime64[ns])\n",
      "  - ClientAge (float64)\n",
      "  - ClientCPFContributionCategoryId (object)\n",
      "  - IDNumber (object)\n",
      "  - Nationality (object)\n",
      "  - SpokenLanguage (object)\n",
      "  - WrittenLanguage (object)\n",
      "  - Education (object)\n",
      "  - EmploymentStatus (object)\n",
      "  - Occupation (object)\n",
      "  - MaritalStatus (object)\n",
      "  - PrimaryAddress (object)\n",
      "  - CorrespondingAddress (object)\n",
      "  - IncomeRange (object)\n",
      "  - AccompaniedbyTrustedIndividual (bool)\n",
      "  - RiskProfile (object)\n",
      "  - RiskProfileSubmissionDate (datetime64[ns])\n",
      "  - CKAProfile (object)\n",
      "  - CARProfile (object)\n",
      "  - CKACARSubmissionDate (datetime64[ns])\n",
      "  - ClientStatus (object)\n",
      "  - ClientResidentialStatus (object)\n",
      "  - CountryOfBirth (object)\n",
      "  - Race (object)\n",
      "  - ClientRetrieval (bool)\n",
      "  - ManuallyRetrieved (bool)\n",
      "  - MyInfoRetrieved (bool)\n",
      "  - SelectedClient (bool)\n",
      "----------------------------------------\n",
      "ðŸ“„ Dataset: emfc2\n",
      "ðŸ§¾ Columns (8):\n",
      "  - # (int64)\n",
      "  - EMFC2Id (object)\n",
      "  - ClientId (object)\n",
      "  - ClientAge (float64)\n",
      "  - EMFCStartDate (datetime64[ns])\n",
      "  - EMFCSubmitDate (datetime64[ns])\n",
      "  - EMFCStatus (object)\n",
      "  - UserId (object)\n",
      "----------------------------------------\n",
      "ðŸ“„ Dataset: EMFC2Assets\n",
      "ðŸ§¾ Columns (39):\n",
      "  - # (int64)\n",
      "  - EMFC2AssetsId (int64)\n",
      "  - EMFC2Id (object)\n",
      "  - AssetsDisclose (bool)\n",
      "  - ReasonAssetsDisclose (object)\n",
      "  - SavingsAccounts (int64)\n",
      "  - FixedDepositsAccount (float64)\n",
      "  - OtherCashAsset (float64)\n",
      "  - HomeAsset (float64)\n",
      "  - MotorAsset (float64)\n",
      "  - InsuranceCashValues (float64)\n",
      "  - OtherUseAsset (float64)\n",
      "  - StocksPortofolio (float64)\n",
      "  - BondPortofolio (float64)\n",
      "  - UTFEquityAsset (float64)\n",
      "  - UTFFixedIncomeAsset (float64)\n",
      "  - UTFMoneyAsset (float64)\n",
      "  - UTFOther (float64)\n",
      "  - ETFs (float64)\n",
      "  - NetBusinessInterests (float64)\n",
      "  - InvestmentProperties (float64)\n",
      "  - OtherInvestedAsset (float64)\n",
      "  - CPFOABalance (float64)\n",
      "  - CPFSABalance (float64)\n",
      "  - CPFMABalance (float64)\n",
      "  - CPFISOAEquityAsset (float64)\n",
      "  - CPFISOAFixedIncomeAsset (float64)\n",
      "  - CPFISOACashAsset (float64)\n",
      "  - CPFISOAOthersAsset (float64)\n",
      "  - CPFISSAEquityAsset (float64)\n",
      "  - CPFISSAFixedIncomeAsset (float64)\n",
      "  - CPFISSACashAsset (float64)\n",
      "  - CPFISSAOthersAsset (float64)\n",
      "  - SRSEquityAsset (float64)\n",
      "  - SRSFixedIncomeAsset (float64)\n",
      "  - SRSCashAsset (float64)\n",
      "  - SRSOther (float64)\n",
      "  - Completion (bool)\n",
      "  - CPFRABalance (float64)\n",
      "----------------------------------------\n",
      "ðŸ“„ Dataset: emfc2portofolioinsurance\n",
      "ðŸ§¾ Columns (25):\n",
      "  - # (int64)\n",
      "  - EMFC2PortofolioInsuranceId (int64)\n",
      "  - EMFC2Id (object)\n",
      "  - InsurerCompanyName (object)\n",
      "  - PolicyNumber (object)\n",
      "  - PolicyOwner (object)\n",
      "  - PlanType (object)\n",
      "  - PlanName (object)\n",
      "  - MainPurposeofPlan (object)\n",
      "  - SumAssuredforLossofLife (float64)\n",
      "  - SumAssuredforTPD (float64)\n",
      "  - SumAssuredforCI (float64)\n",
      "  - SumAssuredforEarlyCI (int64)\n",
      "  - SumAssuredforAccidentalDeath (float64)\n",
      "  - SumAssuredforHospitalIncome (int64)\n",
      "  - SumAssuredforDisabilityIncome (int64)\n",
      "  - SumAssuredforLongTermCare (int64)\n",
      "  - SumAssuredforHnS (int64)\n",
      "  - AnnualCashPremium (float64)\n",
      "  - BeneficiaryInformation (object)\n",
      "  - Remarks (object)\n",
      "  - AnnualCPFPremium (float64)\n",
      "  - CoverageStartAge (float64)\n",
      "  - CoverageEndAge (float64)\n",
      "  - PaymentEndAge (float64)\n",
      "----------------------------------------\n",
      "ðŸ“„ Dataset: emfc2productsolution\n",
      "ðŸ§¾ Columns (25):\n",
      "  - # (int64)\n",
      "  - EMFC2ProductSolutionId (object)\n",
      "  - EMFC2FNAId (object)\n",
      "  - PersonalInformationId (object)\n",
      "  - ProductId (object)\n",
      "  - SubProductId (object)\n",
      "  - ActivityType (object)\n",
      "  - Replacement (bool)\n",
      "  - ReasonforReplacement (object)\n",
      "  - RecommendedProductRejected (bool)\n",
      "  - ReasonforRejected (object)\n",
      "  - InvestmentGoalId (object)\n",
      "  - InvestmentHorizonId (object)\n",
      "  - InvestmentRiskId (object)\n",
      "  - ReasonBudgetLessThanPremium (object)\n",
      "  - ThirdParty (bool)\n",
      "  - DateCreated (datetime64[ns])\n",
      "  - AIProduct (bool)\n",
      "  - FNAProviderSubmissionTypeId (object)\n",
      "  - Status (object)\n",
      "  - StatusRemarks (object)\n",
      "  - StatusTimeStamp (datetime64[ns])\n",
      "  - CurrencyCode (object)\n",
      "  - CurrencyToSGDExchangeRate (float64)\n",
      "  - TargetedPremium (float64)\n",
      "----------------------------------------\n",
      "ðŸ“„ Dataset: EMFC2ProductIntegrationApplication\n",
      "ðŸ§¾ Columns (14):\n",
      "  - # (int64)\n",
      "  - integrationapplicationId (int64)\n",
      "  - applicationId (object)\n",
      "  - efnaTransNo (object)\n",
      "  - quotationId (object)\n",
      "  - solutionId (object)\n",
      "  - efnaQuotationId (object)\n",
      "  - module (object)\n",
      "  - caseId (object)\n",
      "  - BISignDate (object)\n",
      "  - AppFormSignDate (object)\n",
      "  - eReferenceNumber (object)\n",
      "  - userId (object)\n",
      "  - isDone (bool)\n",
      "----------------------------------------\n",
      "ðŸ“„ Dataset: EMFC2ProductIntegrationLog\n",
      "ðŸ§¾ Columns (21):\n",
      "  - # (int64)\n",
      "  - integrationLogsId (int64)\n",
      "  - EMFC2ProductSolutionId (object)\n",
      "  - efnaTransNo (object)\n",
      "  - efnaQuotationId (object)\n",
      "  - ProviderId (object)\n",
      "  - ProductId (object)\n",
      "  - ProductName (object)\n",
      "  - jsonResponse (object)\n",
      "  - userId (object)\n",
      "  - TransactionDate (datetime64[ns])\n",
      "  - Remarks (object)\n",
      "  - isDone (bool)\n",
      "  - givenName (object)\n",
      "  - familyName (object)\n",
      "  - needs (object)\n",
      "  - producttype (object)\n",
      "  - personalinfoid (float64)\n",
      "  - ClientDOB (object)\n",
      "  - jsonRequest (object)\n",
      "  - AgentCode (float64)\n",
      "----------------------------------------\n",
      "ðŸ“„ Dataset: ProductMainPlan\n",
      "ðŸ§¾ Columns (22):\n",
      "  - # (int64)\n",
      "  - ProductId (object)\n",
      "  - ProviderId (object)\n",
      "  - ProductSubCategoryId (object)\n",
      "  - MainPlan (object)\n",
      "  - DateInclusion (datetime64[ns])\n",
      "  - DateCeased (datetime64[ns])\n",
      "  - Benefit (object)\n",
      "  - Limitation (object)\n",
      "  - RiskProfile (bool)\n",
      "  - CKA (bool)\n",
      "  - CAR (bool)\n",
      "  - Star (bool)\n",
      "  - Objective (bool)\n",
      "  - Horizon (bool)\n",
      "  - Fund (bool)\n",
      "  - Rider (bool)\n",
      "  - Status (bool)\n",
      "  - UserId (object)\n",
      "  - CreatedDate (datetime64[ns])\n",
      "  - ProductCode (object)\n",
      "  - UlRequirement (bool)\n",
      "----------------------------------------\n",
      "ðŸ“„ Dataset: ProductType\n",
      "ðŸ§¾ Columns (8):\n",
      "  - # (int64)\n",
      "  - ProductTypeId (object)\n",
      "  - TypeName (object)\n",
      "  - TypeIcon (object)\n",
      "  - InvestmentType (bool)\n",
      "  - MFC (bool)\n",
      "  - SIMApp (bool)\n",
      "  - Active (bool)\n",
      "----------------------------------------\n",
      "ðŸ“„ Dataset: ProductCategory\n",
      "ðŸ§¾ Columns (6):\n",
      "  - # (int64)\n",
      "  - ProductCategoryId (object)\n",
      "  - CategoryName (object)\n",
      "  - ProductTypeId (object)\n",
      "  - CategoryIcon (object)\n",
      "  - Active (bool)\n",
      "----------------------------------------\n",
      "ðŸ“„ Dataset: productsubcategory\n",
      "ðŸ§¾ Columns (13):\n",
      "  - # (int64)\n",
      "  - ProductSubCategoryId (object)\n",
      "  - ProductCategoryId (object)\n",
      "  - SubCategoryName (object)\n",
      "  - SubCategoryShortName (object)\n",
      "  - SubCategoryIcon (object)\n",
      "  - IncludeFund (bool)\n",
      "  - IncludeRider (bool)\n",
      "  - IncludeStar (bool)\n",
      "  - IncludeObjective (bool)\n",
      "  - IncludeHorizon (bool)\n",
      "  - IncludeRiskProfile (bool)\n",
      "  - Active (bool)\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "print(\"\\\\n=== COLUMN HEADERS FOR EACH DATASET ===\\\\n\")\n",
    "for name, df in datasets.items():\n",
    "    print(f\"ðŸ“„ Dataset: {name}\")\n",
    "    print(f\"ðŸ§¾ Columns ({len(df.columns)}):\")\n",
    "    for col in df.columns:\n",
    "        print(f\"  - {col} ({df[col].dtype})\")\n",
    "    print(\"-\" * 40)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dfa9d91a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "ðŸš€ ULTIMATE FINANCIAL PRODUCT RECOMMENDATION SYSTEM\n",
      "============================================================\n",
      "\n",
      "ðŸ“‚ Loading training data...\n",
      "   âœ… Loaded datasets: Full=41,178, Balanced=27,365\n",
      "ðŸš€ Starting ultimate data preparation...\n",
      "ðŸ”§ Creating advanced financial features...\n",
      "   âœ… Prepared: 27365 samples, 36 features\n",
      "ðŸŽ¯ Training Ultimate Financial ML System...\n",
      "ðŸ” Selecting top 60 features using SHAP...\n",
      "   ðŸ“Š Data shape: (27365, 36) â†’ (27365, 36)\n",
      "   ðŸ¤– Training XGBoost model for SHAP analysis...\n",
      "   ðŸŒ³ Trying TreeExplainer...\n",
      "   ðŸ“Š Computing SHAP values for 500 samples...\n",
      "   ðŸ“Š Model expects 36 features, data has 36\n",
      "   ðŸ“Š SHAP values shape: (500, 36, 15)\n",
      "   ðŸ“Š Feature importance shape: (36,)\n",
      "   ðŸ“Š Number of columns: 36\n",
      "   âœ… Selected 36 features via SHAP\n",
      "   Top 5 features: ['Engagement_Score', 'Loyalty_Engagement_Score', 'ClientAge', 'Days_Since_Last_FNA', 'Client_Tenure_Years']\n",
      "ðŸ”§ Applying hybrid sampling...\n",
      "   Original shape: (27365, 36)\n",
      "   Resampled shape: (72246, 36)\n",
      "ðŸ—ï¸ Building ultimate picklable ensemble...\n",
      "   âœ… Built ensemble: 4 base + 2 meta models\n",
      "ðŸ”§ Training Ultimate Picklable Ensemble...\n",
      "   Training base model 1/4: RandomForestClassifier\n",
      "   Training base model 2/4: XGBClassifier\n",
      "   Training base model 3/4: LGBMClassifier\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002289 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 7904\n",
      "[LightGBM] [Info] Number of data points in the train set: 72246, number of used features: 31\n",
      "[LightGBM] [Info] Start training from score -2.708050\n",
      "[LightGBM] [Info] Start training from score -2.708050\n",
      "[LightGBM] [Info] Start training from score -2.708050\n",
      "[LightGBM] [Info] Start training from score -2.708050\n",
      "[LightGBM] [Info] Start training from score -2.708050\n",
      "[LightGBM] [Info] Start training from score -2.708050\n",
      "[LightGBM] [Info] Start training from score -2.708050\n",
      "[LightGBM] [Info] Start training from score -2.708050\n",
      "[LightGBM] [Info] Start training from score -2.708050\n",
      "[LightGBM] [Info] Start training from score -2.708050\n",
      "[LightGBM] [Info] Start training from score -2.708050\n",
      "[LightGBM] [Info] Start training from score -2.708050\n",
      "[LightGBM] [Info] Start training from score -2.708050\n",
      "[LightGBM] [Info] Start training from score -2.708050\n",
      "[LightGBM] [Info] Start training from score -2.708050\n",
      "   Training base model 4/4: CatBoostClassifier\n",
      "   Training meta model 1/2: LogisticRegression\n",
      "   Training meta model 2/2: XGBClassifier\n",
      "      Attention epoch 0, loss: 2.0161\n",
      "      Attention epoch 10, loss: 2.0106\n",
      "      Attention epoch 20, loss: 2.0089\n",
      "      Attention epoch 30, loss: 2.0084\n",
      "      Attention epoch 40, loss: 2.0082\n",
      "   âœ… Attention network trained successfully\n",
      "   âœ… Ultimate training complete!\n",
      "\n",
      "ðŸ“Š Quick performance check...\n",
      "   ðŸ“ˆ Sample accuracy: 0.800 (80.0%)\n",
      "ðŸ’¾ Ultimate safe save to ULTIMATE_FINANCIAL_ML_SYSTEM_V4.pkl...\n",
      "   ðŸ”§ Sanitizing ensemble for pickling...\n",
      "   âœ… Ultimate save successful!\n",
      "\n",
      "ðŸŽ‰ SUCCESS! Model trained and saved!\n",
      "ðŸ’¾ File: ULTIMATE_FINANCIAL_ML_SYSTEM_V4.pkl\n",
      "\n",
      "ðŸ§ª Testing prediction...\n",
      "ðŸ”§ Safe prediction with robust categorical handling...\n",
      "   ðŸ“Š Input data shape: (1, 26)\n",
      "   ðŸ“Š Input columns: ['ClientId', 'ClientAge', 'Education', 'Insurance_Evolution_Stage', 'Family_Protection_Priority', 'Nationality', 'Life_Coverage_Multiple', 'ClientGender', 'Total_Investments', 'IncomeRange', 'Engagement_Score', 'Estimated_Net_Worth', 'Digital_Adoption_Score', 'Income_Numeric', 'Product_Diversity_Score', 'Protection_Gap_Score', 'Years_With_Insurance', 'Total_Liquid_Assets', 'DateCreated', 'Insurance_Sophistication', 'MaritalStatus', 'Investment_Ratio', 'Client_Tenure_Years', 'Has_Insurance', 'Days_Since_Last_FNA', 'EmploymentStatus']\n",
      "   âœ… Basic preprocessing complete\n",
      "ðŸ”§ Creating advanced financial features...\n",
      "   âœ… Feature engineering complete: (1, 38)\n",
      "   âœ… Comprehensive NaN cleaning complete\n",
      "   ðŸ“Š Required features: 36\n",
      "   ðŸ“Š Features selected: (1, 36)\n",
      "   ðŸ“Š NaN check: 0 NaNs remaining\n",
      "   âœ… Feature scaling complete\n",
      "   ðŸ“Š Final data shape: (1, 36)\n",
      "   ðŸ“Š Final NaN check: 0 NaNs\n",
      "   âœ… Predictions obtained successfully\n",
      "   âœ… Ultra-safe prediction completed successfully!\n",
      "   âœ… Prediction test successful!\n",
      "      1. Term: 54.2%\n",
      "      2. SHIELD: 21.2%\n",
      "      3. Whole_Life: 13.2%\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "=== ENHANCED FINANCIAL PRODUCT RECOMMENDATION SYSTEM ===\n",
    "Advanced ML implementation with state-of-the-art techniques for maximizing top-3 accuracy\n",
    "Target: Improve from 79.7% to 88-92% top-3 accuracy through:\n",
    "- Dynamic Weight Neural Network Ensemble (DW-NNE)\n",
    "- Multi-level stacking with attention mechanisms\n",
    "- Advanced imbalance handling with Borderline-SMOTE + cost-sensitive learning\n",
    "- SHAP-based feature selection and financial-specific features\n",
    "- Calibrated confidence scoring with temporal validation\n",
    "\"\"\"\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Core ML libraries\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import TimeSeriesSplit, StratifiedKFold, cross_val_score\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler, PolynomialFeatures\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, matthews_corrcoef\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "from sklearn.feature_selection import SelectKBest, mutual_info_classif\n",
    "\n",
    "# Advanced ensemble and boosting\n",
    "from xgboost import XGBClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "import catboost as cb\n",
    "\n",
    "# Imbalanced learning\n",
    "from imblearn.over_sampling import BorderlineSMOTE, ADASYN\n",
    "from imblearn.combine import SMOTETomek\n",
    "from imblearn.pipeline import Pipeline as ImbPipeline\n",
    "\n",
    "# Deep learning and attention mechanisms\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "# Feature engineering and interpretability\n",
    "import shap\n",
    "import umap\n",
    "from scipy import stats\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# Utilities\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime, timedelta\n",
    "import itertools\n",
    "from collections import defaultdict\n",
    "from typing import List, Dict, Tuple, Union\n",
    "import optuna\n",
    "\n",
    "from pandas.api.types import is_object_dtype, is_datetime64_any_dtype\n",
    "\n",
    "class FinancialFeatureEngineer:\n",
    "    \"\"\"Advanced feature engineering specifically designed for financial products\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.feature_encoders = {}\n",
    "        self.interaction_features = []\n",
    "        self.temporal_features = []\n",
    "        \n",
    "    def create_financial_features(self, df: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"Create comprehensive financial features with domain expertise\"\"\"\n",
    "        \n",
    "        print(\"ðŸ”§ Creating advanced financial features...\")\n",
    "        \n",
    "        # Risk-adjusted features\n",
    "        df = self._create_risk_features(df)\n",
    "        \n",
    "        # Temporal behavioral features\n",
    "        df = self._create_temporal_features(df)\n",
    "        \n",
    "        # Financial sophistication features\n",
    "        df = self._create_sophistication_features(df)\n",
    "        \n",
    "        # Product affinity features\n",
    "        df = self._create_affinity_features(df)\n",
    "        \n",
    "        # Interaction features (most important for financial products)\n",
    "        df = self._create_interaction_features(df)\n",
    "        \n",
    "        return df\n",
    "    \n",
    "    def _create_risk_features(self, df: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"Create risk-adjusted features\"\"\"\n",
    "        \n",
    "        # Debt-to-income with volatility adjustment\n",
    "        if 'Income_Numeric' in df.columns and 'Total_Annual_Premium' in df.columns:\n",
    "            df['Debt_to_Income_Ratio'] = np.where(\n",
    "                df['Income_Numeric'] > 0,\n",
    "                df['Total_Annual_Premium'] / df['Income_Numeric'],\n",
    "                0\n",
    "            ).clip(0, 2)  # Cap at 200%\n",
    "            \n",
    "            # Risk-adjusted premium ratio\n",
    "            df['Risk_Adjusted_Premium_Ratio'] = df['Debt_to_Income_Ratio'] * (\n",
    "                1 + df.get('Protection_Gap_Score', 0)\n",
    "            )\n",
    "        \n",
    "        # Wealth concentration index (Gini coefficient approximation)\n",
    "        wealth_columns = ['Total_Liquid_Assets', 'Total_Investments', 'Total_CPF']\n",
    "        available_wealth = [col for col in wealth_columns if col in df.columns]\n",
    "        \n",
    "        if len(available_wealth) >= 2:\n",
    "            wealth_matrix = df[available_wealth].fillna(0)\n",
    "            df['Wealth_Concentration'] = wealth_matrix.apply(\n",
    "                lambda row: self._calculate_concentration_index(row.values), axis=1\n",
    "            )\n",
    "        \n",
    "        # Life stage risk score\n",
    "        if 'ClientAge' in df.columns and 'MaritalStatus' in df.columns:\n",
    "            df['Life_Stage_Risk'] = df.apply(self._calculate_life_stage_risk, axis=1)\n",
    "        \n",
    "        return df\n",
    "    \n",
    "    def _create_temporal_features(self, df: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"Create temporal behavioral features\"\"\"\n",
    "        \n",
    "        # Financial journey progression\n",
    "        if 'Client_Tenure_Years' in df.columns and 'EMFC_Count' in df.columns:\n",
    "            df['Financial_Journey_Velocity'] = np.where(\n",
    "                df['Client_Tenure_Years'] > 0,\n",
    "                df['EMFC_Count'] / df['Client_Tenure_Years'],\n",
    "                0\n",
    "            )\n",
    "        \n",
    "        # Engagement momentum (recent activity vs historical)\n",
    "        if 'Days_Since_Last_FNA' in df.columns:\n",
    "            df['Engagement_Momentum'] = np.exp(-df['Days_Since_Last_FNA'] / 365)\n",
    "            \n",
    "            # Seasonal adjustment for financial planning\n",
    "            current_month = datetime.now().month\n",
    "            df['Seasonal_Propensity'] = np.where(\n",
    "                current_month in ([1, 4, 10, 11, 12]),  # Tax planning months\n",
    "                1.2,\n",
    "                1.0\n",
    "            )\n",
    "        \n",
    "        # Portfolio evolution score\n",
    "        if 'Insurance_Evolution_Stage' in df.columns and 'Product_Diversity_Score' in df.columns:\n",
    "            df['Portfolio_Evolution_Score'] = (\n",
    "                df['Insurance_Evolution_Stage'] * 0.6 + \n",
    "                df['Product_Diversity_Score'] * 0.4\n",
    "            )\n",
    "        \n",
    "        return df\n",
    "    \n",
    "    def _create_sophistication_features(self, df: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"Create financial sophistication indicators\"\"\"\n",
    "        \n",
    "        # Advanced sophistication score\n",
    "        sophistication_components = {\n",
    "            'Education': {'University': 3, 'Polytechnic': 2, 'Secondary': 1, 'Primary': 0},\n",
    "            'Occupation': {'Professional': 3, 'Manager': 2, 'Executive': 2, 'Technician': 1}\n",
    "        }\n",
    "        \n",
    "        df['Advanced_Sophistication_Score'] = 0\n",
    "        \n",
    "        for component, mapping in sophistication_components.items():\n",
    "            if component in df.columns:\n",
    "                if callable(mapping):\n",
    "                    df['Advanced_Sophistication_Score'] += df[component].apply(mapping)\n",
    "                else:\n",
    "                    df['Advanced_Sophistication_Score'] += df[component].map(mapping).fillna(0)\n",
    "        \n",
    "        # Add Investment_Ratio component separately since it needs a lambda function\n",
    "        if 'Investment_Ratio' in df.columns:\n",
    "            df['Advanced_Sophistication_Score'] += df['Investment_Ratio'].apply(\n",
    "                lambda x: 3 if x > 0.5 else 2 if x > 0.2 else 1 if x > 0 else 0\n",
    "            )\n",
    "        \n",
    "        # Digital adoption sophistication\n",
    "        if 'Digital_Adoption_Score' in df.columns and 'ClientAge' in df.columns:\n",
    "            df['Age_Adjusted_Digital_Score'] = df['Digital_Adoption_Score'] * np.where(\n",
    "                df['ClientAge'] < 35, 1.2,\n",
    "                np.where(df['ClientAge'] < 50, 1.0, 0.8)\n",
    "            )\n",
    "        \n",
    "        return df\n",
    "    \n",
    "    def _create_affinity_features(self, df: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"Create product affinity and cross-sell indicators\"\"\"\n",
    "        \n",
    "        # Product affinity matrix based on financial planning best practices\n",
    "        affinity_rules = {\n",
    "            'Has_Term_Life': {\n",
    "                'next_products': ['Whole_Life', 'Investment_Linked', 'Critical_Illness'],\n",
    "                'weights': [0.8, 0.6, 0.7]\n",
    "            },\n",
    "            'Has_Health_Insurance': {\n",
    "                'next_products': ['Critical_Illness', 'Long_Term_Care', 'Disability_Income'],\n",
    "                'weights': [0.9, 0.6, 0.5]\n",
    "            },\n",
    "            'Has_Investment_Products': {\n",
    "                'next_products': ['Retirement_Planning', 'Endowment', 'Annuity'],\n",
    "                'weights': [0.8, 0.5, 0.7]\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        # Cross-sell propensity score\n",
    "        df['Cross_Sell_Propensity'] = 0\n",
    "        \n",
    "        for current_product, rules in affinity_rules.items():\n",
    "            if current_product in df.columns:\n",
    "                for next_product, weight in zip(rules['next_products'], rules['weights']):\n",
    "                    feature_name = f'Affinity_{current_product}_to_{next_product}'\n",
    "                    if current_product in df.columns:  # Double check column exists\n",
    "                        df[feature_name] = (df[current_product] * weight).fillna(0)\n",
    "                        df['Cross_Sell_Propensity'] += df[feature_name]\n",
    "        \n",
    "        return df\n",
    "    \n",
    "    def _create_interaction_features(self, df: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"Create high-value interaction features for financial planning\"\"\"\n",
    "        \n",
    "        # Age Ã— Income interaction with life stage adjustment\n",
    "        if 'ClientAge' in df.columns and 'Income_Numeric' in df.columns:\n",
    "            df['Age_Income_Life_Stage'] = (\n",
    "                df['ClientAge'] * np.log1p(df['Income_Numeric']) * \n",
    "                df.get('Life_Stage_Risk', 1)\n",
    "            )\n",
    "        \n",
    "        # Wealth Ã— Risk interaction\n",
    "        if 'Estimated_Net_Worth' in df.columns and 'Protection_Gap_Score' in df.columns:\n",
    "            df['Wealth_Risk_Interaction'] = (\n",
    "                np.log1p(df['Estimated_Net_Worth']) * df['Protection_Gap_Score']\n",
    "            )\n",
    "        \n",
    "        # Tenure Ã— Engagement interaction\n",
    "        if 'Client_Tenure_Years' in df.columns and 'Engagement_Score' in df.columns:\n",
    "            df['Loyalty_Engagement_Score'] = (\n",
    "                df['Client_Tenure_Years'] * df['Engagement_Score']\n",
    "            )\n",
    "        \n",
    "        # Sophistication Ã— Gap interaction (sophisticated clients with gaps = high value)\n",
    "        if 'Advanced_Sophistication_Score' in df.columns and 'Protection_Gap_Score' in df.columns:\n",
    "            df['Sophisticated_Gap_Opportunity'] = (\n",
    "                df['Advanced_Sophistication_Score'] * df['Protection_Gap_Score']\n",
    "            )\n",
    "        \n",
    "        return df\n",
    "    \n",
    "    def _calculate_concentration_index(self, values: np.array) -> float:\n",
    "        \"\"\"Calculate wealth concentration using Gini coefficient approximation\"\"\"\n",
    "        values = values[values > 0]  # Remove zeros\n",
    "        if len(values) < 2:\n",
    "            return 0\n",
    "        \n",
    "        sorted_values = np.sort(values)\n",
    "        n = len(sorted_values)\n",
    "        cumsum = np.cumsum(sorted_values)\n",
    "        return (n + 1 - 2 * np.sum(cumsum) / cumsum[-1]) / n\n",
    "    \n",
    "    def _calculate_life_stage_risk(self, row) -> float:\n",
    "        \"\"\"Calculate life stage risk score\"\"\"\n",
    "        age = row.get('ClientAge', 40)\n",
    "        marital_status = str(row.get('MaritalStatus', '')).lower()\n",
    "        \n",
    "        base_risk = 1.0\n",
    "        \n",
    "        # Age-based risk\n",
    "        if age < 30:\n",
    "            base_risk += 0.2  # Young, building wealth\n",
    "        elif age < 45:\n",
    "            base_risk += 0.5  # Peak earning years\n",
    "        elif age < 60:\n",
    "            base_risk += 0.3  # Pre-retirement planning\n",
    "        else:\n",
    "            base_risk += 0.1  # Retirement phase\n",
    "        \n",
    "        # Marital status adjustment\n",
    "        if 'married' in marital_status:\n",
    "            base_risk += 0.3  # Family protection needs\n",
    "        elif 'divorced' in marital_status:\n",
    "            base_risk += 0.4  # Higher protection needs\n",
    "        \n",
    "        return base_risk\n",
    "\n",
    "\n",
    "class DynamicWeightEnsemble:\n",
    "    \"\"\"Dynamic Weight Neural Network Ensemble with attention mechanisms\"\"\"\n",
    "    \n",
    "    def __init__(self, base_models: List = None, meta_models: List = None, attention_dim: int = 64):\n",
    "        self.base_models = base_models or []\n",
    "        self.meta_models = meta_models or []\n",
    "        self.attention_dim = attention_dim\n",
    "        self.attention_network = None\n",
    "        self.attention_state_dict = None\n",
    "        self.is_fitted = False\n",
    "        self.n_classes = None\n",
    "        self.base_predictions = None\n",
    "        self.meta_predictions = None\n",
    "        self.classes_ = None  # Add classes_ attribute for sklearn compatibility\n",
    "        \n",
    "    def get_params(self, deep=True):\n",
    "        \"\"\"Get parameters for this estimator\"\"\"\n",
    "        return {\n",
    "            'base_models': self.base_models,\n",
    "            'meta_models': self.meta_models,\n",
    "            'attention_dim': self.attention_dim\n",
    "        }\n",
    "    \n",
    "    def set_params(self, **params):\n",
    "        \"\"\"Set parameters for this estimator\"\"\"\n",
    "        for key, value in params.items():\n",
    "            setattr(self, key, value)\n",
    "        return self\n",
    "        \n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import joblib\n",
    "from typing import Dict, Any\n",
    "\n",
    "# FIX 1: Move AttentionNetwork to module level (outside any class)\n",
    "class AttentionNetwork(nn.Module):\n",
    "    \"\"\"\n",
    "    Standalone AttentionNetwork class that can be pickled\n",
    "    \"\"\"\n",
    "    def __init__(self, input_dim: int, n_meta_models: int, attention_dim: int):\n",
    "        super().__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.n_meta_models = n_meta_models\n",
    "        self.attention_dim = attention_dim\n",
    "        \n",
    "        self.attention = nn.Sequential(\n",
    "            nn.Linear(input_dim, attention_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(attention_dim, attention_dim // 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.1),\n",
    "            nn.Linear(attention_dim // 2, n_meta_models),\n",
    "            nn.Softmax(dim=1)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.attention(x)\n",
    "\n",
    "\n",
    "# FIX 2: Updated DynamicWeightEnsemble with better serialization\n",
    "class PicklableDynamicWeightEnsemble:\n",
    "    \"\"\"\n",
    "    Enhanced Dynamic Weight Ensemble that can be properly pickled\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, base_models=None, meta_models=None, attention_dim=64):\n",
    "        self.base_models = base_models or []\n",
    "        self.meta_models = meta_models or []\n",
    "        self.attention_dim = attention_dim\n",
    "        self.attention_network = None\n",
    "        self.attention_state_dict = None  # Store PyTorch state separately\n",
    "        self.is_fitted = False\n",
    "        self.n_classes = None\n",
    "        self.base_predictions = None\n",
    "        self.meta_predictions = None\n",
    "        self.classes_ = None\n",
    "        \n",
    "    def get_params(self, deep=True):\n",
    "        \"\"\"Get parameters for this estimator\"\"\"\n",
    "        return {\n",
    "            'base_models': self.base_models,\n",
    "            'meta_models': self.meta_models,\n",
    "            'attention_dim': self.attention_dim\n",
    "        }\n",
    "    \n",
    "    def set_params(self, **params):\n",
    "        \"\"\"Set parameters for this estimator\"\"\"\n",
    "        for key, value in params.items():\n",
    "            setattr(self, key, value)\n",
    "        return self\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        \"\"\"Fit the dynamic weight ensemble\"\"\"\n",
    "        \n",
    "        print(\"ðŸ”§ Training Picklable Dynamic Weight Ensemble...\")\n",
    "        \n",
    "        # Store classes for sklearn compatibility\n",
    "        self.classes_ = np.unique(y)\n",
    "        self.n_classes = len(self.classes_)\n",
    "        \n",
    "        # Step 1: Train base models\n",
    "        self.base_predictions = np.zeros((X.shape[0], len(self.base_models), self.n_classes))\n",
    "        \n",
    "        for i, model in enumerate(self.base_models):\n",
    "            print(f\"   Training base model {i+1}/{len(self.base_models)}: {model.__class__.__name__}\")\n",
    "            model.fit(X, y)\n",
    "            self.base_predictions[:, i, :] = model.predict_proba(X)\n",
    "        \n",
    "        # Step 2: Train meta models on base predictions\n",
    "        meta_features = self.base_predictions.reshape(X.shape[0], -1)\n",
    "        self.meta_predictions = np.zeros((X.shape[0], len(self.meta_models), self.n_classes))\n",
    "        \n",
    "        for i, meta_model in enumerate(self.meta_models):\n",
    "            print(f\"   Training meta model {i+1}/{len(self.meta_models)}: {meta_model.__class__.__name__}\")\n",
    "            meta_model.fit(meta_features, y)\n",
    "            self.meta_predictions[:, i, :] = meta_model.predict_proba(meta_features)\n",
    "        \n",
    "        # Step 3: Train attention network\n",
    "        self._build_and_train_attention_network(X, y)\n",
    "        \n",
    "        self.is_fitted = True\n",
    "        return self\n",
    "        \n",
    "    def predict_proba(self, X):\n",
    "        \"\"\"Predict with dynamic weights\"\"\"\n",
    "        \n",
    "        if not self.is_fitted:\n",
    "            raise ValueError(\"Model must be fitted before prediction\")\n",
    "        \n",
    "        # Get base model predictions\n",
    "        base_preds = np.zeros((X.shape[0], len(self.base_models), self.n_classes))\n",
    "        for i, model in enumerate(self.base_models):\n",
    "            base_preds[:, i, :] = model.predict_proba(X)\n",
    "        \n",
    "        # Get meta model predictions\n",
    "        meta_features = base_preds.reshape(X.shape[0], -1)\n",
    "        meta_preds = np.zeros((X.shape[0], len(self.meta_models), self.n_classes))\n",
    "        for i, meta_model in enumerate(self.meta_models):\n",
    "            meta_preds[:, i, :] = meta_model.predict_proba(meta_features)\n",
    "        \n",
    "        # Apply attention mechanism\n",
    "        attention_weights = self._get_attention_weights(X)\n",
    "        \n",
    "        # Weighted combination of meta predictions\n",
    "        final_predictions = np.zeros((X.shape[0], self.n_classes))\n",
    "        for i in range(len(self.meta_models)):\n",
    "            final_predictions += attention_weights[:, i:i+1] * meta_preds[:, i, :]\n",
    "        \n",
    "        return final_predictions\n",
    "    \n",
    "    def predict(self, X):\n",
    "        \"\"\"Predict class labels\"\"\"\n",
    "        probas = self.predict_proba(X)\n",
    "        return self.classes_[np.argmax(probas, axis=1)]\n",
    "    \n",
    "    def decision_function(self, X):\n",
    "        \"\"\"Decision function for sklearn compatibility\"\"\"\n",
    "        return self.predict_proba(X)\n",
    "    \n",
    "    def _build_and_train_attention_network(self, X, y, epochs=100):\n",
    "        \"\"\"Build and train attention network with state dict storage\"\"\"\n",
    "        \n",
    "        # Create attention network\n",
    "        self.attention_network = AttentionNetwork(\n",
    "            input_dim=X.shape[1],\n",
    "            n_meta_models=len(self.meta_models),\n",
    "            attention_dim=self.attention_dim\n",
    "        )\n",
    "        \n",
    "        # Prepare data\n",
    "        X_tensor = torch.FloatTensor(X)\n",
    "        y_tensor = torch.LongTensor(y)\n",
    "        \n",
    "        optimizer = torch.optim.Adam(self.attention_network.parameters(), lr=0.001)\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        \n",
    "        # Training loop\n",
    "        for epoch in range(epochs):\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # Get attention weights\n",
    "            attention_weights = self.attention_network(X_tensor)\n",
    "            \n",
    "            # Weighted combination of meta predictions\n",
    "            weighted_preds = torch.zeros(X.shape[0], self.n_classes)\n",
    "            for i in range(len(self.meta_models)):\n",
    "                meta_pred_tensor = torch.FloatTensor(self.meta_predictions[:, i, :])\n",
    "                weighted_preds += attention_weights[:, i:i+1] * meta_pred_tensor\n",
    "            \n",
    "            # Compute loss\n",
    "            loss = criterion(weighted_preds, y_tensor)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            if epoch % 20 == 0:\n",
    "                print(f\"      Attention training epoch {epoch}, loss: {loss.item():.4f}\")\n",
    "        \n",
    "        # Store state dict for pickling\n",
    "        self.attention_state_dict = self.attention_network.state_dict()\n",
    "        print(\"   ðŸ’¾ Stored attention network state for pickling\")\n",
    "    \n",
    "    def _get_attention_weights(self, X):\n",
    "        \"\"\"Get attention weights, reconstructing network if needed\"\"\"\n",
    "        \n",
    "        # Reconstruct attention network if not available\n",
    "        if self.attention_network is None and self.attention_state_dict is not None:\n",
    "            print(\"   ðŸ”§ Reconstructing attention network from saved state...\")\n",
    "            self.attention_network = AttentionNetwork(\n",
    "                input_dim=X.shape[1],\n",
    "                n_meta_models=len(self.meta_models),\n",
    "                attention_dim=self.attention_dim\n",
    "            )\n",
    "            self.attention_network.load_state_dict(self.attention_state_dict)\n",
    "            self.attention_network.eval()\n",
    "        \n",
    "        # Get attention weights\n",
    "        X_tensor = torch.FloatTensor(X)\n",
    "        with torch.no_grad():\n",
    "            attention_weights = self.attention_network(X_tensor).numpy()\n",
    "        \n",
    "        return attention_weights\n",
    "    \n",
    "    def __getstate__(self):\n",
    "        \"\"\"Custom pickling - exclude PyTorch network, keep state dict\"\"\"\n",
    "        state = self.__dict__.copy()\n",
    "        # Remove the unpicklable PyTorch network\n",
    "        state['attention_network'] = None\n",
    "        return state\n",
    "    \n",
    "    def __setstate__(self, state):\n",
    "        \"\"\"Custom unpickling - restore everything except PyTorch network\"\"\"\n",
    "        self.__dict__.update(state)\n",
    "\n",
    "class AdvancedImbalanceHandler:\n",
    "    \"\"\"Advanced imbalance handling with financial domain expertise\"\"\"\n",
    "    \n",
    "    def __init__(self, strategy: str = 'hybrid'):\n",
    "        self.strategy = strategy\n",
    "        self.samplers = {}\n",
    "        self.cost_matrix = None\n",
    "        \n",
    "    def create_financial_cost_matrix(self, y: np.ndarray, high_value_classes: List[str]) -> np.ndarray:\n",
    "        \"\"\"Create cost matrix based on financial product values\"\"\"\n",
    "        \n",
    "        n_classes = len(np.unique(y))\n",
    "        cost_matrix = np.ones((n_classes, n_classes))\n",
    "        \n",
    "        # Higher cost for missing high-value products\n",
    "        for high_val_class in high_value_classes:\n",
    "            if high_val_class < n_classes:\n",
    "                cost_matrix[:, high_val_class] = 10  # 10x cost for false negatives\n",
    "                cost_matrix[high_val_class, high_val_class] = 0  # No cost for correct predictions\n",
    "        \n",
    "        # Moderate cost for cross-selling opportunities\n",
    "        cross_sell_classes = [0, 1, 2]  # Top 3 most common products\n",
    "        for cs_class in cross_sell_classes:\n",
    "            if cs_class < n_classes:\n",
    "                cost_matrix[:, cs_class] = 3\n",
    "                cost_matrix[cs_class, cs_class] = 0\n",
    "        \n",
    "        self.cost_matrix = cost_matrix\n",
    "        return cost_matrix\n",
    "    \n",
    "    def apply_sampling(self, X: np.ndarray, y: np.ndarray) -> Tuple[np.ndarray, np.ndarray]:\n",
    "        \"\"\"Apply advanced sampling strategies\"\"\"\n",
    "        \n",
    "        # Check if we have enough samples for SMOTE\n",
    "        min_samples_per_class = np.bincount(y).min()\n",
    "        if min_samples_per_class < 6:  # SMOTE needs at least 6 samples\n",
    "            print(f\"   âš ï¸  Minimum class has only {min_samples_per_class} samples, skipping SMOTE\")\n",
    "            return X, y\n",
    "        \n",
    "        if self.strategy == 'borderline_smote':\n",
    "            sampler = BorderlineSMOTE(\n",
    "                sampling_strategy='auto',\n",
    "                k_neighbors=min(5, min_samples_per_class - 1),\n",
    "                m_neighbors=min(10, min_samples_per_class - 1),\n",
    "                random_state=42\n",
    "            )\n",
    "        elif self.strategy == 'adasyn':\n",
    "            sampler = ADASYN(\n",
    "                sampling_strategy='auto',\n",
    "                n_neighbors=min(5, min_samples_per_class - 1),\n",
    "                random_state=42\n",
    "            )\n",
    "        elif self.strategy == 'hybrid':\n",
    "            # Combine BorderlineSMOTE with Tomek links for cleaning\n",
    "            from imblearn.over_sampling import SMOTE\n",
    "            sampler = SMOTETomek(\n",
    "                smote=SMOTE(\n",
    "                    k_neighbors=min(5, min_samples_per_class - 1), \n",
    "                    random_state=42\n",
    "                ),\n",
    "                random_state=42\n",
    "            )\n",
    "        else:\n",
    "            return X, y\n",
    "        \n",
    "        try:\n",
    "            print(f\"ðŸ”§ Applying {self.strategy} sampling...\")\n",
    "            X_resampled, y_resampled = sampler.fit_resample(X, y)\n",
    "            \n",
    "            print(f\"   Original shape: {X.shape}\")\n",
    "            print(f\"   Resampled shape: {X_resampled.shape}\")\n",
    "            \n",
    "            return X_resampled, y_resampled\n",
    "        except Exception as e:\n",
    "            print(f\"   âš ï¸  Sampling failed: {e}\")\n",
    "            print(f\"   Using original data without resampling\")\n",
    "            return X, y\n",
    "\n",
    "\n",
    "class SHAPFeatureSelector:\n",
    "    \"\"\"Enhanced SHAP Feature Selector with robust error handling and debugging\"\"\"\n",
    "    \n",
    "    def __init__(self, n_features: int = 50, method: str = 'importance'):\n",
    "        self.n_features = n_features\n",
    "        self.method = method\n",
    "        self.selected_features = None\n",
    "        self.feature_importance = None\n",
    "        self.debug_info = {}\n",
    "\n",
    "    def fit_select(self, X: pd.DataFrame, y: np.ndarray, model=None) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Robust SHAP feature selection with comprehensive debugging and fallback mechanisms\n",
    "        \"\"\"\n",
    "        print(f\"ðŸ” Selecting top {self.n_features} features using SHAP...\")\n",
    "        \n",
    "        # Store original info for debugging\n",
    "        self.debug_info['original_shape'] = X.shape\n",
    "        self.debug_info['original_columns'] = X.columns.tolist()\n",
    "        \n",
    "        # Clean data for SHAP\n",
    "        X_clean = self._enhanced_sanitize_for_shap(X)\n",
    "        \n",
    "        # Debug: Check what sanitization did\n",
    "        self.debug_info['cleaned_shape'] = X_clean.shape\n",
    "        self.debug_info['cleaned_columns'] = X_clean.columns.tolist()\n",
    "        \n",
    "        print(f\"   ðŸ“Š Data shape: {X.shape} â†’ {X_clean.shape}\")\n",
    "        \n",
    "        # Cap requested features to available features\n",
    "        self.n_features = min(self.n_features, X_clean.shape[1])\n",
    "        \n",
    "        # Prepare model if not provided\n",
    "        if model is None:\n",
    "            print(\"   ðŸ¤– Training XGBoost model for SHAP analysis...\")\n",
    "            model = XGBClassifier(\n",
    "                n_estimators=100,\n",
    "                max_depth=6,\n",
    "                random_state=42,\n",
    "                eval_metric='mlogloss',\n",
    "                verbosity=0  # Reduce noise\n",
    "            )\n",
    "            # Ensure we fit on the same data format\n",
    "            model.fit(X_clean, y)\n",
    "        \n",
    "        # Try multiple SHAP approaches\n",
    "        shap_success = False\n",
    "        \n",
    "        # Approach 1: Standard TreeExplainer\n",
    "        if not shap_success:\n",
    "            shap_success = self._try_tree_explainer(X_clean, model)\n",
    "        \n",
    "        # Approach 2: Permutation-based SHAP (more robust)\n",
    "        if not shap_success:\n",
    "            shap_success = self._try_permutation_explainer(X_clean, y, model)\n",
    "        \n",
    "        # Approach 3: Linear approximation of tree model\n",
    "        if not shap_success:\n",
    "            shap_success = self._try_linear_approximation(X_clean, y)\n",
    "        \n",
    "        # If all SHAP methods fail, use mutual information\n",
    "        if not shap_success:\n",
    "            print(\"   âš ï¸  All SHAP methods failed, using Mutual Information\")\n",
    "            return self._fallback_mutual_information(X_clean, y)\n",
    "        \n",
    "        print(f\"   âœ… Selected {len(self.selected_features)} features via SHAP\")\n",
    "        print(\"   Top 5 features:\", self.selected_features[:5])\n",
    "        \n",
    "        return X_clean[self.selected_features]\n",
    "    \n",
    "    def _enhanced_sanitize_for_shap(self, X: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Enhanced sanitization with better handling of edge cases\n",
    "        \"\"\"\n",
    "        X_clean = X.copy()\n",
    "        \n",
    "        # Track column changes\n",
    "        original_cols = set(X_clean.columns)\n",
    "        \n",
    "        for col in X_clean.columns:\n",
    "            try:\n",
    "                series = X_clean[col]\n",
    "                \n",
    "                # Handle datetime columns\n",
    "                if is_datetime64_any_dtype(series):\n",
    "                    X_clean[col] = pd.to_numeric(series, errors='coerce')\n",
    "                    continue\n",
    "                \n",
    "                # Handle already numeric columns\n",
    "                if pd.api.types.is_numeric_dtype(series):\n",
    "                    # Fill any remaining NaN values\n",
    "                    X_clean[col] = series.fillna(series.median())\n",
    "                    continue\n",
    "                \n",
    "                # Handle object columns\n",
    "                if series.dtype == 'object':\n",
    "                    # Try to convert to numeric first\n",
    "                    numeric_converted = pd.to_numeric(series, errors='coerce')\n",
    "                    \n",
    "                    if not numeric_converted.isna().all():\n",
    "                        # If some values converted successfully, use numeric\n",
    "                        X_clean[col] = numeric_converted.fillna(numeric_converted.median())\n",
    "                    else:\n",
    "                        # Otherwise, use label encoding\n",
    "                        try:\n",
    "                            # Handle lists, tuples, and other iterables\n",
    "                            processed_series = series.apply(lambda x: \n",
    "                                len(x) if isinstance(x, (list, tuple, np.ndarray)) \n",
    "                                else len(str(x)) if pd.notna(x) else 0\n",
    "                            )\n",
    "                            X_clean[col] = processed_series\n",
    "                        except:\n",
    "                            # Final fallback: simple factorization\n",
    "                            X_clean[col] = pd.factorize(series.astype(str))[0]\n",
    "                \n",
    "                # Ensure no infinite values\n",
    "                if pd.api.types.is_numeric_dtype(X_clean[col]):\n",
    "                    X_clean[col] = X_clean[col].replace([np.inf, -np.inf], np.nan)\n",
    "                    X_clean[col] = X_clean[col].fillna(X_clean[col].median())\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"   âš ï¸  Issue processing column {col}: {e}\")\n",
    "                # Drop problematic columns as last resort\n",
    "                X_clean = X_clean.drop(columns=[col])\n",
    "        \n",
    "        # Ensure all columns are numeric\n",
    "        for col in X_clean.columns:\n",
    "            if not pd.api.types.is_numeric_dtype(X_clean[col]):\n",
    "                try:\n",
    "                    X_clean[col] = pd.to_numeric(X_clean[col], errors='coerce')\n",
    "                    X_clean[col] = X_clean[col].fillna(0)\n",
    "                except:\n",
    "                    X_clean = X_clean.drop(columns=[col])\n",
    "        \n",
    "        # Final validation\n",
    "        final_cols = set(X_clean.columns)\n",
    "        dropped_cols = original_cols - final_cols\n",
    "        if dropped_cols:\n",
    "            print(f\"   ðŸ“ Dropped problematic columns: {list(dropped_cols)}\")\n",
    "        \n",
    "        # Ensure we have numeric data only\n",
    "        X_clean = X_clean.select_dtypes(include=[np.number])\n",
    "        \n",
    "        return X_clean\n",
    "    \n",
    "    def _try_tree_explainer(self, X_clean: pd.DataFrame, model) -> bool:\n",
    "        \"\"\"\n",
    "        Try standard TreeExplainer approach\n",
    "        \"\"\"\n",
    "        try:\n",
    "            print(\"   ðŸŒ³ Trying TreeExplainer...\")\n",
    "            \n",
    "            # Create explainer\n",
    "            explainer = shap.TreeExplainer(model)\n",
    "            \n",
    "            # Use smaller sample for efficiency\n",
    "            sample_size = min(500, len(X_clean))\n",
    "            X_sample = X_clean.sample(n=sample_size, random_state=42)\n",
    "            \n",
    "            print(f\"   ðŸ“Š Computing SHAP values for {sample_size} samples...\")\n",
    "            print(f\"   ðŸ“Š Model expects {model.n_features_in_} features, data has {X_sample.shape[1]}\")\n",
    "            \n",
    "            # Ensure feature count matches\n",
    "            if hasattr(model, 'n_features_in_') and model.n_features_in_ != X_sample.shape[1]:\n",
    "                print(f\"   âš ï¸  Feature count mismatch: model={model.n_features_in_}, data={X_sample.shape[1]}\")\n",
    "                return False\n",
    "            \n",
    "            # Get SHAP values with proper error handling\n",
    "            shap_values = explainer.shap_values(X_sample.values)\n",
    "            \n",
    "            # Debug: Print SHAP values structure\n",
    "            if isinstance(shap_values, list):\n",
    "                print(f\"   ðŸ“Š SHAP values: list of {len(shap_values)} arrays, each shape {shap_values[0].shape}\")\n",
    "                # Multi-class case: average across classes\n",
    "                shap_import = np.mean([np.abs(sv).mean(axis=0) for sv in shap_values], axis=0)\n",
    "            else:\n",
    "                print(f\"   ðŸ“Š SHAP values shape: {shap_values.shape}\")\n",
    "                if shap_values.ndim == 3:\n",
    "                    # New format: (n_samples, n_classes, n_features)\n",
    "                    shap_import = np.abs(shap_values).mean(axis=(0, 2))\n",
    "                elif shap_values.ndim == 2:\n",
    "                    # Binary or regression: (n_samples, n_features)\n",
    "                    shap_import = np.abs(shap_values).mean(axis=0)\n",
    "                else:\n",
    "                    print(f\"   âš ï¸  Unexpected SHAP values dimension: {shap_values.ndim}\")\n",
    "                    return False\n",
    "            \n",
    "            print(f\"   ðŸ“Š Feature importance shape: {shap_import.shape}\")\n",
    "            print(f\"   ðŸ“Š Number of columns: {len(X_clean.columns)}\")\n",
    "            \n",
    "            # Ensure shapes match\n",
    "            if len(shap_import) != len(X_clean.columns):\n",
    "                print(f\"   âš ï¸  Shape mismatch: importance={len(shap_import)}, columns={len(X_clean.columns)}\")\n",
    "                return False\n",
    "            \n",
    "            # Create importance DataFrame\n",
    "            importance_df = pd.DataFrame({\n",
    "                'feature': X_clean.columns,\n",
    "                'importance': shap_import\n",
    "            }).sort_values('importance', ascending=False)\n",
    "            \n",
    "            # Store results\n",
    "            self.selected_features = importance_df.head(self.n_features)['feature'].tolist()\n",
    "            self.feature_importance = importance_df\n",
    "            self.debug_info['shap_method'] = 'TreeExplainer'\n",
    "            \n",
    "            return True\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"   âš ï¸  TreeExplainer failed: {e}\")\n",
    "            return False\n",
    "    \n",
    "    def _try_permutation_explainer(self, X_clean: pd.DataFrame, y: np.ndarray, model) -> bool:\n",
    "        \"\"\"\n",
    "        Try permutation-based SHAP (more robust but slower)\n",
    "        \"\"\"\n",
    "        try:\n",
    "            print(\"   ðŸ”„ Trying Permutation-based explanation...\")\n",
    "            \n",
    "            # Use smaller sample for permutation\n",
    "            sample_size = min(200, len(X_clean))\n",
    "            X_sample = X_clean.sample(n=sample_size, random_state=42)\n",
    "            \n",
    "            # Create permutation explainer\n",
    "            explainer = shap.Explainer(model.predict_proba, X_sample)\n",
    "            \n",
    "            # Get SHAP values\n",
    "            shap_values = explainer(X_sample)\n",
    "            \n",
    "            # Extract importance values\n",
    "            if hasattr(shap_values, 'values'):\n",
    "                if shap_values.values.ndim == 3:\n",
    "                    # Multi-class: average across classes and samples\n",
    "                    shap_import = np.abs(shap_values.values).mean(axis=(0, 1))\n",
    "                else:\n",
    "                    # Binary: average across samples\n",
    "                    shap_import = np.abs(shap_values.values).mean(axis=0)\n",
    "            else:\n",
    "                return False\n",
    "            \n",
    "            # Create importance DataFrame\n",
    "            importance_df = pd.DataFrame({\n",
    "                'feature': X_clean.columns,\n",
    "                'importance': shap_import\n",
    "            }).sort_values('importance', ascending=False)\n",
    "            \n",
    "            # Store results\n",
    "            self.selected_features = importance_df.head(self.n_features)['feature'].tolist()\n",
    "            self.feature_importance = importance_df\n",
    "            self.debug_info['shap_method'] = 'Permutation'\n",
    "            \n",
    "            return True\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"   âš ï¸  Permutation explainer failed: {e}\")\n",
    "            return False\n",
    "    \n",
    "    def _try_linear_approximation(self, X_clean: pd.DataFrame, y: np.ndarray) -> bool:\n",
    "        \"\"\"\n",
    "        Try linear approximation using feature importance from tree models\n",
    "        \"\"\"\n",
    "        try:\n",
    "            print(\"   ðŸ“ˆ Trying linear approximation of feature importance...\")\n",
    "            \n",
    "            # Use tree-based feature importance as proxy\n",
    "            model = RandomForestClassifier(\n",
    "                n_estimators=100,\n",
    "                random_state=42,\n",
    "                n_jobs=-1\n",
    "            )\n",
    "            model.fit(X_clean, y)\n",
    "            \n",
    "            # Get feature importance\n",
    "            feature_importance = model.feature_importances_\n",
    "            \n",
    "            # Create importance DataFrame\n",
    "            importance_df = pd.DataFrame({\n",
    "                'feature': X_clean.columns,\n",
    "                'importance': feature_importance\n",
    "            }).sort_values('importance', ascending=False)\n",
    "            \n",
    "            # Store results\n",
    "            self.selected_features = importance_df.head(self.n_features)['feature'].tolist()\n",
    "            self.feature_importance = importance_df\n",
    "            self.debug_info['shap_method'] = 'Linear_Approximation'\n",
    "            \n",
    "            return True\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"   âš ï¸  Linear approximation failed: {e}\")\n",
    "            return False\n",
    "    \n",
    "    def _fallback_mutual_information(self, X_clean: pd.DataFrame, y: np.ndarray) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Fallback to mutual information feature selection\n",
    "        \"\"\"\n",
    "        print(\"   ðŸ”„ Using Mutual Information feature selection...\")\n",
    "        \n",
    "        try:\n",
    "            # Calculate mutual information scores\n",
    "            mi_scores = mutual_info_classif(\n",
    "                X_clean, y, \n",
    "                discrete_features='auto',\n",
    "                random_state=42\n",
    "            )\n",
    "            \n",
    "            # Create importance DataFrame\n",
    "            importance_df = pd.DataFrame({\n",
    "                'feature': X_clean.columns,\n",
    "                'importance': mi_scores\n",
    "            }).sort_values('importance', ascending=False)\n",
    "            \n",
    "            # Store results\n",
    "            self.selected_features = importance_df.head(self.n_features)['feature'].tolist()\n",
    "            self.feature_importance = importance_df\n",
    "            self.debug_info['shap_method'] = 'Mutual_Information'\n",
    "            \n",
    "            print(f\"   âœ… Selected {len(self.selected_features)} features via MI\")\n",
    "            print(\"   Top 5 features:\", self.selected_features[:5])\n",
    "            \n",
    "            return X_clean[self.selected_features]\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"   âš ï¸  Mutual Information failed: {e}\")\n",
    "            # Last resort: select top features by variance\n",
    "            return self._last_resort_selection(X_clean)\n",
    "    \n",
    "    def _last_resort_selection(self, X_clean: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Last resort: select features by variance\n",
    "        \"\"\"\n",
    "        print(\"   ðŸ“Š Last resort: selecting by variance...\")\n",
    "        \n",
    "        # Calculate variance for each feature\n",
    "        variances = X_clean.var()\n",
    "        \n",
    "        # Select top features by variance\n",
    "        top_features = variances.nlargest(self.n_features).index.tolist()\n",
    "        \n",
    "        self.selected_features = top_features\n",
    "        self.feature_importance = pd.DataFrame({\n",
    "            'feature': top_features,\n",
    "            'importance': variances[top_features].values\n",
    "        })\n",
    "        self.debug_info['shap_method'] = 'Variance_Selection'\n",
    "        \n",
    "        print(f\"   âœ… Selected {len(self.selected_features)} features by variance\")\n",
    "        \n",
    "        return X_clean[self.selected_features]\n",
    "    \n",
    "    def get_debug_info(self) -> Dict:\n",
    "        \"\"\"\n",
    "        Get debugging information about the feature selection process\n",
    "        \"\"\"\n",
    "        return self.debug_info\n",
    "\n",
    "class TemporalValidator:\n",
    "    \"\"\"Advanced temporal validation for financial ML\"\"\"\n",
    "    \n",
    "    def __init__(self, n_splits: int = 5, embargo_days: int = 21):\n",
    "        self.n_splits = n_splits\n",
    "        self.embargo_days = embargo_days\n",
    "        \n",
    "    def time_series_split_with_embargo(self, X: pd.DataFrame, y: np.ndarray, \n",
    "                                     date_column: str = 'DateCreated'):\n",
    "        \"\"\"Create time series splits with embargo periods\"\"\"\n",
    "        \n",
    "        if date_column not in X.columns:\n",
    "            # Fallback to regular time series split\n",
    "            tscv = TimeSeriesSplit(n_splits=self.n_splits)\n",
    "            for train_idx, test_idx in tscv.split(X):\n",
    "                yield train_idx, test_idx\n",
    "            return\n",
    "        \n",
    "        # Sort by date\n",
    "        X_sorted = X.sort_values(date_column)\n",
    "        y_sorted = y[X_sorted.index]\n",
    "        dates = pd.to_datetime(X_sorted[date_column])\n",
    "        \n",
    "        # Create splits with embargo\n",
    "        n_samples = len(X_sorted)\n",
    "        test_size = n_samples // (self.n_splits + 1)\n",
    "        \n",
    "        for i in range(self.n_splits):\n",
    "            # Define test period\n",
    "            test_start = (i + 1) * test_size\n",
    "            test_end = test_start + test_size\n",
    "            \n",
    "            if test_end > n_samples:\n",
    "                break\n",
    "            \n",
    "            # Add embargo period\n",
    "            embargo_date = dates.iloc[test_start - 1] + timedelta(days=self.embargo_days)\n",
    "            valid_test_mask = dates.iloc[test_start:test_end] > embargo_date\n",
    "            \n",
    "            train_idx = X_sorted.iloc[:test_start].index.tolist()\n",
    "            test_idx = X_sorted.iloc[test_start:test_end][valid_test_mask].index.tolist()\n",
    "            \n",
    "            if len(test_idx) > 0:\n",
    "                yield train_idx, test_idx\n",
    "\n",
    "class AttentionNetwork(nn.Module):\n",
    "    def __init__(self, input_dim: int, n_meta_models: int, attention_dim: int):\n",
    "        super().__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.n_meta_models = n_meta_models\n",
    "        self.attention_dim = attention_dim\n",
    "        \n",
    "        self.attention = nn.Sequential(\n",
    "            nn.Linear(input_dim, attention_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(attention_dim, attention_dim // 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.1),\n",
    "            nn.Linear(attention_dim // 2, n_meta_models),\n",
    "            nn.Softmax(dim=1)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.attention(x)\n",
    "\n",
    "class EnhancedFinancialMLSystem:\n",
    "    \"\"\"Complete enhanced financial ML system\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.feature_engineer = FinancialFeatureEngineer()\n",
    "        self.feature_selector = SHAPFeatureSelector(n_features=60)\n",
    "        self.imbalance_handler = AdvancedImbalanceHandler(strategy='hybrid')\n",
    "        self.temporal_validator = TemporalValidator()\n",
    "        self.ensemble = None\n",
    "        self.scaler = StandardScaler()\n",
    "        self.label_encoder = LabelEncoder()\n",
    "        self.is_fitted = False\n",
    "        \n",
    "    def prepare_data(self, df: pd.DataFrame, target_column: str = 'Target_SubCategory'):\n",
    "        \"\"\"Complete data preparation pipeline\"\"\"\n",
    "        \n",
    "        print(\"ðŸš€ Starting enhanced data preparation...\")\n",
    "        \n",
    "        # Separate features and target\n",
    "        if target_column not in df.columns:\n",
    "            raise ValueError(f\"Target column '{target_column}' not found in DataFrame\")\n",
    "            \n",
    "        y = df[target_column]\n",
    "        X = df.drop(columns=[target_column, 'ClientId', 'DateCreated'], errors='ignore')\n",
    "        \n",
    "        # Feature engineering\n",
    "        X_enhanced = self.feature_engineer.create_financial_features(X)\n",
    "        \n",
    "        # Handle missing values\n",
    "        numeric_columns = X_enhanced.select_dtypes(include=[np.number]).columns\n",
    "        categorical_columns = X_enhanced.select_dtypes(include=['object']).columns\n",
    "        \n",
    "        # Fill numeric missing values with median\n",
    "        if len(numeric_columns) > 0:\n",
    "            X_enhanced[numeric_columns] = X_enhanced[numeric_columns].fillna(\n",
    "                X_enhanced[numeric_columns].median()\n",
    "            )\n",
    "        \n",
    "        # Fill categorical missing values with mode\n",
    "        if len(categorical_columns) > 0:\n",
    "            for col in categorical_columns:\n",
    "                mode_value = X_enhanced[col].mode()\n",
    "                if len(mode_value) > 0:\n",
    "                    X_enhanced[col] = X_enhanced[col].fillna(mode_value.iloc[0])\n",
    "                else:\n",
    "                    X_enhanced[col] = X_enhanced[col].fillna('Unknown')\n",
    "        \n",
    "        # Encode categorical variables\n",
    "        for col in categorical_columns:\n",
    "            if col in X_enhanced.columns:\n",
    "                le = LabelEncoder()\n",
    "                X_enhanced[col] = le.fit_transform(X_enhanced[col].astype(str))\n",
    "        \n",
    "        # Encode target\n",
    "        y_encoded = self.label_encoder.fit_transform(y)\n",
    "        \n",
    "        print(f\"   âœ… Prepared data: {X_enhanced.shape[0]} samples, {X_enhanced.shape[1]} features\")\n",
    "        \n",
    "        return X_enhanced, y_encoded\n",
    "    \n",
    "    def build_enhanced_ensemble(self, n_classes: int):\n",
    "        \"\"\"Build the enhanced ensemble architecture\"\"\"\n",
    "        \n",
    "        print(\"ðŸ—ï¸ Building enhanced ensemble architecture...\")\n",
    "        \n",
    "        # Level-0: Diverse base learners with optimized parameters\n",
    "        base_models = [\n",
    "            RandomForestClassifier(\n",
    "                n_estimators=500,\n",
    "                max_depth=15,\n",
    "                min_samples_split=20,\n",
    "                min_samples_leaf=10,\n",
    "                max_features='sqrt',\n",
    "                class_weight='balanced_subsample',\n",
    "                random_state=42,\n",
    "                n_jobs=-1\n",
    "            ),\n",
    "            XGBClassifier(\n",
    "                n_estimators=300,\n",
    "                max_depth=8,\n",
    "                learning_rate=0.05,\n",
    "                subsample=0.8,\n",
    "                colsample_bytree=0.8,\n",
    "                reg_alpha=1,\n",
    "                reg_lambda=1,\n",
    "                random_state=42,\n",
    "                n_jobs=-1\n",
    "            ),\n",
    "            LGBMClassifier(\n",
    "                n_estimators=300,\n",
    "                max_depth=12,\n",
    "                learning_rate=0.05,\n",
    "                num_leaves=31,\n",
    "                min_child_samples=20,\n",
    "                subsample=0.8,\n",
    "                colsample_bytree=0.8,\n",
    "                class_weight='balanced',\n",
    "                random_state=42,\n",
    "                n_jobs=-1\n",
    "            ),\n",
    "            cb.CatBoostClassifier(\n",
    "                iterations=200,\n",
    "                depth=8,\n",
    "                learning_rate=0.05,\n",
    "                random_seed=42,\n",
    "                verbose=False,\n",
    "                auto_class_weights='Balanced'\n",
    "            ),\n",
    "            MLPClassifier(\n",
    "                hidden_layer_sizes=(256, 128, 64),\n",
    "                alpha=0.01,\n",
    "                learning_rate_init=0.001,\n",
    "                max_iter=300,\n",
    "                random_state=42\n",
    "            )\n",
    "        ]\n",
    "        \n",
    "        # Level-1: Meta models\n",
    "        meta_models = [\n",
    "            LogisticRegression(\n",
    "                C=1.0,\n",
    "                multi_class='multinomial',\n",
    "                max_iter=1000,\n",
    "                class_weight='balanced',\n",
    "                random_state=42\n",
    "            ),\n",
    "            XGBClassifier(\n",
    "                n_estimators=100,\n",
    "                max_depth=4,\n",
    "                learning_rate=0.05,\n",
    "                random_state=42\n",
    "            ),\n",
    "            MLPClassifier(\n",
    "                hidden_layer_sizes=(64, 32),\n",
    "                alpha=0.01,\n",
    "                max_iter=200,\n",
    "                random_state=42\n",
    "            )\n",
    "        ]\n",
    "        \n",
    "        # Build dynamic weight ensemble\n",
    "        self.ensemble = DynamicWeightEnsemble(\n",
    "            base_models=base_models,\n",
    "            meta_models=meta_models,\n",
    "            attention_dim=64\n",
    "        )\n",
    "        \n",
    "        print(f\"   âœ… Built ensemble with {len(base_models)} base models and {len(meta_models)} meta models\")\n",
    "    \n",
    "    def train(self, X: pd.DataFrame, y: np.ndarray, use_temporal_validation: bool = True):\n",
    "        \"\"\"Train the complete system\"\"\"\n",
    "        \n",
    "        print(\"ðŸŽ¯ Training Enhanced Financial ML System...\")\n",
    "        \n",
    "        # Feature selection\n",
    "        X_selected = self.feature_selector.fit_select(X, y)\n",
    "        \n",
    "        # Scale features\n",
    "        X_scaled = pd.DataFrame(\n",
    "            self.scaler.fit_transform(X_selected),\n",
    "            columns=X_selected.columns,\n",
    "            index=X_selected.index\n",
    "        )\n",
    "        \n",
    "        # Handle class imbalance\n",
    "        X_resampled, y_resampled = self.imbalance_handler.apply_sampling(\n",
    "            X_scaled.values, y\n",
    "        )\n",
    "        \n",
    "        # Build ensemble\n",
    "        self.build_enhanced_ensemble(len(np.unique(y)))\n",
    "        \n",
    "        # Train ensemble\n",
    "        self.ensemble.fit(X_resampled, y_resampled)\n",
    "        \n",
    "        # Alternative calibration approach to avoid the cloning issue\n",
    "        print(\"ðŸŽ¯ Creating calibrated predictions...\")\n",
    "        try:\n",
    "            # Try standard calibration first\n",
    "            self.calibrated_ensemble = CalibratedClassifierCV(\n",
    "                self.ensemble,\n",
    "                method='sigmoid',\n",
    "                cv=3\n",
    "            )\n",
    "            self.calibrated_ensemble.fit(X_scaled.values, y)\n",
    "            print(\"   âœ… Standard calibration successful\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"   âš ï¸  Standard calibration failed: {e}\")\n",
    "            print(\"   Using ensemble without calibration (still very effective)\")\n",
    "            \n",
    "            # Use the ensemble directly without calibration\n",
    "            self.calibrated_ensemble = self.ensemble\n",
    "        \n",
    "        self.is_fitted = True\n",
    "        print(\"   âœ… Training complete!\")\n",
    "    \n",
    "    def predict_with_confidence(self, X: pd.DataFrame, k: int = 3) -> List[Tuple]:\n",
    "        \"\"\"\n",
    "        FIXED: Predict top-k recommendations with proper categorical handling\n",
    "        \"\"\"\n",
    "        \n",
    "        if not self.is_fitted:\n",
    "            raise ValueError(\"Model must be fitted before prediction\")\n",
    "        \n",
    "        print(\"ðŸ”§ Creating advanced financial features...\")\n",
    "        \n",
    "        # Step 1: Apply feature engineering (same as training)\n",
    "        X_enhanced = self.feature_engineer.create_financial_features(X.copy())\n",
    "        \n",
    "        # Step 2: Handle categorical variables BEFORE feature selection\n",
    "        # This is the missing piece that causes the error!\n",
    "        \n",
    "        # Handle missing values first\n",
    "        numeric_columns = X_enhanced.select_dtypes(include=[np.number]).columns\n",
    "        categorical_columns = X_enhanced.select_dtypes(include=['object']).columns\n",
    "        \n",
    "        # Fill numeric missing values with median (use training medians if available)\n",
    "        if len(numeric_columns) > 0:\n",
    "            for col in numeric_columns:\n",
    "                if col in X_enhanced.columns:\n",
    "                    if hasattr(self, 'training_medians') and col in self.training_medians:\n",
    "                        # Use training median if available\n",
    "                        X_enhanced[col] = X_enhanced[col].fillna(self.training_medians[col])\n",
    "                    else:\n",
    "                        # Fallback to current median\n",
    "                        X_enhanced[col] = X_enhanced[col].fillna(X_enhanced[col].median())\n",
    "        \n",
    "        # Handle categorical variables (CRITICAL FIX)\n",
    "        if len(categorical_columns) > 0:\n",
    "            for col in categorical_columns:\n",
    "                if col in X_enhanced.columns:\n",
    "                    # Fill missing values first\n",
    "                    mode_value = X_enhanced[col].mode()\n",
    "                    if len(mode_value) > 0:\n",
    "                        X_enhanced[col] = X_enhanced[col].fillna(mode_value.iloc[0])\n",
    "                    else:\n",
    "                        X_enhanced[col] = X_enhanced[col].fillna('Unknown')\n",
    "                    \n",
    "                    # Encode categorical variables\n",
    "                    if hasattr(self, 'label_encoders') and col in self.label_encoders:\n",
    "                        # Use training encoder if available\n",
    "                        le = self.label_encoders[col]\n",
    "                        try:\n",
    "                            # Handle unseen categories\n",
    "                            X_enhanced[col] = X_enhanced[col].apply(\n",
    "                                lambda x: le.transform([x])[0] if x in le.classes_ else 0\n",
    "                            )\n",
    "                        except:\n",
    "                            # If encoding fails, create new encoder\n",
    "                            le_new = LabelEncoder()\n",
    "                            X_enhanced[col] = le_new.fit_transform(X_enhanced[col].astype(str))\n",
    "                    else:\n",
    "                        # Create new encoder (not ideal but works)\n",
    "                        le = LabelEncoder()\n",
    "                        X_enhanced[col] = le.fit_transform(X_enhanced[col].astype(str))\n",
    "        \n",
    "        # Step 3: Select only the features used in training\n",
    "        try:\n",
    "            X_selected = X_enhanced[self.feature_selector.selected_features]\n",
    "        except KeyError as e:\n",
    "            print(f\"   âš ï¸  Missing features in prediction data: {e}\")\n",
    "            # Handle missing features by creating them with default values\n",
    "            missing_features = set(self.feature_selector.selected_features) - set(X_enhanced.columns)\n",
    "            for feature in missing_features:\n",
    "                X_enhanced[feature] = 0  # Default value for missing features\n",
    "            X_selected = X_enhanced[self.feature_selector.selected_features]\n",
    "        \n",
    "        # Step 4: Scale features using training scaler\n",
    "        X_scaled = self.scaler.transform(X_selected)\n",
    "        \n",
    "        # Step 5: Get predictions\n",
    "        pred_proba = self.calibrated_ensemble.predict_proba(X_scaled)\n",
    "        \n",
    "        # Step 6: Generate recommendations\n",
    "        recommendations = []\n",
    "        for i, proba in enumerate(pred_proba):\n",
    "            # Get top-k predictions\n",
    "            top_k_indices = np.argsort(proba)[-k:][::-1]\n",
    "            top_k_probs = proba[top_k_indices]\n",
    "            top_k_classes = self.label_encoder.inverse_transform(top_k_indices)\n",
    "            \n",
    "            # Add business reasoning\n",
    "            client_recs = []\n",
    "            for j, (class_name, confidence) in enumerate(zip(top_k_classes, top_k_probs)):\n",
    "                reasoning = self._generate_business_reasoning(X.iloc[i], class_name, confidence)\n",
    "                client_recs.append((class_name, confidence, reasoning))\n",
    "            \n",
    "            recommendations.append(client_recs)\n",
    "        \n",
    "        return recommendations\n",
    "\n",
    "    def _generate_business_reasoning(self, client_data: pd.Series, product: str, confidence: float) -> str:\n",
    "        \"\"\"Generate business reasoning for recommendations\"\"\"\n",
    "        \n",
    "        reasons = []\n",
    "        \n",
    "        # Age-based reasoning\n",
    "        age = client_data.get('ClientAge', 40)\n",
    "        if age < 35 and 'Term' in product:\n",
    "            reasons.append(\"Young professional - basic protection priority\")\n",
    "        elif age > 50 and 'Retirement' in product:\n",
    "            reasons.append(\"Pre-retirement planning phase\")\n",
    "        elif 35 <= age <= 50 and 'Investment' in product:\n",
    "            reasons.append(\"Peak earning years - wealth accumulation opportunity\")\n",
    "        \n",
    "        # Income-based reasoning\n",
    "        income = client_data.get('Income_Numeric', 0)\n",
    "        if income > 100000 and 'Investment' in product:\n",
    "            reasons.append(\"High income - sophisticated investment suitable\")\n",
    "        elif income < 50000 and 'Shield' in product:\n",
    "            reasons.append(\"Essential healthcare protection need\")\n",
    "        \n",
    "        # Gap analysis reasoning\n",
    "        gap_score = client_data.get('Protection_Gap_Score', 0)\n",
    "        if gap_score > 0.5:\n",
    "            reasons.append(\"Significant protection gaps identified\")\n",
    "        \n",
    "        # Sophistication reasoning\n",
    "        sophistication = client_data.get('Advanced_Sophistication_Score', 0)\n",
    "        if sophistication > 5 and 'Investment' in product:\n",
    "            reasons.append(\"High financial sophistication - complex products suitable\")\n",
    "        \n",
    "        # Default reasoning\n",
    "        if not reasons:\n",
    "            if confidence > 0.7:\n",
    "                reasons.append(\"Strong profile match based on comprehensive analysis\")\n",
    "            elif confidence > 0.4:\n",
    "                reasons.append(\"Good fit based on client profile\")\n",
    "            else:\n",
    "                reasons.append(\"Potential opportunity for financial planning discussion\")\n",
    "        \n",
    "        return \"; \".join(reasons)\n",
    "    \n",
    "    def evaluate_comprehensive(self, X: pd.DataFrame, y: np.ndarray, \n",
    "                             use_temporal_validation: bool = True) -> Dict:\n",
    "        \"\"\"Comprehensive model evaluation with business metrics\"\"\"\n",
    "        \n",
    "        print(\"ðŸ“Š Running comprehensive evaluation...\")\n",
    "        \n",
    "        if use_temporal_validation and 'DateCreated' in X.columns:\n",
    "            cv_scores = []\n",
    "            for train_idx, test_idx in self.temporal_validator.time_series_split_with_embargo(X, y):\n",
    "                X_train, X_test = X.iloc[train_idx], X.iloc[test_idx]\n",
    "                y_train, y_test = y[train_idx], y[test_idx]\n",
    "                \n",
    "                # Train on fold\n",
    "                temp_system = EnhancedFinancialMLSystem()\n",
    "                X_train_prep, y_train_prep = temp_system.prepare_data(\n",
    "                    pd.concat([X_train, pd.DataFrame({'Target_SubCategory': y_train})], axis=1)\n",
    "                )\n",
    "                temp_system.train(X_train_prep, y_train_prep, use_temporal_validation=False)\n",
    "                \n",
    "                # Evaluate on fold\n",
    "                X_test_prep, y_test_prep = temp_system.prepare_data(\n",
    "                    pd.concat([X_test, pd.DataFrame({'Target_SubCategory': y_test})], axis=1)\n",
    "                )\n",
    "                \n",
    "                fold_metrics = self._calculate_metrics(temp_system, X_test_prep, y_test_prep)\n",
    "                cv_scores.append(fold_metrics)\n",
    "            \n",
    "            # Average metrics across folds\n",
    "            avg_metrics = {}\n",
    "            for metric in cv_scores[0].keys():\n",
    "                avg_metrics[metric] = np.mean([fold[metric] for fold in cv_scores])\n",
    "                avg_metrics[f\"{metric}_std\"] = np.std([fold[metric] for fold in cv_scores])\n",
    "            \n",
    "            return avg_metrics\n",
    "        \n",
    "        else:\n",
    "            # Single evaluation\n",
    "            return self._calculate_metrics(self, X, y)\n",
    "    \n",
    "    def _calculate_metrics(self, model, X: pd.DataFrame, y: np.ndarray) -> Dict:\n",
    "        \"\"\"Calculate comprehensive metrics\"\"\"\n",
    "        \n",
    "        # Get predictions\n",
    "        pred_proba = model.calibrated_ensemble.predict_proba(\n",
    "            model.scaler.transform(X[model.feature_selector.selected_features])\n",
    "        )\n",
    "        y_pred = np.argmax(pred_proba, axis=1)\n",
    "        \n",
    "        # Basic metrics\n",
    "        accuracy = accuracy_score(y, y_pred)\n",
    "        mcc = matthews_corrcoef(y, y_pred)\n",
    "        \n",
    "        # Top-k accuracy\n",
    "        def top_k_accuracy(y_true, y_pred_proba, k):\n",
    "            top_k_preds = np.argsort(y_pred_proba, axis=1)[:, -k:]\n",
    "            return np.mean([y_true[i] in top_k_preds[i] for i in range(len(y_true))])\n",
    "        \n",
    "        top1_acc = accuracy\n",
    "        top3_acc = top_k_accuracy(y, pred_proba, k=3)\n",
    "        top5_acc = top_k_accuracy(y, pred_proba, k=5)\n",
    "        \n",
    "        # Business metrics\n",
    "        coverage = top3_acc  # Business coverage = top-3 accuracy\n",
    "        avg_confidence = np.mean(np.max(pred_proba, axis=1))\n",
    "        \n",
    "        # Diversity metric\n",
    "        top3_preds = np.argsort(pred_proba, axis=1)[:, -3:]\n",
    "        unique_recommendations = len(np.unique(top3_preds))\n",
    "        diversity_score = unique_recommendations / (3 * len(y))\n",
    "        \n",
    "        # Calibration quality (Brier score)\n",
    "        n_classes = len(np.unique(y))\n",
    "        y_onehot = np.eye(n_classes)[y]\n",
    "        brier_score = np.mean(np.sum((pred_proba - y_onehot) ** 2, axis=1))\n",
    "        \n",
    "        return {\n",
    "            'accuracy': accuracy,\n",
    "            'top1_accuracy': top1_acc,\n",
    "            'top3_accuracy': top3_acc,\n",
    "            'top5_accuracy': top5_acc,\n",
    "            'matthews_corr_coef': mcc,\n",
    "            'coverage': coverage,\n",
    "            'avg_confidence': avg_confidence,\n",
    "            'diversity_score': diversity_score,\n",
    "            'brier_score': brier_score,\n",
    "            'calibration_quality': 1 - brier_score  # Higher is better\n",
    "        }\n",
    "\n",
    "\n",
    "def hyperparameter_optimization(X: pd.DataFrame, y: np.ndarray, n_trials: int = 50):\n",
    "    \"\"\"Optuna-based hyperparameter optimization\"\"\"\n",
    "    \n",
    "    def objective(trial):\n",
    "        # Suggest hyperparameters\n",
    "        params = {\n",
    "            'rf_n_estimators': trial.suggest_int('rf_n_estimators', 300, 800),\n",
    "            'rf_max_depth': trial.suggest_int('rf_max_depth', 10, 25),\n",
    "            'xgb_learning_rate': trial.suggest_float('xgb_learning_rate', 0.01, 0.2),\n",
    "            'xgb_max_depth': trial.suggest_int('xgb_max_depth', 4, 12),\n",
    "            'lgbm_num_leaves': trial.suggest_int('lgbm_num_leaves', 20, 50),\n",
    "            'attention_dim': trial.suggest_int('attention_dim', 32, 128),\n",
    "            'n_features': trial.suggest_int('n_features', 40, 80)\n",
    "        }\n",
    "        \n",
    "        # Create system with suggested parameters\n",
    "        system = EnhancedFinancialMLSystem()\n",
    "        system.feature_selector.n_features = params['n_features']\n",
    "        \n",
    "        # Quick evaluation with reduced data for speed\n",
    "        sample_size = min(10000, len(X))\n",
    "        X_sample = X.sample(n=sample_size, random_state=42)\n",
    "        y_sample = y[X_sample.index]\n",
    "        \n",
    "        try:\n",
    "            X_prep, y_prep = system.prepare_data(\n",
    "                pd.concat([X_sample, pd.DataFrame({'Target_SubCategory': y_sample})], axis=1)\n",
    "            )\n",
    "            \n",
    "            # Quick train-test split\n",
    "            from sklearn.model_selection import train_test_split\n",
    "            X_train, X_test, y_train, y_test = train_test_split(\n",
    "                X_prep, y_prep, test_size=0.2, random_state=42, stratify=y_prep\n",
    "            )\n",
    "            \n",
    "            system.train(X_train, y_train, use_temporal_validation=False)\n",
    "            metrics = system._calculate_metrics(system, X_test, y_test)\n",
    "            \n",
    "            # Optimize for top-3 accuracy\n",
    "            return metrics['top3_accuracy']\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"Trial failed: {e}\")\n",
    "            return 0.0\n",
    "    \n",
    "    print(\"ðŸ” Starting hyperparameter optimization...\")\n",
    "    study = optuna.create_study(direction='maximize')\n",
    "    study.optimize(objective, n_trials=n_trials)\n",
    "    \n",
    "    print(f\"   Best top-3 accuracy: {study.best_value:.4f}\")\n",
    "    print(f\"   Best parameters: {study.best_params}\")\n",
    "    \n",
    "    return study.best_params\n",
    "\n",
    "# SAFE PREDICTION METHOD (handles any categorical data)\n",
    "def safe_predict_with_confidence(system, X: pd.DataFrame, k: int = 3) -> List[Tuple]:\n",
    "    \"\"\"\n",
    "    Safe prediction method that handles categorical data robustly\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"ðŸ”§ Safe prediction with robust categorical handling...\")\n",
    "    \n",
    "    try:\n",
    "        # Step 1: Create a copy and handle basic preprocessing\n",
    "        X_processed = X.copy()\n",
    "        \n",
    "        print(f\"   ðŸ“Š Input data shape: {X_processed.shape}\")\n",
    "        print(f\"   ðŸ“Š Input columns: {X_processed.columns.tolist()}\")\n",
    "        \n",
    "        # Step 2: Aggressive NaN handling for all columns\n",
    "        for col in X_processed.columns:\n",
    "            if X_processed[col].dtype == 'object':\n",
    "                # Handle categorical columns\n",
    "                X_processed[col] = X_processed[col].fillna('Unknown')\n",
    "                # Convert to hash-based numeric encoding\n",
    "                X_processed[col] = X_processed[col].apply(\n",
    "                    lambda x: abs(hash(str(x))) % 1000 if pd.notna(x) else 0\n",
    "                )\n",
    "            else:\n",
    "                # Handle numeric columns\n",
    "                if X_processed[col].isna().any():\n",
    "                    median_val = X_processed[col].median()\n",
    "                    if pd.isna(median_val):\n",
    "                        median_val = 0  # Ultimate fallback\n",
    "                    X_processed[col] = X_processed[col].fillna(median_val)\n",
    "                \n",
    "                # Handle infinite values\n",
    "                X_processed[col] = X_processed[col].replace([np.inf, -np.inf], 0)\n",
    "        \n",
    "        print(f\"   âœ… Basic preprocessing complete\")\n",
    "        \n",
    "        # Step 3: Apply feature engineering with NaN protection\n",
    "        try:\n",
    "            X_enhanced = system.feature_engineer.create_financial_features(X_processed)\n",
    "            print(f\"   âœ… Feature engineering complete: {X_enhanced.shape}\")\n",
    "        except Exception as e:\n",
    "            print(f\"   âš ï¸  Feature engineering failed: {e}\")\n",
    "            # Use processed data as is\n",
    "            X_enhanced = X_processed.copy()\n",
    "        \n",
    "        # Step 4: Comprehensive NaN cleaning for all columns\n",
    "        for col in X_enhanced.columns:\n",
    "            if not pd.api.types.is_numeric_dtype(X_enhanced[col]):\n",
    "                # Force conversion to numeric\n",
    "                X_enhanced[col] = pd.to_numeric(X_enhanced[col], errors='coerce')\n",
    "            \n",
    "            # Fill any remaining NaN values\n",
    "            if X_enhanced[col].isna().any():\n",
    "                fill_value = X_enhanced[col].median()\n",
    "                if pd.isna(fill_value):\n",
    "                    fill_value = 0\n",
    "                X_enhanced[col] = X_enhanced[col].fillna(fill_value)\n",
    "            \n",
    "            # Final infinite value check\n",
    "            X_enhanced[col] = X_enhanced[col].replace([np.inf, -np.inf], 0)\n",
    "        \n",
    "        print(f\"   âœ… Comprehensive NaN cleaning complete\")\n",
    "        \n",
    "        # Step 5: Handle feature alignment with training features\n",
    "        required_features = system.feature_selector.selected_features\n",
    "        print(f\"   ðŸ“Š Required features: {len(required_features)}\")\n",
    "        \n",
    "        # Create missing features with zeros\n",
    "        for feature in required_features:\n",
    "            if feature not in X_enhanced.columns:\n",
    "                X_enhanced[feature] = 0.0  # Explicit float\n",
    "                print(f\"   ðŸ”§ Created missing feature: {feature}\")\n",
    "        \n",
    "        # Select only required features\n",
    "        X_selected = X_enhanced[required_features].copy()\n",
    "        \n",
    "        # Step 6: Final NaN validation before scaling\n",
    "        if X_selected.isna().any().any():\n",
    "            print(f\"   âš ï¸  Found remaining NaNs, applying final cleanup\")\n",
    "            X_selected = X_selected.fillna(0.0)\n",
    "        \n",
    "        print(f\"   ðŸ“Š Features selected: {X_selected.shape}\")\n",
    "        print(f\"   ðŸ“Š NaN check: {X_selected.isna().sum().sum()} NaNs remaining\")\n",
    "        \n",
    "        # Step 7: Scale features\n",
    "        try:\n",
    "            X_scaled = system.scaler.transform(X_selected)\n",
    "            print(f\"   âœ… Feature scaling complete\")\n",
    "        except Exception as e:\n",
    "            print(f\"   âš ï¸  Scaling failed: {e}\")\n",
    "            # Use unscaled data as fallback\n",
    "            X_scaled = X_selected.values\n",
    "        \n",
    "        # Step 8: Final NaN check on scaled data\n",
    "        if np.isnan(X_scaled).any():\n",
    "            print(f\"   âš ï¸  NaNs detected in scaled data, cleaning...\")\n",
    "            X_scaled = np.nan_to_num(X_scaled, nan=0.0, posinf=0.0, neginf=0.0)\n",
    "        \n",
    "        print(f\"   ðŸ“Š Final data shape: {X_scaled.shape}\")\n",
    "        print(f\"   ðŸ“Š Final NaN check: {np.isnan(X_scaled).sum()} NaNs\")\n",
    "        \n",
    "        # Step 9: Get predictions\n",
    "        pred_proba = system.calibrated_ensemble.predict_proba(X_scaled)\n",
    "        print(f\"   âœ… Predictions obtained successfully\")\n",
    "        \n",
    "        # Step 10: Generate recommendations\n",
    "        recommendations = []\n",
    "        for i, proba in enumerate(pred_proba):\n",
    "            top_k_indices = np.argsort(proba)[-k:][::-1]\n",
    "            top_k_probs = proba[top_k_indices]\n",
    "            top_k_classes = system.label_encoder.inverse_transform(top_k_indices)\n",
    "            \n",
    "            client_recs = []\n",
    "            for j, (class_name, confidence) in enumerate(zip(top_k_classes, top_k_probs)):\n",
    "                reasoning = f\"Confidence: {confidence:.1%} based on enhanced profile analysis\"\n",
    "                client_recs.append((class_name, confidence, reasoning))\n",
    "            \n",
    "            recommendations.append(client_recs)\n",
    "        \n",
    "        print(\"   âœ… Ultra-safe prediction completed successfully!\")\n",
    "        return recommendations\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"   âŒ Ultra-safe prediction failed: {e}\")\n",
    "        # Return dummy prediction to prevent crash\n",
    "        dummy_classes = system.label_encoder.classes_[:k] if len(system.label_encoder.classes_) >= k else system.label_encoder.classes_\n",
    "        dummy_recs = [(cls, 0.33, \"Default prediction due to processing error\") for cls in dummy_classes]\n",
    "        return [dummy_recs]\n",
    "    \n",
    "class SimpleDynamicEnsemble:\n",
    "    \"\"\"\n",
    "    Simplified ensemble without PyTorch - always picklable\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, base_models=None, meta_models=None):\n",
    "        self.base_models = base_models or []\n",
    "        self.meta_models = meta_models or []\n",
    "        self.is_fitted = False\n",
    "        self.n_classes = None\n",
    "        self.classes_ = None\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        \"\"\"Fit the simplified ensemble\"\"\"\n",
    "        \n",
    "        print(\"ðŸ”§ Training Simple Dynamic Ensemble (PyTorch-free)...\")\n",
    "        \n",
    "        self.classes_ = np.unique(y)\n",
    "        self.n_classes = len(self.classes_)\n",
    "        \n",
    "        # Train base models\n",
    "        for i, model in enumerate(self.base_models):\n",
    "            print(f\"   Training base model {i+1}/{len(self.base_models)}: {model.__class__.__name__}\")\n",
    "            model.fit(X, y)\n",
    "        \n",
    "        # Train meta models on base predictions\n",
    "        base_preds = np.zeros((X.shape[0], len(self.base_models), self.n_classes))\n",
    "        for i, model in enumerate(self.base_models):\n",
    "            base_preds[:, i, :] = model.predict_proba(X)\n",
    "        \n",
    "        meta_features = base_preds.reshape(X.shape[0], -1)\n",
    "        \n",
    "        for i, meta_model in enumerate(self.meta_models):\n",
    "            print(f\"   Training meta model {i+1}/{len(self.meta_models)}: {meta_model.__class__.__name__}\")\n",
    "            meta_model.fit(meta_features, y)\n",
    "        \n",
    "        self.is_fitted = True\n",
    "        return self\n",
    "    \n",
    "    def predict_proba(self, X):\n",
    "        \"\"\"Simple ensemble prediction - average meta model outputs\"\"\"\n",
    "        \n",
    "        # Get base predictions\n",
    "        base_preds = np.zeros((X.shape[0], len(self.base_models), self.n_classes))\n",
    "        for i, model in enumerate(self.base_models):\n",
    "            base_preds[:, i, :] = model.predict_proba(X)\n",
    "        \n",
    "        # Get meta predictions\n",
    "        meta_features = base_preds.reshape(X.shape[0], -1)\n",
    "        meta_predictions = []\n",
    "        \n",
    "        for meta_model in self.meta_models:\n",
    "            meta_pred = meta_model.predict_proba(meta_features)\n",
    "            meta_predictions.append(meta_pred)\n",
    "        \n",
    "        # Simple average of meta predictions\n",
    "        final_predictions = np.mean(meta_predictions, axis=0)\n",
    "        return final_predictions\n",
    "    \n",
    "    def predict(self, X):\n",
    "        \"\"\"Predict class labels\"\"\"\n",
    "        probas = self.predict_proba(X)\n",
    "        return self.classes_[np.argmax(probas, axis=1)]\n",
    "\n",
    "\n",
    "class UltimatePicklableEnsemble:\n",
    "    \"\"\"Ultimate picklable ensemble - completely serializable\"\"\"\n",
    "    \n",
    "    def __init__(self, base_models=None, meta_models=None, attention_dim=64):\n",
    "        self.base_models = base_models or []\n",
    "        self.meta_models = meta_models or []\n",
    "        self.attention_dim = attention_dim\n",
    "        self.attention_network = None\n",
    "        self.attention_weights_cache = None  # Store computed weights\n",
    "        self.is_fitted = False\n",
    "        self.n_classes = None\n",
    "        self.classes_ = None\n",
    "        \n",
    "        # Store network architecture parameters for reconstruction\n",
    "        self.network_params = {\n",
    "            'input_dim': None,\n",
    "            'n_meta_models': None,\n",
    "            'attention_dim': attention_dim\n",
    "        }\n",
    "        \n",
    "        # Store PyTorch state as dict (picklable)\n",
    "        self.pytorch_state = None\n",
    "        \n",
    "    def get_params(self, deep=True):\n",
    "        return {\n",
    "            'base_models': self.base_models,\n",
    "            'meta_models': self.meta_models,\n",
    "            'attention_dim': self.attention_dim\n",
    "        }\n",
    "    \n",
    "    def set_params(self, **params):\n",
    "        for key, value in params.items():\n",
    "            setattr(self, key, value)\n",
    "        return self\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        \"\"\"Fit ensemble with PyTorch-free fallback\"\"\"\n",
    "        \n",
    "        print(\"ðŸ”§ Training Ultimate Picklable Ensemble...\")\n",
    "        \n",
    "        self.classes_ = np.unique(y)\n",
    "        self.n_classes = len(self.classes_)\n",
    "        \n",
    "        # Train base models\n",
    "        for i, model in enumerate(self.base_models):\n",
    "            print(f\"   Training base model {i+1}/{len(self.base_models)}: {model.__class__.__name__}\")\n",
    "            model.fit(X, y)\n",
    "        \n",
    "        # Train meta models\n",
    "        base_preds = self._get_base_predictions(X)\n",
    "        meta_features = base_preds.reshape(X.shape[0], -1)\n",
    "        \n",
    "        for i, meta_model in enumerate(self.meta_models):\n",
    "            print(f\"   Training meta model {i+1}/{len(self.meta_models)}: {meta_model.__class__.__name__}\")\n",
    "            meta_model.fit(meta_features, y)\n",
    "        \n",
    "        # Try to train attention network, fallback to simple averaging\n",
    "        try:\n",
    "            self._train_attention_network(X, y)\n",
    "            print(\"   âœ… Attention network trained successfully\")\n",
    "        except Exception as e:\n",
    "            print(f\"   âš ï¸  Attention training failed: {e}\")\n",
    "            print(\"   ðŸ”„ Using simple meta-model averaging\")\n",
    "            self.attention_weights_cache = np.ones((len(self.meta_models),)) / len(self.meta_models)\n",
    "        \n",
    "        self.is_fitted = True\n",
    "        return self\n",
    "        \n",
    "    def predict_proba(self, X):\n",
    "        \"\"\"Predict with dynamic or simple weights\"\"\"\n",
    "        \n",
    "        if not self.is_fitted:\n",
    "            raise ValueError(\"Model must be fitted before prediction\")\n",
    "        \n",
    "        # Get base and meta predictions\n",
    "        base_preds = self._get_base_predictions(X)\n",
    "        meta_features = base_preds.reshape(X.shape[0], -1)\n",
    "        \n",
    "        meta_predictions = []\n",
    "        for meta_model in self.meta_models:\n",
    "            meta_pred = meta_model.predict_proba(meta_features)\n",
    "            meta_predictions.append(meta_pred)\n",
    "        \n",
    "        # Apply attention weights (dynamic or static)\n",
    "        if self.pytorch_state is not None:\n",
    "            # Try dynamic attention\n",
    "            try:\n",
    "                attention_weights = self._get_dynamic_attention_weights(X)\n",
    "            except:\n",
    "                # Fallback to simple average\n",
    "                attention_weights = np.ones((X.shape[0], len(self.meta_models))) / len(self.meta_models)\n",
    "        else:\n",
    "            # Use simple average or cached weights\n",
    "            if self.attention_weights_cache is not None:\n",
    "                attention_weights = np.tile(self.attention_weights_cache, (X.shape[0], 1))\n",
    "            else:\n",
    "                attention_weights = np.ones((X.shape[0], len(self.meta_models))) / len(self.meta_models)\n",
    "        \n",
    "        # Weighted combination\n",
    "        final_predictions = np.zeros((X.shape[0], self.n_classes))\n",
    "        for i, meta_pred in enumerate(meta_predictions):\n",
    "            final_predictions += attention_weights[:, i:i+1] * meta_pred\n",
    "        \n",
    "        return final_predictions\n",
    "    \n",
    "    def predict(self, X):\n",
    "        probas = self.predict_proba(X)\n",
    "        return self.classes_[np.argmax(probas, axis=1)]\n",
    "    \n",
    "    def decision_function(self, X):\n",
    "        return self.predict_proba(X)\n",
    "    \n",
    "    def _get_base_predictions(self, X):\n",
    "        \"\"\"Get base model predictions\"\"\"\n",
    "        base_preds = np.zeros((X.shape[0], len(self.base_models), self.n_classes))\n",
    "        for i, model in enumerate(self.base_models):\n",
    "            base_preds[:, i, :] = model.predict_proba(X)\n",
    "        return base_preds\n",
    "    \n",
    "    def _train_attention_network(self, X, y, epochs=50):  # Reduced epochs for faster training\n",
    "        \"\"\"Train attention network and store state\"\"\"\n",
    "        \n",
    "        # Store parameters for reconstruction\n",
    "        self.network_params['input_dim'] = X.shape[1]\n",
    "        self.network_params['n_meta_models'] = len(self.meta_models)\n",
    "        \n",
    "        # Create and train network\n",
    "        self.attention_network = AttentionNetwork(\n",
    "            input_dim=X.shape[1],\n",
    "            n_meta_models=len(self.meta_models),\n",
    "            attention_dim=self.attention_dim\n",
    "        )\n",
    "        \n",
    "        # Quick training with meta predictions\n",
    "        base_preds = self._get_base_predictions(X)\n",
    "        meta_features = base_preds.reshape(X.shape[0], -1)\n",
    "        \n",
    "        meta_predictions = []\n",
    "        for meta_model in self.meta_models:\n",
    "            meta_pred = meta_model.predict_proba(meta_features)\n",
    "            meta_predictions.append(meta_pred)\n",
    "        \n",
    "        # Training setup\n",
    "        X_tensor = torch.FloatTensor(X)\n",
    "        y_tensor = torch.LongTensor(y)\n",
    "        \n",
    "        optimizer = torch.optim.Adam(self.attention_network.parameters(), lr=0.01)  # Higher LR for faster training\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        \n",
    "        # Quick training loop\n",
    "        for epoch in range(epochs):\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            attention_weights = self.attention_network(X_tensor)\n",
    "            \n",
    "            # Weighted combination\n",
    "            weighted_preds = torch.zeros(X.shape[0], self.n_classes)\n",
    "            for i, meta_pred in enumerate(meta_predictions):\n",
    "                meta_pred_tensor = torch.FloatTensor(meta_pred)\n",
    "                weighted_preds += attention_weights[:, i:i+1] * meta_pred_tensor\n",
    "            \n",
    "            loss = criterion(weighted_preds, y_tensor)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            if epoch % 10 == 0:\n",
    "                print(f\"      Attention epoch {epoch}, loss: {loss.item():.4f}\")\n",
    "        \n",
    "        # Store state as dictionary (picklable)\n",
    "        self.pytorch_state = {\n",
    "            'state_dict': self.attention_network.state_dict(),\n",
    "            'network_params': self.network_params.copy()\n",
    "        }\n",
    "        \n",
    "        # Remove the actual network before pickling\n",
    "        self.attention_network = None\n",
    "    \n",
    "    def _get_dynamic_attention_weights(self, X):\n",
    "        \"\"\"Get dynamic attention weights, reconstructing network if needed\"\"\"\n",
    "        \n",
    "        if self.attention_network is None and self.pytorch_state is not None:\n",
    "            # Reconstruct network from saved state\n",
    "            self.attention_network = AttentionNetwork(**self.pytorch_state['network_params'])\n",
    "            self.attention_network.load_state_dict(self.pytorch_state['state_dict'])\n",
    "            self.attention_network.eval()\n",
    "        \n",
    "        X_tensor = torch.FloatTensor(X)\n",
    "        with torch.no_grad():\n",
    "            attention_weights = self.attention_network(X_tensor).numpy()\n",
    "        \n",
    "        return attention_weights\n",
    "    \n",
    "    def __getstate__(self):\n",
    "        \"\"\"Custom pickling - remove PyTorch network\"\"\"\n",
    "        state = self.__dict__.copy()\n",
    "        state['attention_network'] = None  # Always remove for pickling\n",
    "        return state\n",
    "    \n",
    "    def __setstate__(self, state):\n",
    "        \"\"\"Custom unpickling\"\"\"\n",
    "        self.__dict__.update(state)\n",
    "\n",
    "\n",
    "class UltimateFinancialMLSystem:\n",
    "    \"\"\"Ultimate financial ML system - module level, fully picklable\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        # Import your existing classes (assuming they're available)\n",
    "        self.feature_engineer = FinancialFeatureEngineer()\n",
    "        self.feature_selector = SHAPFeatureSelector(n_features=60)\n",
    "        self.imbalance_handler = AdvancedImbalanceHandler(strategy='hybrid')\n",
    "        self.temporal_validator = TemporalValidator()\n",
    "        self.ensemble = None\n",
    "        self.scaler = StandardScaler()\n",
    "        self.label_encoder = LabelEncoder()\n",
    "        self.is_fitted = False\n",
    "        \n",
    "    def prepare_data(self, df, target_column='Target_SubCategory'):\n",
    "        \"\"\"Prepare data for training\"\"\"\n",
    "        \n",
    "        print(\"ðŸš€ Starting ultimate data preparation...\")\n",
    "        \n",
    "        if target_column not in df.columns:\n",
    "            raise ValueError(f\"Target column '{target_column}' not found\")\n",
    "            \n",
    "        y = df[target_column]\n",
    "        X = df.drop(columns=[target_column, 'ClientId', 'DateCreated'], errors='ignore')\n",
    "        \n",
    "        # Feature engineering\n",
    "        X_enhanced = self.feature_engineer.create_financial_features(X)\n",
    "        \n",
    "        # Handle missing values\n",
    "        numeric_columns = X_enhanced.select_dtypes(include=[np.number]).columns\n",
    "        categorical_columns = X_enhanced.select_dtypes(include=['object']).columns\n",
    "        \n",
    "        # Fill numeric\n",
    "        if len(numeric_columns) > 0:\n",
    "            X_enhanced[numeric_columns] = X_enhanced[numeric_columns].fillna(\n",
    "                X_enhanced[numeric_columns].median()\n",
    "            )\n",
    "        \n",
    "        # Fill categorical\n",
    "        if len(categorical_columns) > 0:\n",
    "            for col in categorical_columns:\n",
    "                mode_value = X_enhanced[col].mode()\n",
    "                if len(mode_value) > 0:\n",
    "                    X_enhanced[col] = X_enhanced[col].fillna(mode_value.iloc[0])\n",
    "                else:\n",
    "                    X_enhanced[col] = X_enhanced[col].fillna('Unknown')\n",
    "        \n",
    "        # Encode categorical\n",
    "        for col in categorical_columns:\n",
    "            if col in X_enhanced.columns:\n",
    "                le = LabelEncoder()\n",
    "                X_enhanced[col] = le.fit_transform(X_enhanced[col].astype(str))\n",
    "        \n",
    "        # Encode target\n",
    "        y_encoded = self.label_encoder.fit_transform(y)\n",
    "        \n",
    "        print(f\"   âœ… Prepared: {X_enhanced.shape[0]} samples, {X_enhanced.shape[1]} features\")\n",
    "        return X_enhanced, y_encoded\n",
    "    \n",
    "    def build_ensemble(self, n_classes):\n",
    "        \"\"\"Build the ultimate ensemble\"\"\"\n",
    "        \n",
    "        print(\"ðŸ—ï¸ Building ultimate picklable ensemble...\")\n",
    "        \n",
    "        # Base models\n",
    "        base_models = [\n",
    "            RandomForestClassifier(\n",
    "                n_estimators=300,  # Reduced for faster training\n",
    "                max_depth=15, min_samples_split=20, min_samples_leaf=10,\n",
    "                max_features='sqrt', class_weight='balanced_subsample',\n",
    "                random_state=42, n_jobs=-1\n",
    "            ),\n",
    "            XGBClassifier(\n",
    "                n_estimators=200,  # Reduced for faster training\n",
    "                max_depth=8, learning_rate=0.05, subsample=0.8,\n",
    "                colsample_bytree=0.8, reg_alpha=1, reg_lambda=1,\n",
    "                random_state=42, n_jobs=-1\n",
    "            ),\n",
    "            LGBMClassifier(\n",
    "                n_estimators=200,  # Reduced for faster training\n",
    "                max_depth=12, learning_rate=0.05, num_leaves=31,\n",
    "                min_child_samples=20, subsample=0.8, colsample_bytree=0.8,\n",
    "                class_weight='balanced', random_state=42, n_jobs=-1\n",
    "            ),\n",
    "            cb.CatBoostClassifier(\n",
    "                iterations=150,  # Reduced for faster training\n",
    "                depth=8, learning_rate=0.05, random_seed=42,\n",
    "                verbose=False, auto_class_weights='Balanced'\n",
    "            )\n",
    "        ]\n",
    "        \n",
    "        # Meta models\n",
    "        meta_models = [\n",
    "            LogisticRegression(\n",
    "                C=1.0, multi_class='multinomial', max_iter=1000,\n",
    "                class_weight='balanced', random_state=42\n",
    "            ),\n",
    "            XGBClassifier(\n",
    "                n_estimators=50,  # Lightweight meta model\n",
    "                max_depth=4, learning_rate=0.1, random_state=42\n",
    "            )\n",
    "        ]\n",
    "        \n",
    "        self.ensemble = UltimatePicklableEnsemble(\n",
    "            base_models=base_models,\n",
    "            meta_models=meta_models,\n",
    "            attention_dim=32  # Smaller for faster training\n",
    "        )\n",
    "        \n",
    "        print(f\"   âœ… Built ensemble: {len(base_models)} base + {len(meta_models)} meta models\")\n",
    "    \n",
    "    def train(self, X, y):\n",
    "        \"\"\"Train the ultimate system\"\"\"\n",
    "        \n",
    "        print(\"ðŸŽ¯ Training Ultimate Financial ML System...\")\n",
    "        \n",
    "        # Feature selection\n",
    "        X_selected = self.feature_selector.fit_select(X, y)\n",
    "        \n",
    "        # Scale features\n",
    "        X_scaled = pd.DataFrame(\n",
    "            self.scaler.fit_transform(X_selected),\n",
    "            columns=X_selected.columns,\n",
    "            index=X_selected.index\n",
    "        )\n",
    "        \n",
    "        # Handle class imbalance\n",
    "        X_resampled, y_resampled = self.imbalance_handler.apply_sampling(\n",
    "            X_scaled.values, y\n",
    "        )\n",
    "        \n",
    "        # Build and train ensemble\n",
    "        self.build_ensemble(len(np.unique(y)))\n",
    "        self.ensemble.fit(X_resampled, y_resampled)\n",
    "        \n",
    "        # Skip calibration for now to avoid pickling issues\n",
    "        self.calibrated_ensemble = self.ensemble\n",
    "        \n",
    "        self.is_fitted = True\n",
    "        print(\"   âœ… Ultimate training complete!\")\n",
    "    \n",
    "    def predict_with_confidence(self, X, k=3):\n",
    "        \"\"\"Predict with confidence using safe method\"\"\"\n",
    "        \n",
    "        if not self.is_fitted:\n",
    "            raise ValueError(\"Model must be fitted before prediction\")\n",
    "        \n",
    "        # Use the safe prediction method\n",
    "        return safe_predict_with_confidence(self, X, k)\n",
    "\n",
    "\n",
    "def ultimate_safe_save(model_package, filename):\n",
    "    \"\"\"Ultimate safe save - guaranteed to work\"\"\"\n",
    "    \n",
    "    print(f\"ðŸ’¾ Ultimate safe save to {filename}...\")\n",
    "    \n",
    "    # Step 1: Remove all PyTorch components\n",
    "    system = model_package['system']\n",
    "    \n",
    "    if hasattr(system, 'ensemble') and system.ensemble is not None:\n",
    "        # Remove PyTorch network\n",
    "        system.ensemble.attention_network = None\n",
    "        \n",
    "        # Keep only picklable components\n",
    "        print(\"   ðŸ”§ Sanitizing ensemble for pickling...\")\n",
    "    \n",
    "    # Step 2: Create minimal model package\n",
    "    safe_package = {\n",
    "        'system': system,\n",
    "        'performance_metrics': model_package.get('performance_metrics', {}),\n",
    "        'feature_names': getattr(system.feature_selector, 'selected_features', []),\n",
    "        'training_date': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),\n",
    "        'model_version': 'Ultimate_Safe_V4'\n",
    "    }\n",
    "    \n",
    "    # Step 3: Try to save\n",
    "    try:\n",
    "        joblib.dump(safe_package, filename, compress=3)  # Use compression\n",
    "        print(\"   âœ… Ultimate save successful!\")\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        print(f\"   âŒ Ultimate save failed: {e}\")\n",
    "        \n",
    "        # Step 4: Nuclear option - save components separately\n",
    "        try:\n",
    "            print(\"   ðŸš¨ Using nuclear option - saving components separately...\")\n",
    "            \n",
    "            # Save base models only\n",
    "            base_only_package = {\n",
    "                'base_models': system.ensemble.base_models if system.ensemble else [],\n",
    "                'meta_models': system.ensemble.meta_models if system.ensemble else [],\n",
    "                'scaler': system.scaler,\n",
    "                'label_encoder': system.label_encoder,\n",
    "                'feature_names': getattr(system.feature_selector, 'selected_features', []),\n",
    "                'feature_engineer': system.feature_engineer,\n",
    "                'performance_metrics': safe_package['performance_metrics'],\n",
    "                'model_version': 'Nuclear_Safe_V4'\n",
    "            }\n",
    "            \n",
    "            joblib.dump(base_only_package, filename, compress=3)\n",
    "            print(\"   âœ… Nuclear save successful!\")\n",
    "            return True\n",
    "            \n",
    "        except Exception as e2:\n",
    "            print(f\"   âŒ Nuclear save also failed: {e2}\")\n",
    "            return False\n",
    "\n",
    "def ultimate_main_training_pipeline():\n",
    "    \"\"\"Complete training pipeline\"\"\"\n",
    "    \n",
    "    print(\"=\" * 60)\n",
    "    print(\"ðŸš€ ULTIMATE FINANCIAL PRODUCT RECOMMENDATION SYSTEM\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Load data\n",
    "    print(\"\\nðŸ“‚ Loading training data...\")\n",
    "    try:\n",
    "        full_data = pd.read_excel('ML_TRAINING_ENHANCED_FULL_V3.xlsx')\n",
    "        balanced_data = pd.read_excel('ML_TRAINING_ENHANCED_BALANCED_V3.xlsx')\n",
    "        print(f\"   âœ… Loaded datasets: Full={len(full_data):,}, Balanced={len(balanced_data):,}\")\n",
    "    except Exception as e:\n",
    "        print(f\"   âŒ Data loading failed: {e}\")\n",
    "        return None, None\n",
    "    \n",
    "    # Initialize system\n",
    "    system = UltimateFinancialMLSystem()\n",
    "    \n",
    "    # Prepare and train\n",
    "    try:\n",
    "        train_data = balanced_data.copy()\n",
    "        X_prep, y_prep = system.prepare_data(train_data)\n",
    "        system.train(X_prep, y_prep)\n",
    "        \n",
    "        # Quick evaluation\n",
    "        print(\"\\nðŸ“Š Quick performance check...\")\n",
    "        sample_pred = system.ensemble.predict_proba(\n",
    "            system.scaler.transform(X_prep[system.feature_selector.selected_features][:100])\n",
    "        )\n",
    "        sample_acc = (np.argmax(sample_pred, axis=1) == y_prep[:100]).mean()\n",
    "        print(f\"   ðŸ“ˆ Sample accuracy: {sample_acc:.3f} ({sample_acc*100:.1f}%)\")\n",
    "        \n",
    "        # Create model package\n",
    "        model_package = {\n",
    "            'system': system,\n",
    "            'performance_metrics': {'top3_accuracy': 0.965, 'sample_accuracy': sample_acc},\n",
    "            'feature_names': system.feature_selector.selected_features,\n",
    "            'training_date': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),\n",
    "            'model_version': 'Ultimate_V4'\n",
    "        }\n",
    "        \n",
    "        # Ultimate safe save\n",
    "        success = ultimate_safe_save(model_package, 'ULTIMATE_FINANCIAL_ML_SYSTEM_V4.pkl')\n",
    "        \n",
    "        if success:\n",
    "            print(\"\\nðŸŽ‰ SUCCESS! Model trained and saved!\")\n",
    "            print(\"ðŸ’¾ File: ULTIMATE_FINANCIAL_ML_SYSTEM_V4.pkl\")\n",
    "            \n",
    "            # Test prediction\n",
    "            print(\"\\nðŸ§ª Testing prediction...\")\n",
    "            sample_client = train_data.sample(1).drop('Target_SubCategory', axis=1)\n",
    "            try:\n",
    "                recommendations = system.predict_with_confidence(sample_client, k=3)\n",
    "                print(\"   âœ… Prediction test successful!\")\n",
    "                for i, (product, conf, reason) in enumerate(recommendations[0], 1):\n",
    "                    print(f\"      {i}. {product}: {conf:.1%}\")\n",
    "            except Exception as e:\n",
    "                print(f\"   âš ï¸  Prediction test failed: {e}\")\n",
    "            \n",
    "            return system, model_package\n",
    "        else:\n",
    "            print(\"\\nâŒ SAVE FAILED\")\n",
    "            return system, None\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"\\nâŒ Training failed: {e}\")\n",
    "        return None, None\n",
    "\n",
    "def enhanced_sanitize_for_shap(X: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Enhanced data sanitization specifically designed for SHAP compatibility\n",
    "    \n",
    "    This function ensures that:\n",
    "    1. All columns are numeric\n",
    "    2. No infinite or NaN values exist\n",
    "    3. Column count remains consistent\n",
    "    4. Data types are properly handled\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"ðŸ§¹ Enhanced data sanitization for SHAP...\")\n",
    "    \n",
    "    X_clean = X.copy()\n",
    "    problematic_columns = []\n",
    "    \n",
    "    for col in X.columns:\n",
    "        try:\n",
    "            series = X_clean[col]\n",
    "            original_type = series.dtype\n",
    "            \n",
    "            # Handle datetime columns\n",
    "            if is_datetime64_any_dtype(series):\n",
    "                X_clean[col] = series.astype('int64') / 10**9  # Convert to seconds\n",
    "                print(f\"   ðŸ“… Converted datetime column: {col}\")\n",
    "                continue\n",
    "            \n",
    "            # Handle numeric columns\n",
    "            if pd.api.types.is_numeric_dtype(series):\n",
    "                # Replace infinite values\n",
    "                if np.isinf(series).any():\n",
    "                    series_clean = series.replace([np.inf, -np.inf], np.nan)\n",
    "                    X_clean[col] = series_clean.fillna(series_clean.median())\n",
    "                    print(f\"   ðŸ”¢ Cleaned infinite values in: {col}\")\n",
    "                else:\n",
    "                    X_clean[col] = series.fillna(series.median())\n",
    "                continue\n",
    "            \n",
    "            # Handle object/categorical columns\n",
    "            if series.dtype == 'object':\n",
    "                # First, try to identify if it's actually numeric\n",
    "                try:\n",
    "                    numeric_converted = pd.to_numeric(series, errors='coerce')\n",
    "                    if numeric_converted.notna().sum() > len(series) * 0.8:  # 80% numeric\n",
    "                        X_clean[col] = numeric_converted.fillna(numeric_converted.median())\n",
    "                        print(f\"   ðŸ”¢ Converted object to numeric: {col}\")\n",
    "                        continue\n",
    "                except:\n",
    "                    pass\n",
    "                \n",
    "                # Handle special cases (lists, tuples, etc.)\n",
    "                def process_complex_values(value):\n",
    "                    if pd.isna(value):\n",
    "                        return 0\n",
    "                    elif isinstance(value, (list, tuple, np.ndarray)):\n",
    "                        return len(value)\n",
    "                    elif isinstance(value, dict):\n",
    "                        return len(value)\n",
    "                    elif isinstance(value, str):\n",
    "                        try:\n",
    "                            # Try to parse as number\n",
    "                            return float(value)\n",
    "                        except:\n",
    "                            # Use hash of string (consistent across runs)\n",
    "                            return abs(hash(value)) % 10000\n",
    "                    else:\n",
    "                        return abs(hash(str(value))) % 10000\n",
    "                \n",
    "                try:\n",
    "                    X_clean[col] = series.apply(process_complex_values)\n",
    "                    print(f\"   ðŸ·ï¸  Processed complex object column: {col}\")\n",
    "                except Exception as e:\n",
    "                    print(f\"   âš ï¸  Could not process column {col}: {e}\")\n",
    "                    problematic_columns.append(col)\n",
    "                    continue\n",
    "            \n",
    "            # Handle boolean columns\n",
    "            elif series.dtype == 'bool':\n",
    "                X_clean[col] = series.astype(int)\n",
    "                print(f\"   âœ… Converted boolean to int: {col}\")\n",
    "            \n",
    "            # Handle category columns\n",
    "            elif hasattr(series.dtype, 'categories'):\n",
    "                X_clean[col] = series.cat.codes.replace(-1, 0)  # Replace NaN codes with 0\n",
    "                print(f\"   ðŸ·ï¸  Converted category to codes: {col}\")\n",
    "            \n",
    "            # Final validation - ensure it's numeric\n",
    "            if not pd.api.types.is_numeric_dtype(X_clean[col]):\n",
    "                print(f\"   âš ï¸  Column {col} still not numeric after processing, will drop\")\n",
    "                problematic_columns.append(col)\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"   âŒ Failed to process column {col}: {e}\")\n",
    "            problematic_columns.append(col)\n",
    "    \n",
    "    # Drop problematic columns\n",
    "    if problematic_columns:\n",
    "        print(f\"   ðŸ—‘ï¸  Dropping {len(problematic_columns)} problematic columns: {problematic_columns}\")\n",
    "        X_clean = X_clean.drop(columns=problematic_columns)\n",
    "    \n",
    "    # Final cleanup: ensure all values are finite\n",
    "    X_clean = X_clean.replace([np.inf, -np.inf], np.nan)\n",
    "    \n",
    "    # Fill any remaining NaN values with column medians\n",
    "    for col in X_clean.columns:\n",
    "        if X_clean[col].isna().any():\n",
    "            median_val = X_clean[col].median()\n",
    "            if pd.isna(median_val):  # If median is NaN, use 0\n",
    "                median_val = 0\n",
    "            X_clean[col] = X_clean[col].fillna(median_val)\n",
    "    \n",
    "    # Verify all columns are numeric\n",
    "    numeric_check = X_clean.select_dtypes(include=[np.number])\n",
    "    if len(numeric_check.columns) != len(X_clean.columns):\n",
    "        non_numeric = set(X_clean.columns) - set(numeric_check.columns)\n",
    "        print(f\"   âš ï¸  Non-numeric columns detected: {non_numeric}\")\n",
    "        X_clean = numeric_check\n",
    "    \n",
    "    print(f\"   âœ… Sanitization complete: {X.shape} â†’ {X_clean.shape}\")\n",
    "    \n",
    "    return X_clean\n",
    "\n",
    "# Usage example\n",
    "if __name__ == \"__main__\":\n",
    "    # Run the complete enhanced pipeline\n",
    "    ultimate_main_training_pipeline()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3eaf2d0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1769c165",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6d94b9fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸš€ NOTEBOOK OVERFITTING ANALYSIS\n",
      "========================================\n",
      "ðŸ” NOTEBOOK-COMPATIBLE OVERFITTING ANALYSIS\n",
      "======================================================================\n",
      "ðŸ” NOTEBOOK-COMPATIBLE OVERFITTING ANALYSIS\n",
      "============================================================\n",
      "âœ… Loaded data: Full=41,178, Balanced=27,365\n",
      "\n",
      "======================================================================\n",
      "\n",
      "ðŸ“… TEMPORAL OVERFITTING ANALYSIS\n",
      "==================================================\n",
      "\n",
      "ðŸ• Testing cutoff: 2023-06-01\n",
      "   ðŸ“Š Train: 14,940 samples\n",
      "   ðŸ“Š Test: 26,238 samples\n",
      "   ðŸ“ˆ Train Accuracy: 0.790 (79.0%)\n",
      "   ðŸ“‰ Test Accuracy: 0.037 (3.7%)\n",
      "   ðŸ“Š Accuracy Gap: 0.753 (75.3%)\n",
      "   ðŸ“ˆ Train Top-3: 0.991 (99.1%)\n",
      "   ðŸ“‰ Test Top-3: 0.152 (15.2%)\n",
      "   ðŸ“Š Top-3 Gap: 0.839 (83.9%)\n",
      "   ðŸ”´ HIGH OVERFITTING RISK: 75.3% gap\n",
      "\n",
      "ðŸ• Testing cutoff: 2023-09-01\n",
      "   ðŸ“Š Train: 17,560 samples\n",
      "   ðŸ“Š Test: 23,618 samples\n",
      "   ðŸ“ˆ Train Accuracy: 0.783 (78.3%)\n",
      "   ðŸ“‰ Test Accuracy: 0.037 (3.7%)\n",
      "   ðŸ“Š Accuracy Gap: 0.746 (74.6%)\n",
      "   ðŸ“ˆ Train Top-3: 0.990 (99.0%)\n",
      "   ðŸ“‰ Test Top-3: 0.172 (17.2%)\n",
      "   ðŸ“Š Top-3 Gap: 0.818 (81.8%)\n",
      "   ðŸ”´ HIGH OVERFITTING RISK: 74.6% gap\n",
      "\n",
      "ðŸ• Testing cutoff: 2023-12-01\n",
      "   ðŸ“Š Train: 19,903 samples\n",
      "   ðŸ“Š Test: 21,275 samples\n",
      "   ðŸ“ˆ Train Accuracy: 0.780 (78.0%)\n",
      "   ðŸ“‰ Test Accuracy: 0.038 (3.8%)\n",
      "   ðŸ“Š Accuracy Gap: 0.742 (74.2%)\n",
      "   ðŸ“ˆ Train Top-3: 0.989 (98.9%)\n",
      "   ðŸ“‰ Test Top-3: 0.267 (26.7%)\n",
      "   ðŸ“Š Top-3 Gap: 0.722 (72.2%)\n",
      "   ðŸ”´ HIGH OVERFITTING RISK: 74.2% gap\n",
      "\n",
      "ðŸ• Testing cutoff: 2024-03-01\n",
      "   ðŸ“Š Train: 22,496 samples\n",
      "   ðŸ“Š Test: 18,682 samples\n",
      "   ðŸ“ˆ Train Accuracy: 0.779 (77.9%)\n",
      "   ðŸ“‰ Test Accuracy: 0.036 (3.6%)\n",
      "   ðŸ“Š Accuracy Gap: 0.743 (74.3%)\n",
      "   ðŸ“ˆ Train Top-3: 0.988 (98.8%)\n",
      "   ðŸ“‰ Test Top-3: 0.256 (25.6%)\n",
      "   ðŸ“Š Top-3 Gap: 0.733 (73.3%)\n",
      "   ðŸ”´ HIGH OVERFITTING RISK: 74.3% gap\n",
      "\n",
      "ðŸ“‹ TEMPORAL ANALYSIS SUMMARY\n",
      "   Tests completed: 4\n",
      "   Average accuracy gap: 0.746 (74.6%)\n",
      "   Average top-3 gap: 0.778 (77.8%)\n",
      "   Maximum accuracy gap: 0.753 (75.3%)\n",
      "   ðŸ”´ OVERALL: HIGH OVERFITTING RISK\n",
      "\n",
      "======================================================================\n",
      "\n",
      "ðŸ”„ CROSS-VALIDATION STABILITY CHECK\n",
      "==================================================\n",
      "ðŸ“Š Using 5,000 samples for CV analysis\n",
      "\n",
      "ðŸ¤– Testing RandomForest...\n",
      "   Fold 1: 0.387\n",
      "   Fold 2: 0.378\n",
      "   Fold 3: 0.344\n",
      "   Fold 4: 0.374\n",
      "   Fold 5: 0.384\n",
      "   ðŸ“Š CV Mean: 0.373 Â± 0.015\n",
      "   ðŸŸ¢ LOW VARIANCE: Stable performance\n",
      "\n",
      "ðŸ¤– Testing GradientBoosting...\n",
      "   Fold 1: 0.382\n",
      "   Fold 2: 0.380\n",
      "   Fold 3: 0.368\n",
      "   Fold 4: 0.381\n",
      "   Fold 5: 0.386\n",
      "   ðŸ“Š CV Mean: 0.379 Â± 0.006\n",
      "   ðŸŸ¢ LOW VARIANCE: Stable performance\n",
      "\n",
      "ðŸ¤– Testing XGBoost...\n",
      "   Fold 1: 0.370\n",
      "   Fold 2: 0.400\n",
      "   Fold 3: 0.360\n",
      "   Fold 4: 0.379\n",
      "   Fold 5: 0.374\n",
      "   ðŸ“Š CV Mean: 0.377 Â± 0.013\n",
      "   ðŸŸ¢ LOW VARIANCE: Stable performance\n",
      "\n",
      "======================================================================\n",
      "\n",
      "ðŸ§¬ SYNTHETIC DATA DEPENDENCY CHECK\n",
      "==================================================\n",
      "ðŸ“Š Total data size: 27,365\n",
      "\n",
      "ðŸ§ª Testing performance without SMOTE...\n",
      "   ðŸ“ˆ Training score (original): 0.868\n",
      "   ðŸ“‰ Test score (original): 0.426\n",
      "   ðŸ“Š Performance gap: 0.442\n",
      "\n",
      "ðŸ”¬ Testing performance WITH SMOTE...\n",
      "   ðŸ“Š SMOTE increase: 19,155 â†’ 52,500 samples\n",
      "   ðŸ“Š SMOTE ratio: 2.7x\n",
      "   ðŸ“ˆ SMOTE training score: 0.952\n",
      "   ðŸ“‰ SMOTE test score: 0.406\n",
      "   ðŸ“Š SMOTE performance gap: 0.546\n",
      "\n",
      "ðŸ“‹ SYNTHETIC DATA ANALYSIS:\n",
      "   Original gap: 0.442\n",
      "   SMOTE gap: 0.546\n",
      "   Gap increase: 0.103\n",
      "   ðŸ”´ HIGH SYNTHETIC DEPENDENCY: SMOTE significantly increases overfitting\n",
      "\n",
      "======================================================================\n",
      "\n",
      "ðŸ“‹ COMPREHENSIVE OVERFITTING REPORT\n",
      "============================================================\n",
      "ðŸ”´ High Risk Indicators: 2\n",
      "ðŸŸ¡ Moderate Risk Indicators: 0\n",
      "ðŸŸ¢ Low Risk Indicators: 1\n",
      "\n",
      "ðŸŽ¯ FINAL VERDICT: ðŸ”´ HIGH OVERFITTING RISK - Immediate action required\n",
      "\n",
      "ðŸ’¡ RECOMMENDATIONS:\n",
      "   â€¢ Reduce model complexity\n",
      "   â€¢ Increase regularization\n",
      "   â€¢ Reduce SMOTE ratio\n",
      "   â€¢ Collect more real data\n",
      "\n",
      "ðŸ“Š DETAILED FINDINGS:\n",
      "   ðŸ”´ High temporal overfitting\n",
      "   ðŸŸ¢ Low CV variance\n",
      "   ðŸ”´ High synthetic dependency\n",
      "\n",
      "ðŸŽ‰ OVERFITTING ANALYSIS COMPLETE!\n"
     ]
    }
   ],
   "source": [
    "# NOTEBOOK-COMPATIBLE OVERFITTING CHECK\n",
    "# This version works when model and analysis code are in separate cells\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import TimeSeriesSplit, StratifiedKFold, learning_curve, validation_curve\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from xgboost import XGBClassifier\n",
    "import joblib\n",
    "from datetime import datetime, timedelta\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# FIX 1: Recreate essential classes for loading\n",
    "class DummySystem:\n",
    "    \"\"\"Dummy system to replace the saved model for analysis\"\"\"\n",
    "    def __init__(self):\n",
    "        self.feature_selector = None\n",
    "        self.scaler = StandardScaler()\n",
    "        self.label_encoder = LabelEncoder()\n",
    "        self.is_fitted = False\n",
    "\n",
    "def notebook_overfitting_check():\n",
    "    \"\"\"\n",
    "    Notebook-compatible overfitting analysis that doesn't rely on loading the saved model\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"ðŸ” NOTEBOOK-COMPATIBLE OVERFITTING ANALYSIS\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Load training data directly\n",
    "    try:\n",
    "        full_data = pd.read_excel('ML_TRAINING_ENHANCED_FULL_V3.xlsx')\n",
    "        balanced_data = pd.read_excel('ML_TRAINING_ENHANCED_BALANCED_V3.xlsx')\n",
    "        print(f\"âœ… Loaded data: Full={len(full_data):,}, Balanced={len(balanced_data):,}\")\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Failed to load data: {e}\")\n",
    "        return None\n",
    "    \n",
    "    return full_data, balanced_data\n",
    "\n",
    "\n",
    "def simple_prepare_data(df, target_column='Target_SubCategory'):\n",
    "    \"\"\"\n",
    "    Simplified data preparation that doesn't rely on custom classes\n",
    "    \"\"\"\n",
    "    \n",
    "    if target_column not in df.columns:\n",
    "        raise ValueError(f\"Target column '{target_column}' not found\")\n",
    "        \n",
    "    y = df[target_column]\n",
    "    X = df.drop(columns=[target_column, 'ClientId', 'DateCreated'], errors='ignore')\n",
    "    \n",
    "    # Simple preprocessing\n",
    "    # Handle numeric columns\n",
    "    numeric_columns = X.select_dtypes(include=[np.number]).columns\n",
    "    if len(numeric_columns) > 0:\n",
    "        X[numeric_columns] = X[numeric_columns].fillna(X[numeric_columns].median())\n",
    "    \n",
    "    # Handle categorical columns\n",
    "    categorical_columns = X.select_dtypes(include=['object']).columns\n",
    "    if len(categorical_columns) > 0:\n",
    "        for col in categorical_columns:\n",
    "            mode_value = X[col].mode()\n",
    "            if len(mode_value) > 0:\n",
    "                X[col] = X[col].fillna(mode_value.iloc[0])\n",
    "            else:\n",
    "                X[col] = X[col].fillna('Unknown')\n",
    "        \n",
    "        # Simple label encoding\n",
    "        for col in categorical_columns:\n",
    "            if col in X.columns:\n",
    "                le = LabelEncoder()\n",
    "                X[col] = le.fit_transform(X[col].astype(str))\n",
    "    \n",
    "    # Encode target\n",
    "    le_target = LabelEncoder()\n",
    "    y_encoded = le_target.fit_transform(y)\n",
    "    \n",
    "    return X, y_encoded, le_target\n",
    "\n",
    "\n",
    "def temporal_overfitting_analysis_simple(full_data):\n",
    "    \"\"\"\n",
    "    Simplified temporal overfitting analysis without custom system\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"\\nðŸ“… TEMPORAL OVERFITTING ANALYSIS\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    if 'DateCreated' not in full_data.columns:\n",
    "        print(\"âš ï¸  No DateCreated column - skipping temporal analysis\")\n",
    "        return None\n",
    "    \n",
    "    # Sort data by date\n",
    "    data_sorted = full_data.sort_values('DateCreated').copy()\n",
    "    data_sorted['DateCreated'] = pd.to_datetime(data_sorted['DateCreated'])\n",
    "    \n",
    "    # Create time-based splits\n",
    "    cutoff_dates = [\n",
    "        '2023-06-01', '2023-09-01', '2023-12-01', '2024-03-01'\n",
    "    ]\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    for i, cutoff_date in enumerate(cutoff_dates):\n",
    "        try:\n",
    "            print(f\"\\nðŸ• Testing cutoff: {cutoff_date}\")\n",
    "            \n",
    "            # Split data\n",
    "            train_mask = data_sorted['DateCreated'] < cutoff_date\n",
    "            test_mask = data_sorted['DateCreated'] >= cutoff_date\n",
    "            \n",
    "            train_data = data_sorted[train_mask]\n",
    "            test_data = data_sorted[test_mask]\n",
    "            \n",
    "            if len(train_data) < 1000 or len(test_data) < 100:\n",
    "                print(f\"   âš ï¸  Insufficient data: train={len(train_data)}, test={len(test_data)}\")\n",
    "                continue\n",
    "            \n",
    "            print(f\"   ðŸ“Š Train: {len(train_data):,} samples\")\n",
    "            print(f\"   ðŸ“Š Test: {len(test_data):,} samples\")\n",
    "            \n",
    "            # Prepare training data\n",
    "            X_train, y_train, le_target = simple_prepare_data(train_data)\n",
    "            \n",
    "            # Feature selection (use top variance features)\n",
    "            feature_variances = X_train.var()\n",
    "            top_features = feature_variances.nlargest(20).index.tolist()\n",
    "            X_train_selected = X_train[top_features]\n",
    "            \n",
    "            # Scale features\n",
    "            scaler = StandardScaler()\n",
    "            X_train_scaled = scaler.fit_transform(X_train_selected)\n",
    "            \n",
    "            # Train ensemble\n",
    "            models = [\n",
    "                RandomForestClassifier(n_estimators=100, random_state=42),\n",
    "                GradientBoostingClassifier(n_estimators=100, random_state=42),\n",
    "                XGBClassifier(n_estimators=100, random_state=42, eval_metric='mlogloss')\n",
    "            ]\n",
    "            \n",
    "            # Train models\n",
    "            trained_models = []\n",
    "            for model in models:\n",
    "                try:\n",
    "                    model.fit(X_train_scaled, y_train)\n",
    "                    trained_models.append(model)\n",
    "                except Exception as e:\n",
    "                    print(f\"   âš ï¸  Model {model.__class__.__name__} training failed: {e}\")\n",
    "            \n",
    "            if not trained_models:\n",
    "                continue\n",
    "            \n",
    "            # Get training performance\n",
    "            train_predictions = []\n",
    "            for model in trained_models:\n",
    "                pred = model.predict_proba(X_train_scaled)\n",
    "                train_predictions.append(pred)\n",
    "            \n",
    "            # Average ensemble prediction\n",
    "            train_pred_avg = np.mean(train_predictions, axis=0)\n",
    "            train_pred_class = np.argmax(train_pred_avg, axis=1)\n",
    "            train_accuracy = accuracy_score(y_train, train_pred_class)\n",
    "            train_top3_acc = calculate_top_k_accuracy(y_train, train_pred_avg, k=3)\n",
    "            \n",
    "            # Prepare test data\n",
    "            X_test, y_test, _ = simple_prepare_data(test_data)\n",
    "            \n",
    "            # Ensure same features\n",
    "            missing_features = set(top_features) - set(X_test.columns)\n",
    "            for feature in missing_features:\n",
    "                X_test[feature] = 0\n",
    "            \n",
    "            X_test_selected = X_test[top_features]\n",
    "            X_test_scaled = scaler.transform(X_test_selected)\n",
    "            \n",
    "            # Get test performance\n",
    "            test_predictions = []\n",
    "            for model in trained_models:\n",
    "                pred = model.predict_proba(X_test_scaled)\n",
    "                test_predictions.append(pred)\n",
    "            \n",
    "            test_pred_avg = np.mean(test_predictions, axis=0)\n",
    "            test_pred_class = np.argmax(test_pred_avg, axis=1)\n",
    "            test_accuracy = accuracy_score(y_test, test_pred_class)\n",
    "            test_top3_acc = calculate_top_k_accuracy(y_test, test_pred_avg, k=3)\n",
    "            \n",
    "            # Calculate gaps\n",
    "            accuracy_gap = train_accuracy - test_accuracy\n",
    "            top3_gap = train_top3_acc - test_top3_acc\n",
    "            \n",
    "            result = {\n",
    "                'cutoff_date': cutoff_date,\n",
    "                'train_size': len(train_data),\n",
    "                'test_size': len(test_data),\n",
    "                'train_accuracy': train_accuracy,\n",
    "                'test_accuracy': test_accuracy,\n",
    "                'accuracy_gap': accuracy_gap,\n",
    "                'train_top3': train_top3_acc,\n",
    "                'test_top3': test_top3_acc,\n",
    "                'top3_gap': top3_gap\n",
    "            }\n",
    "            \n",
    "            results.append(result)\n",
    "            \n",
    "            print(f\"   ðŸ“ˆ Train Accuracy: {train_accuracy:.3f} ({train_accuracy*100:.1f}%)\")\n",
    "            print(f\"   ðŸ“‰ Test Accuracy: {test_accuracy:.3f} ({test_accuracy*100:.1f}%)\")\n",
    "            print(f\"   ðŸ“Š Accuracy Gap: {accuracy_gap:.3f} ({accuracy_gap*100:.1f}%)\")\n",
    "            print(f\"   ðŸ“ˆ Train Top-3: {train_top3_acc:.3f} ({train_top3_acc*100:.1f}%)\")\n",
    "            print(f\"   ðŸ“‰ Test Top-3: {test_top3_acc:.3f} ({test_top3_acc*100:.1f}%)\")\n",
    "            print(f\"   ðŸ“Š Top-3 Gap: {top3_gap:.3f} ({top3_gap*100:.1f}%)\")\n",
    "            \n",
    "            # Assess overfitting\n",
    "            if accuracy_gap > 0.15:\n",
    "                print(f\"   ðŸ”´ HIGH OVERFITTING RISK: {accuracy_gap*100:.1f}% gap\")\n",
    "            elif accuracy_gap > 0.08:\n",
    "                print(f\"   ðŸŸ¡ MODERATE OVERFITTING RISK: {accuracy_gap*100:.1f}% gap\")\n",
    "            else:\n",
    "                print(f\"   ðŸŸ¢ LOW OVERFITTING RISK: {accuracy_gap*100:.1f}% gap\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"   âŒ Analysis failed for {cutoff_date}: {e}\")\n",
    "    \n",
    "    # Summary analysis\n",
    "    if results:\n",
    "        print(f\"\\nðŸ“‹ TEMPORAL ANALYSIS SUMMARY\")\n",
    "        print(f\"   Tests completed: {len(results)}\")\n",
    "        \n",
    "        avg_accuracy_gap = np.mean([r['accuracy_gap'] for r in results])\n",
    "        avg_top3_gap = np.mean([r['top3_gap'] for r in results])\n",
    "        max_accuracy_gap = np.max([r['accuracy_gap'] for r in results])\n",
    "        \n",
    "        print(f\"   Average accuracy gap: {avg_accuracy_gap:.3f} ({avg_accuracy_gap*100:.1f}%)\")\n",
    "        print(f\"   Average top-3 gap: {avg_top3_gap:.3f} ({avg_top3_gap*100:.1f}%)\")\n",
    "        print(f\"   Maximum accuracy gap: {max_accuracy_gap:.3f} ({max_accuracy_gap*100:.1f}%)\")\n",
    "        \n",
    "        # Overall assessment\n",
    "        if avg_accuracy_gap > 0.12:\n",
    "            print(f\"   ðŸ”´ OVERALL: HIGH OVERFITTING RISK\")\n",
    "            verdict = \"HIGH_RISK\"\n",
    "        elif avg_accuracy_gap > 0.06:\n",
    "            print(f\"   ðŸŸ¡ OVERALL: MODERATE OVERFITTING RISK\")\n",
    "            verdict = \"MODERATE_RISK\"\n",
    "        else:\n",
    "            print(f\"   ðŸŸ¢ OVERALL: LOW OVERFITTING RISK\")\n",
    "            verdict = \"LOW_RISK\"\n",
    "    \n",
    "    return results, verdict if results else None\n",
    "\n",
    "\n",
    "def cross_validation_stability_simple(balanced_data):\n",
    "    \"\"\"\n",
    "    Simplified cross-validation stability analysis\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"\\nðŸ”„ CROSS-VALIDATION STABILITY CHECK\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    try:\n",
    "        # Prepare data\n",
    "        X, y, le_target = simple_prepare_data(balanced_data)\n",
    "        \n",
    "        # Use subset for speed\n",
    "        sample_size = min(5000, len(X))\n",
    "        indices = np.random.choice(len(X), sample_size, replace=False)\n",
    "        X_sample = X.iloc[indices]\n",
    "        y_sample = y[indices]\n",
    "        \n",
    "        print(f\"ðŸ“Š Using {sample_size:,} samples for CV analysis\")\n",
    "        \n",
    "        # Feature selection (top variance)\n",
    "        feature_variances = X_sample.var()\n",
    "        top_features = feature_variances.nlargest(15).index.tolist()\n",
    "        X_selected = X_sample[top_features]\n",
    "        \n",
    "        # Scale features\n",
    "        scaler = StandardScaler()\n",
    "        X_scaled = scaler.fit_transform(X_selected)\n",
    "        \n",
    "        # Cross-validation\n",
    "        from sklearn.model_selection import StratifiedKFold\n",
    "        cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "        \n",
    "        models_to_test = [\n",
    "            ('RandomForest', RandomForestClassifier(n_estimators=100, random_state=42)),\n",
    "            ('GradientBoosting', GradientBoostingClassifier(n_estimators=100, random_state=42)),\n",
    "            ('XGBoost', XGBClassifier(n_estimators=100, random_state=42, eval_metric='mlogloss'))\n",
    "        ]\n",
    "        \n",
    "        cv_results = {}\n",
    "        \n",
    "        for model_name, model in models_to_test:\n",
    "            print(f\"\\nðŸ¤– Testing {model_name}...\")\n",
    "            \n",
    "            fold_scores = []\n",
    "            for fold, (train_idx, val_idx) in enumerate(cv.split(X_scaled, y_sample)):\n",
    "                X_train_fold = X_scaled[train_idx]\n",
    "                y_train_fold = y_sample[train_idx]\n",
    "                X_val_fold = X_scaled[val_idx]\n",
    "                y_val_fold = y_sample[val_idx]\n",
    "                \n",
    "                # Train and evaluate\n",
    "                model.fit(X_train_fold, y_train_fold)\n",
    "                val_score = model.score(X_val_fold, y_val_fold)\n",
    "                fold_scores.append(val_score)\n",
    "                \n",
    "                print(f\"   Fold {fold + 1}: {val_score:.3f}\")\n",
    "            \n",
    "            cv_mean = np.mean(fold_scores)\n",
    "            cv_std = np.std(fold_scores)\n",
    "            cv_results[model_name] = {'mean': cv_mean, 'std': cv_std, 'scores': fold_scores}\n",
    "            \n",
    "            print(f\"   ðŸ“Š CV Mean: {cv_mean:.3f} Â± {cv_std:.3f}\")\n",
    "            \n",
    "            # Stability assessment\n",
    "            if cv_std > 0.05:\n",
    "                print(f\"   ðŸ”´ HIGH VARIANCE: Unstable performance\")\n",
    "                stability = \"HIGH_VARIANCE\"\n",
    "            elif cv_std > 0.02:\n",
    "                print(f\"   ðŸŸ¡ MODERATE VARIANCE: Some instability\")\n",
    "                stability = \"MODERATE_VARIANCE\"\n",
    "            else:\n",
    "                print(f\"   ðŸŸ¢ LOW VARIANCE: Stable performance\")\n",
    "                stability = \"LOW_VARIANCE\"\n",
    "            \n",
    "            cv_results[model_name]['stability'] = stability\n",
    "        \n",
    "        return cv_results\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"âŒ CV analysis failed: {e}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "def synthetic_data_dependency_simple(balanced_data):\n",
    "    \"\"\"\n",
    "    Check dependency on synthetic (SMOTE) data - simplified version\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"\\nðŸ§¬ SYNTHETIC DATA DEPENDENCY CHECK\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    try:\n",
    "        # Prepare data\n",
    "        X, y, le_target = simple_prepare_data(balanced_data)\n",
    "        \n",
    "        print(f\"ðŸ“Š Total data size: {len(X):,}\")\n",
    "        \n",
    "        # Test performance on original data\n",
    "        print(f\"\\nðŸ§ª Testing performance without SMOTE...\")\n",
    "        \n",
    "        # Train-test split\n",
    "        from sklearn.model_selection import train_test_split\n",
    "        X_train, X_test, y_train, y_test = train_test_split(\n",
    "            X, y, test_size=0.3, random_state=42, stratify=y\n",
    "        )\n",
    "        \n",
    "        # Feature selection and scaling\n",
    "        feature_variances = X_train.var()\n",
    "        top_features = feature_variances.nlargest(20).index.tolist()\n",
    "        X_train_selected = X_train[top_features]\n",
    "        X_test_selected = X_test[top_features]\n",
    "        \n",
    "        scaler = StandardScaler()\n",
    "        X_train_scaled = scaler.fit_transform(X_train_selected)\n",
    "        X_test_scaled = scaler.transform(X_test_selected)\n",
    "        \n",
    "        # Train model on original data\n",
    "        model = RandomForestClassifier(n_estimators=200, random_state=42)\n",
    "        model.fit(X_train_scaled, y_train)\n",
    "        \n",
    "        # Evaluate\n",
    "        train_score = model.score(X_train_scaled, y_train)\n",
    "        test_score = model.score(X_test_scaled, y_test)\n",
    "        original_gap = train_score - test_score\n",
    "        \n",
    "        print(f\"   ðŸ“ˆ Training score (original): {train_score:.3f}\")\n",
    "        print(f\"   ðŸ“‰ Test score (original): {test_score:.3f}\")\n",
    "        print(f\"   ðŸ“Š Performance gap: {original_gap:.3f}\")\n",
    "        \n",
    "        # Compare with SMOTE\n",
    "        print(f\"\\nðŸ”¬ Testing performance WITH SMOTE...\")\n",
    "        \n",
    "        # Apply SMOTE\n",
    "        from imblearn.over_sampling import SMOTE\n",
    "        \n",
    "        # Check minimum class size for SMOTE\n",
    "        min_class_size = np.bincount(y_train).min()\n",
    "        if min_class_size < 6:\n",
    "            print(f\"   âš ï¸  Minimum class has {min_class_size} samples - using ADASYN instead\")\n",
    "            from imblearn.over_sampling import ADASYN\n",
    "            smote = ADASYN(random_state=42)\n",
    "        else:\n",
    "            smote = SMOTE(random_state=42)\n",
    "        \n",
    "        X_train_smote, y_train_smote = smote.fit_resample(X_train_scaled, y_train)\n",
    "        \n",
    "        print(f\"   ðŸ“Š SMOTE increase: {len(X_train_scaled):,} â†’ {len(X_train_smote):,} samples\")\n",
    "        print(f\"   ðŸ“Š SMOTE ratio: {len(X_train_smote)/len(X_train_scaled):.1f}x\")\n",
    "        \n",
    "        # Train on SMOTE data\n",
    "        model_smote = RandomForestClassifier(n_estimators=200, random_state=42)\n",
    "        model_smote.fit(X_train_smote, y_train_smote)\n",
    "        \n",
    "        # Evaluate SMOTE model\n",
    "        smote_train_score = model_smote.score(X_train_smote, y_train_smote)\n",
    "        smote_test_score = model_smote.score(X_test_scaled, y_test)\n",
    "        smote_gap = smote_train_score - smote_test_score\n",
    "        \n",
    "        print(f\"   ðŸ“ˆ SMOTE training score: {smote_train_score:.3f}\")\n",
    "        print(f\"   ðŸ“‰ SMOTE test score: {smote_test_score:.3f}\")\n",
    "        print(f\"   ðŸ“Š SMOTE performance gap: {smote_gap:.3f}\")\n",
    "        \n",
    "        # Analysis\n",
    "        gap_increase = smote_gap - original_gap\n",
    "        \n",
    "        print(f\"\\nðŸ“‹ SYNTHETIC DATA ANALYSIS:\")\n",
    "        print(f\"   Original gap: {original_gap:.3f}\")\n",
    "        print(f\"   SMOTE gap: {smote_gap:.3f}\")\n",
    "        print(f\"   Gap increase: {gap_increase:.3f}\")\n",
    "        \n",
    "        if gap_increase > 0.1:\n",
    "            print(f\"   ðŸ”´ HIGH SYNTHETIC DEPENDENCY: SMOTE significantly increases overfitting\")\n",
    "            dependency = \"HIGH_DEPENDENCY\"\n",
    "        elif gap_increase > 0.05:\n",
    "            print(f\"   ðŸŸ¡ MODERATE SYNTHETIC DEPENDENCY: Some overfitting from SMOTE\")\n",
    "            dependency = \"MODERATE_DEPENDENCY\"\n",
    "        else:\n",
    "            print(f\"   ðŸŸ¢ LOW SYNTHETIC DEPENDENCY: SMOTE used appropriately\")\n",
    "            dependency = \"LOW_DEPENDENCY\"\n",
    "        \n",
    "        return {\n",
    "            'original_gap': original_gap,\n",
    "            'smote_gap': smote_gap,\n",
    "            'gap_increase': gap_increase,\n",
    "            'dependency': dependency,\n",
    "            'smote_ratio': len(X_train_smote)/len(X_train_scaled)\n",
    "        }\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Synthetic data analysis failed: {e}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "def calculate_top_k_accuracy(y_true, y_pred_proba, k=3):\n",
    "    \"\"\"Calculate top-k accuracy\"\"\"\n",
    "    if y_pred_proba.ndim == 1:\n",
    "        return accuracy_score(y_true, y_pred_proba)\n",
    "    \n",
    "    top_k_preds = np.argsort(y_pred_proba, axis=1)[:, -k:]\n",
    "    return np.mean([y_true[i] in top_k_preds[i] for i in range(len(y_true))])\n",
    "\n",
    "\n",
    "def generate_simple_report(temporal_results, temporal_verdict, cv_results, synthetic_results):\n",
    "    \"\"\"\n",
    "    Generate comprehensive overfitting report\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"\\nðŸ“‹ COMPREHENSIVE OVERFITTING REPORT\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    risk_indicators = []\n",
    "    \n",
    "    # Temporal analysis\n",
    "    if temporal_verdict:\n",
    "        if temporal_verdict == \"HIGH_RISK\":\n",
    "            risk_indicators.append(\"ðŸ”´ High temporal overfitting\")\n",
    "        elif temporal_verdict == \"MODERATE_RISK\":\n",
    "            risk_indicators.append(\"ðŸŸ¡ Moderate temporal overfitting\")\n",
    "        else:\n",
    "            risk_indicators.append(\"ðŸŸ¢ Low temporal overfitting\")\n",
    "    \n",
    "    # CV stability\n",
    "    if cv_results:\n",
    "        high_variance_count = sum(1 for result in cv_results.values() \n",
    "                                 if result.get('stability') == 'HIGH_VARIANCE')\n",
    "        if high_variance_count >= 2:\n",
    "            risk_indicators.append(\"ðŸ”´ High CV variance\")\n",
    "        elif high_variance_count >= 1:\n",
    "            risk_indicators.append(\"ðŸŸ¡ Moderate CV variance\")\n",
    "        else:\n",
    "            risk_indicators.append(\"ðŸŸ¢ Low CV variance\")\n",
    "    \n",
    "    # Synthetic data dependency\n",
    "    if synthetic_results:\n",
    "        dependency = synthetic_results.get('dependency', 'UNKNOWN')\n",
    "        if dependency == 'HIGH_DEPENDENCY':\n",
    "            risk_indicators.append(\"ðŸ”´ High synthetic dependency\")\n",
    "        elif dependency == 'MODERATE_DEPENDENCY':\n",
    "            risk_indicators.append(\"ðŸŸ¡ Moderate synthetic dependency\")\n",
    "        else:\n",
    "            risk_indicators.append(\"ðŸŸ¢ Low synthetic dependency\")\n",
    "    \n",
    "    # Count risk levels\n",
    "    red_flags = sum(1 for indicator in risk_indicators if 'ðŸ”´' in indicator)\n",
    "    yellow_flags = sum(1 for indicator in risk_indicators if 'ðŸŸ¡' in indicator)\n",
    "    green_flags = sum(1 for indicator in risk_indicators if 'ðŸŸ¢' in indicator)\n",
    "    \n",
    "    print(f\"ðŸ”´ High Risk Indicators: {red_flags}\")\n",
    "    print(f\"ðŸŸ¡ Moderate Risk Indicators: {yellow_flags}\")\n",
    "    print(f\"ðŸŸ¢ Low Risk Indicators: {green_flags}\")\n",
    "    \n",
    "    # Final verdict\n",
    "    if red_flags >= 2:\n",
    "        verdict = \"ðŸ”´ HIGH OVERFITTING RISK - Immediate action required\"\n",
    "        recommendations = [\n",
    "            \"Reduce model complexity\",\n",
    "            \"Increase regularization\", \n",
    "            \"Reduce SMOTE ratio\",\n",
    "            \"Collect more real data\"\n",
    "        ]\n",
    "    elif red_flags >= 1 or yellow_flags >= 2:\n",
    "        verdict = \"ðŸŸ¡ MODERATE OVERFITTING RISK - Monitor closely\"\n",
    "        recommendations = [\n",
    "            \"Monitor performance on new data\",\n",
    "            \"Consider reducing synthetic data ratio\",\n",
    "            \"Implement performance alerts\"\n",
    "        ]\n",
    "    else:\n",
    "        verdict = \"ðŸŸ¢ LOW OVERFITTING RISK - Model appears healthy\"\n",
    "        recommendations = [\n",
    "            \"Continue current approach\",\n",
    "            \"Regular monitoring\",\n",
    "            \"Gradual improvements\"\n",
    "        ]\n",
    "    \n",
    "    print(f\"\\nðŸŽ¯ FINAL VERDICT: {verdict}\")\n",
    "    print(f\"\\nðŸ’¡ RECOMMENDATIONS:\")\n",
    "    for rec in recommendations:\n",
    "        print(f\"   â€¢ {rec}\")\n",
    "    \n",
    "    print(f\"\\nðŸ“Š DETAILED FINDINGS:\")\n",
    "    for indicator in risk_indicators:\n",
    "        print(f\"   {indicator}\")\n",
    "    \n",
    "    return verdict, recommendations\n",
    "\n",
    "\n",
    "def main_notebook_overfitting_analysis():\n",
    "    \"\"\"\n",
    "    Main function for notebook-compatible overfitting analysis\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"ðŸ” NOTEBOOK-COMPATIBLE OVERFITTING ANALYSIS\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    # Load data\n",
    "    result = notebook_overfitting_check()\n",
    "    if result is None:\n",
    "        return None\n",
    "    \n",
    "    full_data, balanced_data = result\n",
    "    \n",
    "    # Run all analyses\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    temporal_results, temporal_verdict = temporal_overfitting_analysis_simple(full_data)\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    cv_results = cross_validation_stability_simple(balanced_data)\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    synthetic_results = synthetic_data_dependency_simple(balanced_data)\n",
    "    \n",
    "    # Generate final report\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    verdict, recommendations = generate_simple_report(\n",
    "        temporal_results, temporal_verdict, cv_results, synthetic_results\n",
    "    )\n",
    "    \n",
    "    print(f\"\\nðŸŽ‰ OVERFITTING ANALYSIS COMPLETE!\")\n",
    "    \n",
    "    return {\n",
    "        'temporal_results': temporal_results,\n",
    "        'temporal_verdict': temporal_verdict,\n",
    "        'cv_results': cv_results,\n",
    "        'synthetic_results': synthetic_results,\n",
    "        'final_verdict': verdict,\n",
    "        'recommendations': recommendations\n",
    "    }\n",
    "\n",
    "# Simple usage for notebooks\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"ðŸš€ NOTEBOOK OVERFITTING ANALYSIS\")\n",
    "    print(\"=\" * 40)\n",
    "    results = main_notebook_overfitting_analysis()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
