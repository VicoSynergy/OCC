{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "240e3fc7-cac1-4fb8-8d59-f3b80d24abe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Layer 1 - Client Feature Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5cf08e90-f3f8-4fe9-a27e-820859a602fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== LOADING ALL DATASETS ===\n",
      "Loading client... ✓ (45,700 rows, 49 columns)\n",
      "Loading emfc2fna... ✓ (51,772 rows, 31 columns)\n",
      "Loading emfc2personalinformation... ✓ (52,534 rows, 37 columns)\n",
      "Loading emfc2... ✓ (51,769 rows, 8 columns)\n",
      "Loading EMFC2Assets... ✓ (50,502 rows, 39 columns)\n",
      "Loading emfc2productsolution... ✓ (61,733 rows, 25 columns)\n",
      "Loading emfc2productsolutiondetail... ✓ (154,286 rows, 5 columns)\n",
      "Loading EMFC2ProductIntegrationApplication... ✓ (560 rows, 14 columns)\n",
      "Loading EMFC2ProductIntegrationLog... ✓ (977 rows, 21 columns)\n",
      "Loading ProductMainPlan... ✓ (1,535 rows, 22 columns)\n",
      "Loading ProductType... ✓ (4 rows, 8 columns)\n",
      "Loading ProductCategory... ✓ (10 rows, 6 columns)\n",
      "Loading productsubcategory... ✓ (47 rows, 13 columns)\n",
      "\n",
      "Successfully loaded 13 datasets\n",
      "Available datasets: ['client', 'emfc2fna', 'emfc2personalinformation', 'emfc2', 'EMFC2Assets', 'emfc2productsolution', 'emfc2productsolutiondetail', 'EMFC2ProductIntegrationApplication', 'EMFC2ProductIntegrationLog', 'ProductMainPlan', 'ProductType', 'ProductCategory', 'productsubcategory']\n"
     ]
    }
   ],
   "source": [
    "# Data gathering\n",
    "import pandas as pd\n",
    "import msoffcrypto\n",
    "import io\n",
    "\n",
    "def load_encrypted_excel(file_path: str, password: str) -> pd.DataFrame:\n",
    "    with open(file_path, 'rb') as f:\n",
    "        office_file = msoffcrypto.OfficeFile(f)\n",
    "        office_file.load_key(password=password)\n",
    "        decrypted = io.BytesIO()\n",
    "        office_file.decrypt(decrypted)\n",
    "        decrypted.seek(0)\n",
    "        return pd.read_excel(decrypted)\n",
    "\n",
    "# File configurations\n",
    "files = [\n",
    "    # Core Client & FNA Process Tables\n",
    "    {\"name\": \"client\", \"path\": \"client.xlsx\", \"password\": \"_XlN@a9)EVy1\"},\n",
    "    {\"name\": \"emfc2fna\", \"path\": \"emfc2fna.xlsx\", \"password\": \"dQq9T%pC^?22\"},\n",
    "    {\"name\": \"emfc2personalinformation\", \"path\": \"emfc2personalinformation.xlsx\", \"password\": \"(ZuUpT69WVCR\"},\n",
    "    {\"name\": \"emfc2\", \"path\": \"emfc2.xlsx\", \"password\": \"79GYEd%l(2Bf\"},\n",
    "    {\"name\": \"EMFC2Assets\", \"path\": \"EMFC2Assets.xlsx\", \"password\": \"!suNZ=%YA13k\"},\n",
    "\n",
    "    # Product & Solution Workflow\n",
    "    {\"name\": \"emfc2productsolution\", \"path\": \"emfc2productsolution.xlsx\", \"password\": \"mcZ4KQ$&d*?u\"},\n",
    "    {\"name\": \"emfc2productsolutiondetail\", \"path\": \"emfc2productsolutiondetail.xlsx\", \"password\": \"Y$cbGx+75BUx\"},\n",
    "    {\"name\": \"EMFC2ProductIntegrationApplication\", \"path\": \"EMFC2ProductIntegrationApplication.xlsx\", \"password\": \"(FZsw7#vz-bN\"},\n",
    "    {\"name\": \"EMFC2ProductIntegrationLog\", \"path\": \"EMFC2ProductIntegrationLog.xlsx\", \"password\": \"?wcAx*P4n=9&\"},\n",
    "\n",
    "    # Product & Category Lookup Tables\n",
    "    {\"name\": \"ProductMainPlan\", \"path\": \"ProductMainPlan.xlsx\", \"password\": \")XQ4ZDssowrA\"},\n",
    "    {\"name\": \"ProductType\", \"path\": \"ProductType.xlsx\", \"password\": \"#9zCw?^-xTO?\"},\n",
    "    {\"name\": \"ProductCategory\", \"path\": \"ProductCategory.xlsx\", \"password\": \"#F)cdAEOVJ@4\"},\n",
    "    {\"name\": \"productsubcategory\", \"path\": \"productsubcategory.xlsx\", \"password\": \"y-^t$N9>%S%C\"}\n",
    "]\n",
    "\n",
    "# Load all datasets into memory\n",
    "datasets = {}\n",
    "\n",
    "print(\"=== LOADING ALL DATASETS ===\")\n",
    "for file in files:\n",
    "    print(f\"Loading {file['name']}...\", end=\" \")\n",
    "    try:\n",
    "        datasets[file['name']] = load_encrypted_excel(file[\"path\"], file[\"password\"])\n",
    "        shape = datasets[file['name']].shape\n",
    "        print(f\"✓ ({shape[0]:,} rows, {shape[1]} columns)\")\n",
    "    except Exception as e:\n",
    "        print(f\"✗ Error: {e}\")\n",
    "\n",
    "print(f\"\\nSuccessfully loaded {len(datasets)} datasets\")\n",
    "print(\"Available datasets:\", list(datasets.keys()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7b7afcee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== COLUMN HEADERS FOR EACH DATASET ===\n",
      "\n",
      "📄 Dataset: client\n",
      "🧾 Columns (49):\n",
      "  - #\n",
      "  - ClientId\n",
      "  - ClientName\n",
      "  - ClientMobileNumber\n",
      "  - ClientMNVerified\n",
      "  - ClientMNVeriCode\n",
      "  - ClientMNVeriCodeTime\n",
      "  - ClientEmail\n",
      "  - ClientContactPreferences\n",
      "  - ClientGender\n",
      "  - ClientDOB\n",
      "  - ClientCPFContributionCategoryId\n",
      "  - IDNumber\n",
      "  - Nationality\n",
      "  - SpokenLanguage\n",
      "  - WrittenLanguage\n",
      "  - Education\n",
      "  - EmploymentStatus\n",
      "  - Occupation\n",
      "  - MaritalStatus\n",
      "  - PrimaryAddress\n",
      "  - CorrespondingAddress\n",
      "  - IncomeRange\n",
      "  - AccompaniedbyTrustedIndividual\n",
      "  - ClientInvitedDate\n",
      "  - ClientStatus\n",
      "  - RiskProfile\n",
      "  - RiskProfileSubmissionDate\n",
      "  - CKAProfile\n",
      "  - CARProfile\n",
      "  - CKACARSubmissionDate\n",
      "  - UserId\n",
      "  - ClientEmailVerified\n",
      "  - ClientEmailVeriCode\n",
      "  - ClientEmailVeriCodeTime\n",
      "  - ClientResidentialStatus\n",
      "  - CountryOfBirth\n",
      "  - Race\n",
      "  - UEN\n",
      "  - ClientSource\n",
      "  - LastModifiedTime\n",
      "  - AccreditedInvestor\n",
      "  - ClientType\n",
      "  - ManuallyRetrieved\n",
      "  - MyInfoRetrieved\n",
      "  - NextReviewDate\n",
      "  - AITransactionCapability\n",
      "  - TempContactPreferences\n",
      "  - SelectedClient\n",
      "----------------------------------------\n",
      "📄 Dataset: emfc2fna\n",
      "🧾 Columns (31):\n",
      "  - #\n",
      "  - EMFC2FNAId\n",
      "  - EMFC2Id\n",
      "  - TradeType\n",
      "  - CoBroke\n",
      "  - CoBrokeUserId\n",
      "  - CoBrokeSFACode\n",
      "  - JointApplicant\n",
      "  - JointApplicantClientId\n",
      "  - StartTime\n",
      "  - SubmissionTime\n",
      "  - FNAStatus\n",
      "  - NonFaceToFace\n",
      "  - SignRemotely\n",
      "  - NFTFCommunicationMode\n",
      "  - NFTFCommunicationPlatform\n",
      "  - UserId\n",
      "  - RenewalFNA\n",
      "  - MaterialChanges\n",
      "  - FundSwitch\n",
      "  - NonResident\n",
      "  - RequirementNeeded\n",
      "  - RequirementCompleted\n",
      "  - FNAIdentifier\n",
      "  - RequirementResolvementTime\n",
      "  - HasIntroducer\n",
      "  - AITransaction\n",
      "  - SelfSubmission\n",
      "  - AdviserId\n",
      "  - ManagerAdviserId\n",
      "  - DirectorAdviserId\n",
      "----------------------------------------\n",
      "📄 Dataset: emfc2personalinformation\n",
      "🧾 Columns (37):\n",
      "  - #\n",
      "  - PersonalInformationId\n",
      "  - EMFC2FNAId\n",
      "  - ClientId\n",
      "  - ClientName\n",
      "  - ClientMobileNumber\n",
      "  - ClientEmail\n",
      "  - ClientContactPreferences\n",
      "  - ClientGender\n",
      "  - ClientDOB\n",
      "  - ClientAge\n",
      "  - ClientCPFContributionCategoryId\n",
      "  - IDNumber\n",
      "  - Nationality\n",
      "  - SpokenLanguage\n",
      "  - WrittenLanguage\n",
      "  - Education\n",
      "  - EmploymentStatus\n",
      "  - Occupation\n",
      "  - MaritalStatus\n",
      "  - PrimaryAddress\n",
      "  - CorrespondingAddress\n",
      "  - IncomeRange\n",
      "  - AccompaniedbyTrustedIndividual\n",
      "  - RiskProfile\n",
      "  - RiskProfileSubmissionDate\n",
      "  - CKAProfile\n",
      "  - CARProfile\n",
      "  - CKACARSubmissionDate\n",
      "  - ClientStatus\n",
      "  - ClientResidentialStatus\n",
      "  - CountryOfBirth\n",
      "  - Race\n",
      "  - ClientRetrieval\n",
      "  - ManuallyRetrieved\n",
      "  - MyInfoRetrieved\n",
      "  - SelectedClient\n",
      "----------------------------------------\n",
      "📄 Dataset: emfc2\n",
      "🧾 Columns (8):\n",
      "  - #\n",
      "  - EMFC2Id\n",
      "  - ClientId\n",
      "  - ClientAge\n",
      "  - EMFCStartDate\n",
      "  - EMFCSubmitDate\n",
      "  - EMFCStatus\n",
      "  - UserId\n",
      "----------------------------------------\n",
      "📄 Dataset: EMFC2Assets\n",
      "🧾 Columns (39):\n",
      "  - #\n",
      "  - EMFC2AssetsId\n",
      "  - EMFC2Id\n",
      "  - AssetsDisclose\n",
      "  - ReasonAssetsDisclose\n",
      "  - SavingsAccounts\n",
      "  - FixedDepositsAccount\n",
      "  - OtherCashAsset\n",
      "  - HomeAsset\n",
      "  - MotorAsset\n",
      "  - InsuranceCashValues\n",
      "  - OtherUseAsset\n",
      "  - StocksPortofolio\n",
      "  - BondPortofolio\n",
      "  - UTFEquityAsset\n",
      "  - UTFFixedIncomeAsset\n",
      "  - UTFMoneyAsset\n",
      "  - UTFOther\n",
      "  - ETFs\n",
      "  - NetBusinessInterests\n",
      "  - InvestmentProperties\n",
      "  - OtherInvestedAsset\n",
      "  - CPFOABalance\n",
      "  - CPFSABalance\n",
      "  - CPFMABalance\n",
      "  - CPFISOAEquityAsset\n",
      "  - CPFISOAFixedIncomeAsset\n",
      "  - CPFISOACashAsset\n",
      "  - CPFISOAOthersAsset\n",
      "  - CPFISSAEquityAsset\n",
      "  - CPFISSAFixedIncomeAsset\n",
      "  - CPFISSACashAsset\n",
      "  - CPFISSAOthersAsset\n",
      "  - SRSEquityAsset\n",
      "  - SRSFixedIncomeAsset\n",
      "  - SRSCashAsset\n",
      "  - SRSOther\n",
      "  - Completion\n",
      "  - CPFRABalance\n",
      "----------------------------------------\n",
      "📄 Dataset: emfc2productsolution\n",
      "🧾 Columns (25):\n",
      "  - #\n",
      "  - EMFC2ProductSolutionId\n",
      "  - EMFC2FNAId\n",
      "  - PersonalInformationId\n",
      "  - ProductId\n",
      "  - SubProductId\n",
      "  - ActivityType\n",
      "  - Replacement\n",
      "  - ReasonforReplacement\n",
      "  - RecommendedProductRejected\n",
      "  - ReasonforRejected\n",
      "  - InvestmentGoalId\n",
      "  - InvestmentHorizonId\n",
      "  - InvestmentRiskId\n",
      "  - ReasonBudgetLessThanPremium\n",
      "  - ThirdParty\n",
      "  - DateCreated\n",
      "  - AIProduct\n",
      "  - FNAProviderSubmissionTypeId\n",
      "  - Status\n",
      "  - StatusRemarks\n",
      "  - StatusTimeStamp\n",
      "  - CurrencyCode\n",
      "  - CurrencyToSGDExchangeRate\n",
      "  - TargetedPremium\n",
      "----------------------------------------\n",
      "📄 Dataset: emfc2productsolutiondetail\n",
      "🧾 Columns (5):\n",
      "  - #\n",
      "  - EMFC2ProductSolutionDetailId\n",
      "  - EMFC2ProductSolutionId\n",
      "  - ProductComponentCode\n",
      "  - ProductComponentValue\n",
      "----------------------------------------\n",
      "📄 Dataset: EMFC2ProductIntegrationApplication\n",
      "🧾 Columns (14):\n",
      "  - #\n",
      "  - integrationapplicationId\n",
      "  - applicationId\n",
      "  - efnaTransNo\n",
      "  - quotationId\n",
      "  - solutionId\n",
      "  - efnaQuotationId\n",
      "  - module\n",
      "  - caseId\n",
      "  - BISignDate\n",
      "  - AppFormSignDate\n",
      "  - eReferenceNumber\n",
      "  - userId\n",
      "  - isDone\n",
      "----------------------------------------\n",
      "📄 Dataset: EMFC2ProductIntegrationLog\n",
      "🧾 Columns (21):\n",
      "  - #\n",
      "  - integrationLogsId\n",
      "  - EMFC2ProductSolutionId\n",
      "  - efnaTransNo\n",
      "  - efnaQuotationId\n",
      "  - ProviderId\n",
      "  - ProductId\n",
      "  - ProductName\n",
      "  - jsonResponse\n",
      "  - userId\n",
      "  - TransactionDate\n",
      "  - Remarks\n",
      "  - isDone\n",
      "  - givenName\n",
      "  - familyName\n",
      "  - needs\n",
      "  - producttype\n",
      "  - personalinfoid\n",
      "  - ClientDOB\n",
      "  - jsonRequest\n",
      "  - AgentCode\n",
      "----------------------------------------\n",
      "📄 Dataset: ProductMainPlan\n",
      "🧾 Columns (22):\n",
      "  - #\n",
      "  - ProductId\n",
      "  - ProviderId\n",
      "  - ProductSubCategoryId\n",
      "  - MainPlan\n",
      "  - DateInclusion\n",
      "  - DateCeased\n",
      "  - Benefit\n",
      "  - Limitation\n",
      "  - RiskProfile\n",
      "  - CKA\n",
      "  - CAR\n",
      "  - Star\n",
      "  - Objective\n",
      "  - Horizon\n",
      "  - Fund\n",
      "  - Rider\n",
      "  - Status\n",
      "  - UserId\n",
      "  - CreatedDate\n",
      "  - ProductCode\n",
      "  - UlRequirement\n",
      "----------------------------------------\n",
      "📄 Dataset: ProductType\n",
      "🧾 Columns (8):\n",
      "  - #\n",
      "  - ProductTypeId\n",
      "  - TypeName\n",
      "  - TypeIcon\n",
      "  - InvestmentType\n",
      "  - MFC\n",
      "  - SIMApp\n",
      "  - Active\n",
      "----------------------------------------\n",
      "📄 Dataset: ProductCategory\n",
      "🧾 Columns (6):\n",
      "  - #\n",
      "  - ProductCategoryId\n",
      "  - CategoryName\n",
      "  - ProductTypeId\n",
      "  - CategoryIcon\n",
      "  - Active\n",
      "----------------------------------------\n",
      "📄 Dataset: productsubcategory\n",
      "🧾 Columns (13):\n",
      "  - #\n",
      "  - ProductSubCategoryId\n",
      "  - ProductCategoryId\n",
      "  - SubCategoryName\n",
      "  - SubCategoryShortName\n",
      "  - SubCategoryIcon\n",
      "  - IncludeFund\n",
      "  - IncludeRider\n",
      "  - IncludeStar\n",
      "  - IncludeObjective\n",
      "  - IncludeHorizon\n",
      "  - IncludeRiskProfile\n",
      "  - Active\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n=== COLUMN HEADERS FOR EACH DATASET ===\\n\")\n",
    "\n",
    "for name, df in datasets.items():\n",
    "    print(f\"📄 Dataset: {name}\")\n",
    "    print(f\"🧾 Columns ({len(df.columns)}):\")\n",
    "    for col in df.columns:\n",
    "        print(f\"  - {col}\")\n",
    "    print(\"-\" * 40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9102ee4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== COMPLETE UNIFIED DATASET CREATION WITH INCOME ENGINEERING ===\n",
      "🚀 Starting complete dataset unification...\n",
      "📊 Input datasets:\n",
      "   Client: 45,700 records\n",
      "   Personal Info: 52,534 records\n",
      "   EMFC2: 51,769 records\n",
      "   Assets: 50,502 records\n",
      "\n",
      "🏗️  Base dataset: 45,700 client records\n",
      "\n",
      "📝 Adding unique columns from emfc2personalinformation...\n",
      "   ✅ Added: PersonalInformationId\n",
      "   ✅ Added: EMFC2FNAId\n",
      "   ✅ Added: ClientAge\n",
      "   ✅ Added: ClientRetrieval\n",
      "\n",
      "🔄 Smart replacement for better quality columns...\n",
      "   🔄 ClientGender: Updated 1 missing values\n",
      "   🔄 IncomeRange: Updated 19,345 missing values\n",
      "   🔄 MaritalStatus: Updated 17,900 missing values\n",
      "   🔄 Education: Updated 763 missing values\n",
      "   🔄 EmploymentStatus: Updated 19,055 missing values\n",
      "   🔄 RiskProfile: Updated 12,183 missing values\n",
      "   🔄 CKAProfile: Updated 1,257 missing values\n",
      "   🔄 CARProfile: Updated 1,509 missing values\n",
      "   🔄 Nationality: Updated 19,699 missing values\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 309\u001b[39m\n\u001b[32m    306\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m unified_df\n\u001b[32m    308\u001b[39m \u001b[38;5;66;03m# Execute the complete unification\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m309\u001b[39m unified_dataset = \u001b[43mcreate_complete_unified_dataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdatasets\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 74\u001b[39m, in \u001b[36mcreate_complete_unified_dataset\u001b[39m\u001b[34m(datasets)\u001b[39m\n\u001b[32m     72\u001b[39m \u001b[38;5;66;03m# Find matching client records\u001b[39;00m\n\u001b[32m     73\u001b[39m mask = unified_df[\u001b[33m'\u001b[39m\u001b[33mClientId\u001b[39m\u001b[33m'\u001b[39m] == client_id\n\u001b[32m---> \u001b[39m\u001b[32m74\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mmask\u001b[49m\u001b[43m.\u001b[49m\u001b[43many\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[32m     75\u001b[39m     \u001b[38;5;66;03m# Update if current value is null\u001b[39;00m\n\u001b[32m     76\u001b[39m     null_mask = mask & unified_df[col].isnull()\n\u001b[32m     77\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m null_mask.any():\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\_vico\\source\\repos\\synergysg\\OCC\\.venv\\Lib\\site-packages\\pandas\\core\\series.py:6482\u001b[39m, in \u001b[36mSeries.any\u001b[39m\u001b[34m(self, axis, bool_only, skipna, **kwargs)\u001b[39m\n\u001b[32m   6480\u001b[39m nv.validate_logical_func((), kwargs, fname=\u001b[33m\"\u001b[39m\u001b[33many\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m   6481\u001b[39m validate_bool_kwarg(skipna, \u001b[33m\"\u001b[39m\u001b[33mskipna\u001b[39m\u001b[33m\"\u001b[39m, none_allowed=\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[32m-> \u001b[39m\u001b[32m6482\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_reduce\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   6483\u001b[39m \u001b[43m    \u001b[49m\u001b[43mnanops\u001b[49m\u001b[43m.\u001b[49m\u001b[43mnanany\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   6484\u001b[39m \u001b[43m    \u001b[49m\u001b[43mname\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43many\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   6485\u001b[39m \u001b[43m    \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m=\u001b[49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   6486\u001b[39m \u001b[43m    \u001b[49m\u001b[43mnumeric_only\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbool_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   6487\u001b[39m \u001b[43m    \u001b[49m\u001b[43mskipna\u001b[49m\u001b[43m=\u001b[49m\u001b[43mskipna\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   6488\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfilter_type\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mbool\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   6489\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\_vico\\source\\repos\\synergysg\\OCC\\.venv\\Lib\\site-packages\\pandas\\core\\series.py:6468\u001b[39m, in \u001b[36mSeries._reduce\u001b[39m\u001b[34m(self, op, name, axis, skipna, numeric_only, filter_type, **kwds)\u001b[39m\n\u001b[32m   6463\u001b[39m     \u001b[38;5;66;03m# GH#47500 - change to TypeError to match other methods\u001b[39;00m\n\u001b[32m   6464\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[32m   6465\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mSeries.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m does not allow \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkwd_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnumeric_only\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   6466\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mwith non-numeric dtypes.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   6467\u001b[39m     )\n\u001b[32m-> \u001b[39m\u001b[32m6468\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mop\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdelegate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mskipna\u001b[49m\u001b[43m=\u001b[49m\u001b[43mskipna\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\_vico\\source\\repos\\synergysg\\OCC\\.venv\\Lib\\site-packages\\pandas\\core\\nanops.py:520\u001b[39m, in \u001b[36mnanany\u001b[39m\u001b[34m(values, axis, skipna, mask)\u001b[39m\n\u001b[32m    489\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    490\u001b[39m \u001b[33;03mCheck if any elements along an axis evaluate to True.\u001b[39;00m\n\u001b[32m    491\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m    514\u001b[39m \u001b[33;03mFalse\u001b[39;00m\n\u001b[32m    515\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    516\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m values.dtype.kind \u001b[38;5;129;01min\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33miub\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m mask \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    517\u001b[39m     \u001b[38;5;66;03m# GH#26032 fastpath\u001b[39;00m\n\u001b[32m    518\u001b[39m     \u001b[38;5;66;03m# error: Incompatible return value type (got \"Union[bool_, ndarray]\",\u001b[39;00m\n\u001b[32m    519\u001b[39m     \u001b[38;5;66;03m# expected \"bool\")\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m520\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mvalues\u001b[49m\u001b[43m.\u001b[49m\u001b[43many\u001b[49m\u001b[43m(\u001b[49m\u001b[43maxis\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type: ignore[return-value]\u001b[39;00m\n\u001b[32m    522\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m values.dtype.kind == \u001b[33m\"\u001b[39m\u001b[33mM\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m    523\u001b[39m     \u001b[38;5;66;03m# GH#34479\u001b[39;00m\n\u001b[32m    524\u001b[39m     warnings.warn(\n\u001b[32m    525\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33m'\u001b[39m\u001b[33many\u001b[39m\u001b[33m'\u001b[39m\u001b[33m with datetime64 dtypes is deprecated and will raise in a \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    526\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mfuture version. Use (obj != pd.Timestamp(0)).any() instead.\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    527\u001b[39m         \u001b[38;5;167;01mFutureWarning\u001b[39;00m,\n\u001b[32m    528\u001b[39m         stacklevel=find_stack_level(),\n\u001b[32m    529\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\_vico\\source\\repos\\synergysg\\OCC\\.venv\\Lib\\site-packages\\numpy\\_core\\_methods.py:63\u001b[39m, in \u001b[36m_any\u001b[39m\u001b[34m(a, axis, dtype, out, keepdims, where)\u001b[39m\n\u001b[32m     61\u001b[39m \u001b[38;5;66;03m# Parsing keyword arguments is currently fairly slow, so avoid it for now\u001b[39;00m\n\u001b[32m     62\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m where \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m63\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mumr_any\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkeepdims\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     64\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m umr_any(a, axis, dtype, out, keepdims, where=where)\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "print(\"=== COMPLETE UNIFIED DATASET CREATION WITH INCOME ENGINEERING ===\")\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "\n",
    "def create_complete_unified_dataset(datasets):\n",
    "    \"\"\"Create complete unified dataset with client, personal info, and assets\"\"\"\n",
    "    \n",
    "    print(\"🚀 Starting complete dataset unification...\")\n",
    "    start_time = datetime.now()\n",
    "    \n",
    "    # Step 1: Get base datasets\n",
    "    client_df = datasets['client'].copy()\n",
    "    personal_df = datasets['emfc2personalinformation'].copy()\n",
    "    emfc2_df = datasets['emfc2'].copy()\n",
    "    assets_df = datasets['EMFC2Assets'].copy()\n",
    "    \n",
    "    print(f\"📊 Input datasets:\")\n",
    "    print(f\"   Client: {len(client_df):,} records\")\n",
    "    print(f\"   Personal Info: {len(personal_df):,} records\") \n",
    "    print(f\"   EMFC2: {len(emfc2_df):,} records\")\n",
    "    print(f\"   Assets: {len(assets_df):,} records\")\n",
    "    \n",
    "    # Step 2: Start with client as base (most comprehensive)\n",
    "    unified_df = client_df.copy()\n",
    "    print(f\"\\n🏗️  Base dataset: {len(unified_df):,} client records\")\n",
    "    \n",
    "    # Step 3: Add unique columns from personal info\n",
    "    print(f\"\\n📝 Adding unique columns from emfc2personalinformation...\")\n",
    "    personal_unique_cols = ['PersonalInformationId', 'EMFC2FNAId', 'ClientAge', 'ClientRetrieval']\n",
    "    \n",
    "    # Merge personal info (left join to keep all clients)\n",
    "    personal_merge_cols = ['ClientId'] + personal_unique_cols\n",
    "    available_personal_cols = [col for col in personal_merge_cols if col in personal_df.columns]\n",
    "    \n",
    "    unified_df = unified_df.merge(\n",
    "        personal_df[available_personal_cols],\n",
    "        on='ClientId',\n",
    "        how='left',\n",
    "        suffixes=('', '_personal')\n",
    "    )\n",
    "    \n",
    "    for col in personal_unique_cols:\n",
    "        if col in available_personal_cols:\n",
    "            print(f\"   ✅ Added: {col}\")\n",
    "    \n",
    "    # Step 4: Smart replacement for overlapping columns\n",
    "    print(f\"\\n🔄 Smart replacement for better quality columns...\")\n",
    "    \n",
    "    # Define overlapping columns to check\n",
    "    overlap_cols = [\n",
    "        'ClientGender', 'IncomeRange', 'MaritalStatus', 'Education', \n",
    "        'EmploymentStatus', 'RiskProfile', 'CKAProfile', 'CARProfile',\n",
    "        'Nationality', 'SpokenLanguage', 'WrittenLanguage'\n",
    "    ]\n",
    "    \n",
    "    # Create a mapping of better values from personal info\n",
    "    for col in overlap_cols:\n",
    "        if col in personal_df.columns and col in unified_df.columns:\n",
    "            # Get non-null values from personal info\n",
    "            personal_updates = personal_df[['ClientId', col]].dropna()\n",
    "            \n",
    "            if len(personal_updates) > 0:\n",
    "                updated_count = 0\n",
    "                \n",
    "                # Update where client data is null OR personal info has data\n",
    "                for _, row in personal_updates.iterrows():\n",
    "                    client_id = row['ClientId']\n",
    "                    new_value = row[col]\n",
    "                    \n",
    "                    # Find matching client records\n",
    "                    mask = unified_df['ClientId'] == client_id\n",
    "                    if mask.any():\n",
    "                        # Update if current value is null\n",
    "                        null_mask = mask & unified_df[col].isnull()\n",
    "                        if null_mask.any():\n",
    "                            unified_df.loc[null_mask, col] = new_value\n",
    "                            updated_count += null_mask.sum()\n",
    "                \n",
    "                if updated_count > 0:\n",
    "                    print(f\"   🔄 {col}: Updated {updated_count:,} missing values\")\n",
    "    \n",
    "    # Step 5: Add EMFC2Id bridge for assets\n",
    "    print(f\"\\n🌉 Adding EMFC2Id bridge...\")\n",
    "    \n",
    "    # Get the most recent EMFC2Id for each client (in case multiple sessions)\n",
    "    emfc2_bridge = emfc2_df.sort_values(['ClientId', 'EMFCSubmitDate'], na_position='last')\n",
    "    emfc2_bridge = emfc2_bridge.groupby('ClientId').agg({\n",
    "        'EMFC2Id': 'last',  # Most recent session\n",
    "        'EMFCStartDate': 'last',\n",
    "        'EMFCSubmitDate': 'last',\n",
    "        'EMFCStatus': 'last'\n",
    "    }).reset_index()\n",
    "    \n",
    "    # Add EMFC2 info to unified dataset\n",
    "    unified_df = unified_df.merge(\n",
    "        emfc2_bridge,\n",
    "        on='ClientId',\n",
    "        how='left'\n",
    "    )\n",
    "    \n",
    "    clients_with_emfc2 = unified_df['EMFC2Id'].notna().sum()\n",
    "    print(f\"   ✅ Clients with EMFC2 sessions: {clients_with_emfc2:,}\")\n",
    "    \n",
    "    # Step 6: Add financial assets\n",
    "    print(f\"\\n💰 Adding financial assets...\")\n",
    "    \n",
    "    # Select key asset columns\n",
    "    asset_cols = [\n",
    "        'EMFC2Id', 'SavingsAccounts', 'FixedDepositsAccount', 'HomeAsset', \n",
    "        'MotorAsset', 'InsuranceCashValues', 'StocksPortofolio', 'BondPortofolio',\n",
    "        'UTFEquityAsset', 'ETFs', 'InvestmentProperties', 'CPFOABalance', \n",
    "        'CPFSABalance', 'CPFMABalance', 'SRSEquityAsset'\n",
    "    ]\n",
    "    \n",
    "    available_asset_cols = [col for col in asset_cols if col in assets_df.columns]\n",
    "    \n",
    "    # Merge assets (left join to keep all clients)\n",
    "    unified_df = unified_df.merge(\n",
    "        assets_df[available_asset_cols],\n",
    "        on='EMFC2Id',\n",
    "        how='left'\n",
    "    )\n",
    "    \n",
    "    clients_with_assets = unified_df[available_asset_cols[1:]].notna().any(axis=1).sum()\n",
    "    print(f\"   ✅ Clients with asset data: {clients_with_assets:,}\")\n",
    "    \n",
    "    # Step 7: Create derived features\n",
    "    print(f\"\\n⚙️  Creating derived features...\")\n",
    "    \n",
    "    # Income feature engineering\n",
    "    if 'IncomeRange' in unified_df.columns:\n",
    "        print(f\"   💰 Engineering income features...\")\n",
    "        \n",
    "        # Income_Numeric - convert ranges to numeric midpoints\n",
    "        income_mapping = {\n",
    "            'No Income': 0,\n",
    "            'Below S$30,000': 15000,\n",
    "            'S$30,000 - S$49,999': 40000,\n",
    "            'S$50,000 - S$99,999': 75000,\n",
    "            'S$100,000 and above': 150000\n",
    "        }\n",
    "        \n",
    "        unified_df['Income_Numeric'] = unified_df['IncomeRange'].map(income_mapping)\n",
    "        print(f\"   ✅ Created: Income_Numeric\")\n",
    "        \n",
    "        # Income_Category - simplified groupings\n",
    "        income_category_mapping = {\n",
    "            'No Income': 'Low',\n",
    "            'Below S$30,000': 'Low',\n",
    "            'S$30,000 - S$49,999': 'Medium',\n",
    "            'S$50,000 - S$99,999': 'Medium',\n",
    "            'S$100,000 and above': 'High'\n",
    "        }\n",
    "        \n",
    "        unified_df['Income_Category'] = unified_df['IncomeRange'].map(income_category_mapping)\n",
    "        print(f\"   ✅ Created: Income_Category\")\n",
    "        \n",
    "        # Income_Ordinal - for ML algorithms that prefer numeric encoding\n",
    "        income_ordinal_mapping = {\n",
    "            'No Income': 0,\n",
    "            'Below S$30,000': 1,\n",
    "            'S$30,000 - S$49,999': 2,\n",
    "            'S$50,000 - S$99,999': 3,\n",
    "            'S$100,000 and above': 4\n",
    "        }\n",
    "        \n",
    "        unified_df['Income_Ordinal'] = unified_df['IncomeRange'].map(income_ordinal_mapping)\n",
    "        print(f\"   ✅ Created: Income_Ordinal\")\n",
    "    \n",
    "    # Total liquid assets\n",
    "    liquid_assets = ['SavingsAccounts', 'FixedDepositsAccount']\n",
    "    available_liquid = [col for col in liquid_assets if col in unified_df.columns]\n",
    "    if available_liquid:\n",
    "        unified_df['Total_Liquid_Assets'] = unified_df[available_liquid].fillna(0).sum(axis=1)\n",
    "        print(f\"   ✅ Created: Total_Liquid_Assets\")\n",
    "    \n",
    "    # Total investments\n",
    "    investment_assets = ['StocksPortofolio', 'BondPortofolio', 'UTFEquityAsset', 'ETFs']\n",
    "    available_investments = [col for col in investment_assets if col in unified_df.columns]\n",
    "    if available_investments:\n",
    "        unified_df['Total_Investments'] = unified_df[available_investments].fillna(0).sum(axis=1)\n",
    "        print(f\"   ✅ Created: Total_Investments\")\n",
    "    \n",
    "    # Total CPF\n",
    "    cpf_assets = ['CPFOABalance', 'CPFSABalance', 'CPFMABalance']\n",
    "    available_cpf = [col for col in cpf_assets if col in unified_df.columns]\n",
    "    if available_cpf:\n",
    "        unified_df['Total_CPF'] = unified_df[available_cpf].fillna(0).sum(axis=1)\n",
    "        print(f\"   ✅ Created: Total_CPF\")\n",
    "    \n",
    "    # Total net worth (excluding home to avoid double counting)\n",
    "    wealth_components = ['Total_Liquid_Assets', 'Total_Investments', 'Total_CPF']\n",
    "    if 'InvestmentProperties' in unified_df.columns:\n",
    "        wealth_components.append('InvestmentProperties')\n",
    "    \n",
    "    available_wealth = [col for col in wealth_components if col in unified_df.columns]\n",
    "    if available_wealth:\n",
    "        unified_df['Estimated_Net_Worth'] = unified_df[available_wealth].fillna(0).sum(axis=1)\n",
    "        print(f\"   ✅ Created: Estimated_Net_Worth\")\n",
    "    \n",
    "    # Investment ratio (investments / total financial assets)\n",
    "    if 'Total_Investments' in unified_df.columns and 'Total_Liquid_Assets' in unified_df.columns:\n",
    "        total_financial = unified_df['Total_Liquid_Assets'] + unified_df['Total_Investments']\n",
    "        unified_df['Investment_Ratio'] = np.where(\n",
    "            total_financial > 0,\n",
    "            unified_df['Total_Investments'] / total_financial,\n",
    "            0\n",
    "        )\n",
    "        print(f\"   ✅ Created: Investment_Ratio\")\n",
    "    \n",
    "    # Wealth-to-income ratio (financial capacity indicator)\n",
    "    if 'Estimated_Net_Worth' in unified_df.columns and 'Income_Numeric' in unified_df.columns:\n",
    "        unified_df['Wealth_to_Income_Ratio'] = np.where(\n",
    "            unified_df['Income_Numeric'] > 0,\n",
    "            unified_df['Estimated_Net_Worth'] / unified_df['Income_Numeric'],\n",
    "            np.nan  # Cannot calculate for zero income\n",
    "        )\n",
    "        print(f\"   ✅ Created: Wealth_to_Income_Ratio\")\n",
    "    \n",
    "    # Age groups for better ML performance\n",
    "    if 'ClientAge' in unified_df.columns:\n",
    "        unified_df['Age_Group'] = pd.cut(\n",
    "            unified_df['ClientAge'], \n",
    "            bins=[0, 25, 35, 45, 55, 65, 100], \n",
    "            labels=['Under_25', '25-35', '35-45', '45-55', '55-65', 'Over_65'],\n",
    "            include_lowest=True\n",
    "        )\n",
    "        print(f\"   ✅ Created: Age_Group\")\n",
    "    \n",
    "    # Life stage indicator (combination of age and marital status)\n",
    "    if 'Age_Group' in unified_df.columns and 'MaritalStatus' in unified_df.columns:\n",
    "        def determine_life_stage(row):\n",
    "            age_group = row['Age_Group']\n",
    "            marital_status = row['MaritalStatus']\n",
    "            \n",
    "            if pd.isna(age_group) or pd.isna(marital_status):\n",
    "                return 'Unknown'\n",
    "            \n",
    "            age_str = str(age_group)\n",
    "            marital_str = str(marital_status).lower()\n",
    "            \n",
    "            if age_str in ['Under_25', '25-35']:\n",
    "                if 'single' in marital_str:\n",
    "                    return 'Young_Single'\n",
    "                else:\n",
    "                    return 'Young_Family'\n",
    "            elif age_str in ['35-45', '45-55']:\n",
    "                if 'single' in marital_str:\n",
    "                    return 'Mid_Career_Single'\n",
    "                else:\n",
    "                    return 'Mid_Career_Family'\n",
    "            else:  # 55+ \n",
    "                return 'Pre_Retirement'\n",
    "        \n",
    "        unified_df['Life_Stage'] = unified_df.apply(determine_life_stage, axis=1)\n",
    "        print(f\"   ✅ Created: Life_Stage\")\n",
    "    \n",
    "    # Financial sophistication indicator\n",
    "    sophistication_indicators = []\n",
    "    if 'Education' in unified_df.columns:\n",
    "        sophistication_indicators.append('Education')\n",
    "    if 'Investment_Ratio' in unified_df.columns:\n",
    "        sophistication_indicators.append('Investment_Ratio')\n",
    "    \n",
    "    if len(sophistication_indicators) >= 2:\n",
    "        # Simple sophistication score based on education and investment behavior\n",
    "        def calculate_sophistication(row):\n",
    "            score = 0\n",
    "            \n",
    "            # Education component\n",
    "            education = str(row.get('Education', '')).lower()\n",
    "            if 'university' in education or 'degree' in education:\n",
    "                score += 2\n",
    "            elif 'diploma' in education or 'college' in education:\n",
    "                score += 1\n",
    "            \n",
    "            # Investment behavior component\n",
    "            inv_ratio = row.get('Investment_Ratio', 0)\n",
    "            if inv_ratio > 0.3:  # High investment allocation\n",
    "                score += 2\n",
    "            elif inv_ratio > 0.1:  # Some investment allocation\n",
    "                score += 1\n",
    "            \n",
    "            if score >= 3:\n",
    "                return 'High'\n",
    "            elif score >= 1:\n",
    "                return 'Medium'\n",
    "            else:\n",
    "                return 'Low'\n",
    "        \n",
    "        unified_df['Financial_Sophistication'] = unified_df.apply(calculate_sophistication, axis=1)\n",
    "        print(f\"   ✅ Created: Financial_Sophistication\")\n",
    "    \n",
    "    # Completion timestamp\n",
    "    end_time = datetime.now()\n",
    "    processing_time = (end_time - start_time).total_seconds()\n",
    "    \n",
    "    print(f\"\\n✅ UNIFICATION COMPLETE!\")\n",
    "    print(f\"   Final records: {len(unified_df):,}\")\n",
    "    print(f\"   Final columns: {len(unified_df.columns)}\")\n",
    "    print(f\"   Processing time: {processing_time:.1f} seconds\")\n",
    "    print(f\"   Memory usage: {unified_df.memory_usage(deep=True).sum() / 1024**2:.1f} MB\")\n",
    "    \n",
    "    return unified_df\n",
    "\n",
    "# Execute the complete unification\n",
    "unified_dataset = create_complete_unified_dataset(datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc093eb7-d2dd-4ede-813a-b13962dde832",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== STEP 2: CREATE UNIFIED CLIENT DATASET ===\n",
      "🔧 Creating unified client dataset...\n",
      "Base dataset: 45,700 client records\n",
      "\n",
      "📝 Adding unique columns from emfc2personalinformation:\n",
      "   Adding: PersonalInformationId\n",
      "   Adding: EMFC2FNAId\n",
      "   Adding: ClientAge\n",
      "   Adding: ClientRetrieval\n",
      "\n",
      "🔄 Smart replacement for better quality columns:\n",
      "   Replacing SelectedClient: 69.4% quality improvement\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\_vico\\AppData\\Local\\Temp\\ipykernel_16452\\2950318133.py:49: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value 'False' has dtype incompatible with float64, please explicitly cast to a compatible dtype first.\n",
      "  unified_df.loc[mask, col] = new_value\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 79\u001b[39m\n\u001b[32m     76\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m unified_df\n\u001b[32m     78\u001b[39m \u001b[38;5;66;03m# Create the unified dataset\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m79\u001b[39m unified_client = \u001b[43mcreate_unified_client_dataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mclient_df\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpersonal_df\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moverlap_quality\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     81\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m✅ UNIFIED CLIENT DATASET CREATED\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     82\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m   Records: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(unified_client)\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m,\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 49\u001b[39m, in \u001b[36mcreate_unified_client_dataset\u001b[39m\u001b[34m(client_df, personal_df, overlap_quality)\u001b[39m\n\u001b[32m     47\u001b[39m             \u001b[38;5;66;03m# Update in unified dataset where ClientId matches\u001b[39;00m\n\u001b[32m     48\u001b[39m             mask = unified_df[\u001b[33m'\u001b[39m\u001b[33mClientId\u001b[39m\u001b[33m'\u001b[39m] == client_id\n\u001b[32m---> \u001b[39m\u001b[32m49\u001b[39m             \u001b[43munified_df\u001b[49m\u001b[43m.\u001b[49m\u001b[43mloc\u001b[49m\u001b[43m[\u001b[49m\u001b[43mmask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcol\u001b[49m\u001b[43m]\u001b[49m = new_value\n\u001b[32m     51\u001b[39m \u001b[38;5;66;03m# Fill remaining missing values with personal info data\u001b[39;00m\n\u001b[32m     52\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m🩹 Filling remaining gaps:\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\_vico\\source\\repos\\synergysg\\OCC\\venv\\Lib\\site-packages\\pandas\\core\\indexing.py:911\u001b[39m, in \u001b[36m_LocationIndexer.__setitem__\u001b[39m\u001b[34m(self, key, value)\u001b[39m\n\u001b[32m    908\u001b[39m \u001b[38;5;28mself\u001b[39m._has_valid_setitem_indexer(key)\n\u001b[32m    910\u001b[39m iloc = \u001b[38;5;28mself\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.name == \u001b[33m\"\u001b[39m\u001b[33miloc\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m.obj.iloc\n\u001b[32m--> \u001b[39m\u001b[32m911\u001b[39m \u001b[43miloc\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_setitem_with_indexer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindexer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\_vico\\source\\repos\\synergysg\\OCC\\venv\\Lib\\site-packages\\pandas\\core\\indexing.py:1942\u001b[39m, in \u001b[36m_iLocIndexer._setitem_with_indexer\u001b[39m\u001b[34m(self, indexer, value, name)\u001b[39m\n\u001b[32m   1939\u001b[39m \u001b[38;5;66;03m# align and set the values\u001b[39;00m\n\u001b[32m   1940\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m take_split_path:\n\u001b[32m   1941\u001b[39m     \u001b[38;5;66;03m# We have to operate column-wise\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1942\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_setitem_with_indexer_split_path\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindexer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1943\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1944\u001b[39m     \u001b[38;5;28mself\u001b[39m._setitem_single_block(indexer, value, name)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\_vico\\source\\repos\\synergysg\\OCC\\venv\\Lib\\site-packages\\pandas\\core\\indexing.py:1970\u001b[39m, in \u001b[36m_iLocIndexer._setitem_with_indexer_split_path\u001b[39m\u001b[34m(self, indexer, value, name)\u001b[39m\n\u001b[32m   1967\u001b[39m ilocs = \u001b[38;5;28mself\u001b[39m._ensure_iterable_column_indexer(info_axis)\n\u001b[32m   1969\u001b[39m pi = indexer[\u001b[32m0\u001b[39m]\n\u001b[32m-> \u001b[39m\u001b[32m1970\u001b[39m lplane_indexer = \u001b[43mlength_of_indexer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m.\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1971\u001b[39m \u001b[38;5;66;03m# lplane_indexer gives the expected length of obj[indexer[0]]\u001b[39;00m\n\u001b[32m   1972\u001b[39m \n\u001b[32m   1973\u001b[39m \u001b[38;5;66;03m# we need an iterable, with a ndim of at least 1\u001b[39;00m\n\u001b[32m   1974\u001b[39m \u001b[38;5;66;03m# eg. don't pass through np.array(0)\u001b[39;00m\n\u001b[32m   1975\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_list_like_indexer(value) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(value, \u001b[33m\"\u001b[39m\u001b[33mndim\u001b[39m\u001b[33m\"\u001b[39m, \u001b[32m1\u001b[39m) > \u001b[32m0\u001b[39m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\_vico\\source\\repos\\synergysg\\OCC\\venv\\Lib\\site-packages\\pandas\\core\\indexers\\utils.py:323\u001b[39m, in \u001b[36mlength_of_indexer\u001b[39m\u001b[34m(indexer, target)\u001b[39m\n\u001b[32m    319\u001b[39m         indexer = np.array(indexer)\n\u001b[32m    321\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m indexer.dtype == \u001b[38;5;28mbool\u001b[39m:\n\u001b[32m    322\u001b[39m         \u001b[38;5;66;03m# GH#25774\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m323\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mindexer\u001b[49m\u001b[43m.\u001b[49m\u001b[43msum\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    324\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(indexer)\n\u001b[32m    325\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(indexer, \u001b[38;5;28mrange\u001b[39m):\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\_vico\\source\\repos\\synergysg\\OCC\\venv\\Lib\\site-packages\\numpy\\_core\\_methods.py:51\u001b[39m, in \u001b[36m_sum\u001b[39m\u001b[34m(a, axis, dtype, out, keepdims, initial, where)\u001b[39m\n\u001b[32m     49\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_sum\u001b[39m(a, axis=\u001b[38;5;28;01mNone\u001b[39;00m, dtype=\u001b[38;5;28;01mNone\u001b[39;00m, out=\u001b[38;5;28;01mNone\u001b[39;00m, keepdims=\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[32m     50\u001b[39m          initial=_NoValue, where=\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[32m---> \u001b[39m\u001b[32m51\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mumr_sum\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkeepdims\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minitial\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwhere\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "def export_to_excel_with_analysis(unified_df, filename='unified_client_dataset.xlsx'):\n",
    "    \"\"\"Export unified dataset to Excel with analysis sheets\"\"\"\n",
    "    \n",
    "    print(f\"💾 Exporting to {filename}...\")\n",
    "    \n",
    "    with pd.ExcelWriter(filename, engine='openpyxl') as writer:\n",
    "        \n",
    "        # Sheet 1: Main unified dataset\n",
    "        print(\"   📄 Writing main dataset...\")\n",
    "        unified_df.to_excel(writer, sheet_name='Unified_Client_Data', index=False)\n",
    "        \n",
    "        # Sheet 2: Data quality summary\n",
    "        print(\"   📊 Creating data quality summary...\")\n",
    "        quality_summary = []\n",
    "        \n",
    "        for col in unified_df.columns:\n",
    "            if col != '#':  # Skip row number column\n",
    "                total_records = len(unified_df)\n",
    "                missing_count = unified_df[col].isnull().sum()\n",
    "                missing_pct = (missing_count / total_records) * 100\n",
    "                unique_count = unified_df[col].nunique()\n",
    "                data_type = str(unified_df[col].dtype)\n",
    "                \n",
    "                # Sample values\n",
    "                sample_values = unified_df[col].dropna().head(3).tolist()\n",
    "                sample_str = ', '.join([str(x)[:20] for x in sample_values])\n",
    "                \n",
    "                quality_summary.append({\n",
    "                    'Column': col,\n",
    "                    'Data_Type': data_type,\n",
    "                    'Total_Records': total_records,\n",
    "                    'Missing_Count': missing_count,\n",
    "                    'Missing_%': round(missing_pct, 2),\n",
    "                    'Unique_Values': unique_count,\n",
    "                    'Sample_Values': sample_str\n",
    "                })\n",
    "        \n",
    "        quality_df = pd.DataFrame(quality_summary)\n",
    "        quality_df = quality_df.sort_values('Missing_%')\n",
    "        quality_df.to_excel(writer, sheet_name='Data_Quality_Report', index=False)\n",
    "        \n",
    "        # Sheet 3: ML-ready feature summary (UPDATED)\n",
    "        print(\"   🤖 Creating ML feature summary...\")\n",
    "        \n",
    "        # Define ML feature categories (UPDATED)\n",
    "        ml_feature_categories = {\n",
    "            'Demographics': ['ClientAge', 'Age_Group', 'ClientGender', 'Nationality', 'MaritalStatus'],\n",
    "            'Socioeconomic': ['Education', 'EmploymentStatus', 'IncomeRange', 'Income_Numeric', 'Income_Category', 'Income_Ordinal'],\n",
    "            'Risk_Profile': ['RiskProfile', 'CKAProfile', 'CARProfile'],\n",
    "            'Assets_Raw': ['SavingsAccounts', 'HomeAsset', 'StocksPortofolio', 'CPFOABalance'],\n",
    "            'Assets_Derived': ['Total_Liquid_Assets', 'Total_Investments', 'Total_CPF', 'Estimated_Net_Worth'],\n",
    "            'Financial_Ratios': ['Investment_Ratio', 'Wealth_to_Income_Ratio'],\n",
    "            'Life_Stage': ['Life_Stage', 'Financial_Sophistication'],\n",
    "            'Behavioral': ['ClientContactPreferences', 'SpokenLanguage', 'AccompaniedbyTrustedIndividual'],\n",
    "            'Business': ['ClientStatus', 'ClientType', 'AITransactionCapability', 'ClientSource']\n",
    "        }\n",
    "        \n",
    "        ml_summary = []\n",
    "        for category, features in ml_feature_categories.items():\n",
    "            for feature in features:\n",
    "                if feature in unified_df.columns:\n",
    "                    missing_pct = (unified_df[feature].isnull().sum() / len(unified_df)) * 100\n",
    "                    unique_count = unified_df[feature].nunique()\n",
    "                    \n",
    "                    # Quality score\n",
    "                    completeness_score = 100 - missing_pct\n",
    "                    if unique_count == 1:\n",
    "                        variance_score = 0\n",
    "                    elif unique_count > len(unified_df) * 0.9:\n",
    "                        variance_score = 50\n",
    "                    else:\n",
    "                        variance_score = 100\n",
    "                    \n",
    "                    overall_quality = (completeness_score * 0.7) + (variance_score * 0.3)\n",
    "                    \n",
    "                    recommendation = ('USE' if overall_quality >= 70 \n",
    "                                    else 'REVIEW' if overall_quality >= 40 \n",
    "                                    else 'EXCLUDE')\n",
    "                    \n",
    "                    ml_summary.append({\n",
    "                        'Category': category,\n",
    "                        'Feature': feature,\n",
    "                        'Missing_%': round(missing_pct, 1),\n",
    "                        'Unique_Values': unique_count,\n",
    "                        'Quality_Score': round(overall_quality, 1),\n",
    "                        'Recommendation': recommendation\n",
    "                    })\n",
    "        \n",
    "        ml_features_df = pd.DataFrame(ml_summary)\n",
    "        ml_features_df = ml_features_df.sort_values(['Category', 'Quality_Score'], ascending=[True, False])\n",
    "        ml_features_df.to_excel(writer, sheet_name='ML_Features_Analysis', index=False)\n",
    "        \n",
    "        # Sheet 4: Income Analysis (NEW)\n",
    "        print(\"   💰 Creating income analysis...\")\n",
    "        if 'IncomeRange' in unified_df.columns:\n",
    "            income_analysis = []\n",
    "            \n",
    "            # Income distribution\n",
    "            income_dist = unified_df['IncomeRange'].value_counts()\n",
    "            for income_range, count in income_dist.items():\n",
    "                pct = (count / len(unified_df)) * 100\n",
    "                income_analysis.append({\n",
    "                    'Income_Range': income_range,\n",
    "                    'Count': count,\n",
    "                    'Percentage': round(pct, 2),\n",
    "                    'Numeric_Value': unified_df[unified_df['IncomeRange'] == income_range]['Income_Numeric'].iloc[0] if 'Income_Numeric' in unified_df.columns else None\n",
    "                })\n",
    "            \n",
    "            income_df = pd.DataFrame(income_analysis)\n",
    "            income_df.to_excel(writer, sheet_name='Income_Analysis', index=False)\n",
    "        \n",
    "        # Sheet 5: Derived Features Summary (NEW)\n",
    "        print(\"   ⚙️  Creating derived features summary...\")\n",
    "        \n",
    "        derived_features_info = {\n",
    "            'Income_Numeric': 'Numeric midpoints of income ranges for calculations',\n",
    "            'Income_Category': 'Simplified Low/Medium/High income groupings',\n",
    "            'Income_Ordinal': 'Ordinal encoding (0-4) for ML algorithms',\n",
    "            'Total_Liquid_Assets': 'Sum of savings and fixed deposits',\n",
    "            'Total_Investments': 'Sum of stocks, bonds, ETFs, unit trusts',\n",
    "            'Total_CPF': 'Sum of all CPF account balances',\n",
    "            'Estimated_Net_Worth': 'Total financial assets (excluding primary residence)',\n",
    "            'Investment_Ratio': 'Investments / (Investments + Liquid Assets)',\n",
    "            'Wealth_to_Income_Ratio': 'Net worth / Annual income (financial capacity)',\n",
    "            'Age_Group': 'Age ranges for life stage analysis',\n",
    "            'Life_Stage': 'Combined age and marital status indicator',\n",
    "            'Financial_Sophistication': 'Education + investment behavior indicator'\n",
    "        }\n",
    "        \n",
    "        derived_summary = []\n",
    "        for feature, description in derived_features_info.items():\n",
    "            if feature in unified_df.columns:\n",
    "                missing_pct = (unified_df[feature].isnull().sum() / len(unified_df)) * 100\n",
    "                unique_count = unified_df[feature].nunique()\n",
    "                data_type = str(unified_df[feature].dtype)\n",
    "                \n",
    "                # Sample statistics for numeric features\n",
    "                if pd.api.types.is_numeric_dtype(unified_df[feature]):\n",
    "                    stats = unified_df[feature].describe()\n",
    "                    sample_stats = f\"Mean: {stats['mean']:.0f}, Median: {stats['50%']:.0f}\"\n",
    "                else:\n",
    "                    top_values = unified_df[feature].value_counts().head(3)\n",
    "                    sample_stats = ', '.join([f\"{k}: {v}\" for k, v in top_values.items()])\n",
    "                \n",
    "                derived_summary.append({\n",
    "                    'Feature': feature,\n",
    "                    'Description': description,\n",
    "                    'Data_Type': data_type,\n",
    "                    'Missing_%': round(missing_pct, 1),\n",
    "                    'Unique_Values': unique_count,\n",
    "                    'Sample_Statistics': sample_stats\n",
    "                })\n",
    "        \n",
    "        derived_df = pd.DataFrame(derived_summary)\n",
    "        derived_df.to_excel(writer, sheet_name='Derived_Features', index=False)\n",
    "        \n",
    "        # Sheet 6: Summary statistics (existing)\n",
    "        print(\"   📈 Creating summary statistics...\")\n",
    "        \n",
    "        # Numeric columns summary\n",
    "        numeric_cols = unified_df.select_dtypes(include=[np.number]).columns\n",
    "        if len(numeric_cols) > 0:\n",
    "            numeric_summary = unified_df[numeric_cols].describe()\n",
    "            numeric_summary.to_excel(writer, sheet_name='Numeric_Statistics')\n",
    "    \n",
    "    print(f\"✅ Export complete: {filename}\")\n",
    "    \n",
    "    # File size info\n",
    "    import os\n",
    "    file_size = os.path.getsize(filename) / 1024**2\n",
    "    print(f\"   📏 File size: {file_size:.1f} MB\")\n",
    "    \n",
    "    return filename\n",
    "\n",
    "# Export to Excel\n",
    "excel_filename = export_to_excel_with_analysis(unified_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6bc38e53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== OUTLIER ANALYSIS ===\n",
      "\n",
      "--- SavingsAccounts ---\n",
      "   Records with data: 50,712\n",
      "   Median: $0\n",
      "   Mean: $77,807\n",
      "   Max: $50,000,000\n",
      "   Statistical outliers (>Q3+1.5*IQR): 6,418 (12.7%)\n",
      "   Extreme outliers (>Q3+3*IQR): 3,788 (7.5%)\n",
      "   Unreasonable values (>$1,000,000): 355\n",
      "   Top 5 extreme values: [50000000.0, 30000000.0, 30000000.0, 30000000.0, 30000000.0]\n",
      "\n",
      "--- FixedDepositsAccount ---\n",
      "   Records with data: 50,712\n",
      "   Median: $0\n",
      "   Mean: $7,840\n",
      "   Max: $5,000,000\n",
      "   Statistical outliers (>Q3+1.5*IQR): 3,076 (6.1%)\n",
      "   Extreme outliers (>Q3+3*IQR): 3,076 (6.1%)\n",
      "   Unreasonable values (>$2,000,000): 3\n",
      "   Top 5 extreme values: [5000000.0, 3000000.0, 2014000.0, 2000000.0, 2000000.0]\n",
      "\n",
      "--- HomeAsset ---\n",
      "   Records with data: 50,712\n",
      "   Median: $0\n",
      "   Mean: $139,269\n",
      "   Max: $100,000,000\n",
      "   Statistical outliers (>Q3+1.5*IQR): 7,470 (14.7%)\n",
      "   Extreme outliers (>Q3+3*IQR): 7,470 (14.7%)\n",
      "   Unreasonable values (>$5,000,000): 78\n",
      "   Top 5 extreme values: [100000000.0, 100000000.0, 20000000.0, 20000000.0, 15000000.0]\n",
      "\n",
      "--- StocksPortofolio ---\n",
      "   Records with data: 50,712\n",
      "   Median: $0\n",
      "   Mean: $16,393\n",
      "   Max: $36,304,000\n",
      "   Statistical outliers (>Q3+1.5*IQR): 3,367 (6.6%)\n",
      "   Extreme outliers (>Q3+3*IQR): 3,367 (6.6%)\n",
      "   Unreasonable values (>$10,000,000): 10\n",
      "   Top 5 extreme values: [36304000.0, 36304000.0, 30000000.0, 20000000.0, 20000000.0]\n",
      "\n",
      "--- Total_Liquid_Assets ---\n",
      "   Records with data: 65,089\n",
      "   Median: $0\n",
      "   Mean: $66,729\n",
      "   Max: $50,000,000\n",
      "   Statistical outliers (>Q3+1.5*IQR): 10,892 (16.7%)\n",
      "   Extreme outliers (>Q3+3*IQR): 7,221 (11.1%)\n",
      "   Unreasonable values (>$3,000,000): 64\n",
      "   Top 5 extreme values: [50000000.0, 30000000.0, 30000000.0, 30000000.0, 30000000.0]\n",
      "\n",
      "--- Total_Investments ---\n",
      "   Records with data: 65,089\n",
      "   Median: $0\n",
      "   Mean: $32,643\n",
      "   Max: $36,304,000\n",
      "   Statistical outliers (>Q3+1.5*IQR): 8,042 (12.4%)\n",
      "   Extreme outliers (>Q3+3*IQR): 8,042 (12.4%)\n",
      "   Unreasonable values (>$15,000,000): 9\n",
      "   Top 5 extreme values: [36304000.0, 36304000.0, 30000000.0, 20000000.0, 20000000.0]\n",
      "\n",
      "--- Total_CPF ---\n",
      "   Records with data: 65,089\n",
      "   Median: $0\n",
      "   Mean: $43,711\n",
      "   Max: $34,992,163\n",
      "   Statistical outliers (>Q3+1.5*IQR): 13,382 (20.6%)\n",
      "   Extreme outliers (>Q3+3*IQR): 13,382 (20.6%)\n",
      "   Unreasonable values (>$500,000): 665\n",
      "   Top 5 extreme values: [34992163.0, 16529501.0, 16529501.0, 16529501.0, 14666482.0]\n",
      "\n",
      "--- Estimated_Net_Worth ---\n",
      "   Records with data: 65,089\n",
      "   Median: $0\n",
      "   Mean: $160,674\n",
      "   Max: $65,000,000\n",
      "   Statistical outliers (>Q3+1.5*IQR): 9,620 (14.8%)\n",
      "   Extreme outliers (>Q3+3*IQR): 6,176 (9.5%)\n",
      "   Unreasonable values (>$20,000,000): 33\n",
      "   Top 5 extreme values: [65000000.0, 50400000.0, 50400000.0, 50400000.0, 50400000.0]\n",
      "\n",
      "=== OUTLIER SUMMARY TABLE ===\n",
      "              column  median          mean   max_value  extreme_outlier_count  unreasonable_count\n",
      "     SavingsAccounts     0.0  77806.676566  50000000.0                   3788                 355\n",
      "FixedDepositsAccount     0.0   7840.493596   5000000.0                   3076                   3\n",
      "           HomeAsset     0.0 139269.369207 100000000.0                   7470                  78\n",
      "    StocksPortofolio     0.0  16393.030648  36304000.0                   3367                  10\n",
      " Total_Liquid_Assets     0.0  66729.236787  50000000.0                   7221                  64\n",
      "   Total_Investments     0.0  32643.351256  36304000.0                   8042                   9\n",
      "           Total_CPF     0.0  43711.252194  34992163.0                  13382                 665\n",
      " Estimated_Net_Worth     0.0 160673.555798  65000000.0                   6176                  33\n",
      "\n",
      "=== DATA ENTRY ERROR PATTERNS ===\n",
      "SavingsAccounts: 27808 values ending in 6+ zeros\n",
      "HomeAsset: 43879 values ending in 6+ zeros\n",
      "StocksPortofolio: 47390 values ending in 6+ zeros\n",
      "\n",
      "=== SPECIFIC ERROR CHECKS ===\n",
      "Possible 'cents instead of dollars' errors: 0\n",
      "Impossible: Total_Liquid_Assets < SavingsAccounts: 0\n"
     ]
    }
   ],
   "source": [
    "# Check unified Client dataset statistics\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Load the unified dataset\n",
    "unified_df = pd.read_excel('unified_client_dataset.xlsx', sheet_name='Unified_Client_Data')\n",
    "\n",
    "print(\"=== OUTLIER ANALYSIS ===\")\n",
    "\n",
    "# Define financial columns to check\n",
    "financial_cols = [\n",
    "    'SavingsAccounts', 'FixedDepositsAccount', 'HomeAsset', 'StocksPortofolio', \n",
    "    'Total_Liquid_Assets', 'Total_Investments', 'Total_CPF', 'Estimated_Net_Worth'\n",
    "]\n",
    "\n",
    "def analyze_outliers(df, column):\n",
    "    \"\"\"Analyze outliers in a financial column\"\"\"\n",
    "    if column not in df.columns:\n",
    "        print(f\"   ❌ {column} not found\")\n",
    "        return\n",
    "    \n",
    "    data = df[column].dropna()\n",
    "    if len(data) == 0:\n",
    "        print(f\"   ❌ {column} has no data\")\n",
    "        return\n",
    "    \n",
    "    # Basic stats\n",
    "    q1 = data.quantile(0.25)\n",
    "    q3 = data.quantile(0.75)\n",
    "    iqr = q3 - q1\n",
    "    median = data.median()\n",
    "    mean = data.mean()\n",
    "    \n",
    "    # Outlier thresholds\n",
    "    lower_bound = q1 - 1.5 * iqr\n",
    "    upper_bound = q3 + 1.5 * iqr\n",
    "    \n",
    "    # Extreme outlier thresholds  \n",
    "    extreme_upper = q3 + 3 * iqr\n",
    "    \n",
    "    # Count outliers\n",
    "    outliers = data[(data < lower_bound) | (data > upper_bound)]\n",
    "    extreme_outliers = data[data > extreme_upper]\n",
    "    \n",
    "    # Reasonable maximums for Singapore context\n",
    "    reasonable_max = {\n",
    "        'SavingsAccounts': 1_000_000,      # 1M SGD reasonable max savings\n",
    "        'FixedDepositsAccount': 2_000_000,  # 2M SGD reasonable max FD\n",
    "        'HomeAsset': 5_000_000,            # 5M SGD reasonable max home value\n",
    "        'StocksPortofolio': 10_000_000,    # 10M SGD reasonable max stocks\n",
    "        'Total_Liquid_Assets': 3_000_000,   # 3M SGD reasonable max liquid\n",
    "        'Total_Investments': 15_000_000,    # 15M SGD reasonable max investments\n",
    "        'Total_CPF': 500_000,              # 500K SGD reasonable max CPF\n",
    "        'Estimated_Net_Worth': 20_000_000   # 20M SGD reasonable max net worth\n",
    "    }\n",
    "    \n",
    "    unreasonable = data[data > reasonable_max.get(column, float('inf'))]\n",
    "    \n",
    "    print(f\"\\n--- {column} ---\")\n",
    "    print(f\"   Records with data: {len(data):,}\")\n",
    "    print(f\"   Median: ${median:,.0f}\")\n",
    "    print(f\"   Mean: ${mean:,.0f}\")\n",
    "    print(f\"   Max: ${data.max():,.0f}\")\n",
    "    print(f\"   Statistical outliers (>Q3+1.5*IQR): {len(outliers):,} ({len(outliers)/len(data)*100:.1f}%)\")\n",
    "    print(f\"   Extreme outliers (>Q3+3*IQR): {len(extreme_outliers):,} ({len(extreme_outliers)/len(data)*100:.1f}%)\")\n",
    "    print(f\"   Unreasonable values (>${reasonable_max.get(column, 0):,}): {len(unreasonable):,}\")\n",
    "    \n",
    "    if len(extreme_outliers) > 0:\n",
    "        print(f\"   Top 5 extreme values: {sorted(extreme_outliers, reverse=True)[:5]}\")\n",
    "    \n",
    "    return {\n",
    "        'column': column,\n",
    "        'total_records': len(data),\n",
    "        'median': median,\n",
    "        'mean': mean,\n",
    "        'max_value': data.max(),\n",
    "        'outlier_count': len(outliers),\n",
    "        'extreme_outlier_count': len(extreme_outliers),\n",
    "        'unreasonable_count': len(unreasonable)\n",
    "    }\n",
    "\n",
    "# Analyze each financial column\n",
    "outlier_summary = []\n",
    "for col in financial_cols:\n",
    "    result = analyze_outliers(unified_df, col)\n",
    "    if result:\n",
    "        outlier_summary.append(result)\n",
    "\n",
    "# Summary table\n",
    "print(f\"\\n=== OUTLIER SUMMARY TABLE ===\")\n",
    "summary_df = pd.DataFrame(outlier_summary)\n",
    "if not summary_df.empty:\n",
    "    print(summary_df[['column', 'median', 'mean', 'max_value', 'extreme_outlier_count', 'unreasonable_count']].to_string(index=False))\n",
    "\n",
    "# Check for data entry patterns that suggest errors\n",
    "print(f\"\\n=== DATA ENTRY ERROR PATTERNS ===\")\n",
    "\n",
    "# Look for suspiciously round numbers or repeated patterns\n",
    "for col in ['SavingsAccounts', 'HomeAsset', 'StocksPortofolio']:\n",
    "    if col in unified_df.columns:\n",
    "        data = unified_df[col].dropna()\n",
    "        \n",
    "        # Check for values ending in many zeros (likely data entry errors)\n",
    "        very_round = data[data % 1_000_000 == 0]  # Values ending in 6+ zeros\n",
    "        print(f\"{col}: {len(very_round)} values ending in 6+ zeros\")\n",
    "        \n",
    "        # Check for scientific notation-like values\n",
    "        huge_values = data[data > 1_000_000_000]  # > 1 billion\n",
    "        if len(huge_values) > 0:\n",
    "            print(f\"{col}: {len(huge_values)} values > 1 billion - likely data errors\")\n",
    "\n",
    "# Specific checks for common data entry mistakes\n",
    "print(f\"\\n=== SPECIFIC ERROR CHECKS ===\")\n",
    "\n",
    "# Check if values might be in cents instead of dollars\n",
    "cents_candidates = unified_df[unified_df['SavingsAccounts'] > 100_000_000]  # >100M\n",
    "print(f\"Possible 'cents instead of dollars' errors: {len(cents_candidates)}\")\n",
    "\n",
    "# Check for impossible combinations\n",
    "if 'Total_Liquid_Assets' in unified_df.columns and 'SavingsAccounts' in unified_df.columns:\n",
    "    impossible = unified_df[unified_df['Total_Liquid_Assets'] < unified_df['SavingsAccounts']]\n",
    "    print(f\"Impossible: Total_Liquid_Assets < SavingsAccounts: {len(impossible)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5b8276fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== FINAL PREPARATION FOR ML ===\n",
      "Step 1: Deduplicating for ML training...\n",
      "  Original records: 65,089\n",
      "  Unique clients: 45,698\n",
      "  Removed duplicates: 19,391\n",
      "\n",
      "Step 2: Final feature quality assessment...\n",
      "  ML-ready features: 14\n",
      "  High-quality categories: 4\n",
      "\n",
      "💾 Saved: ML_READY_client_dataset.xlsx\n",
      "   Records: 45,698 unique clients\n",
      "   Features: 84 columns\n",
      "   Status: Ready for Week 2 - Product Recommendation Analysis\n",
      "\n",
      "📊 FINAL DATA PREVIEW:\n",
      "Age distribution: {'25%': 33.0, '50%': 40.0, '75%': 49.0}\n",
      "Income distribution: {'Medium': 14761, 'Low': 10477, 'High': 6883}\n",
      "Asset holders: 10,934 clients (23.9%)\n"
     ]
    }
   ],
   "source": [
    "print(\"=== FINAL PREPARATION FOR ML ===\")\n",
    "\n",
    "# Load your manually cleaned data\n",
    "unified_df = pd.read_excel('unified_client_dataset.xlsx')  # Replace with your actual filename\n",
    "\n",
    "# 1. Deduplicate for ML training\n",
    "print(\"Step 1: Deduplicating for ML training...\")\n",
    "\n",
    "# Strategy: Keep most recent session with asset data per client\n",
    "unified_df['Has_Assets'] = (\n",
    "    unified_df['SavingsAccounts'].fillna(0) + \n",
    "    unified_df['HomeAsset'].fillna(0) + \n",
    "    unified_df['StocksPortofolio'].fillna(0)\n",
    ") > 0\n",
    "\n",
    "# Sort and deduplicate\n",
    "unified_df_sorted = unified_df.sort_values([\n",
    "    'ClientId', 'Has_Assets', 'EMFCStartDate'\n",
    "], ascending=[True, False, False])\n",
    "\n",
    "deduplicated_df = unified_df_sorted.drop_duplicates('ClientId', keep='first')\n",
    "\n",
    "print(f\"  Original records: {len(unified_df):,}\")\n",
    "print(f\"  Unique clients: {len(deduplicated_df):,}\")\n",
    "print(f\"  Removed duplicates: {len(unified_df) - len(deduplicated_df):,}\")\n",
    "\n",
    "# 2. Final feature quality check\n",
    "print(f\"\\nStep 2: Final feature quality assessment...\")\n",
    "\n",
    "ml_ready_features = {\n",
    "    'Demographics': ['ClientAge', 'ClientGender', 'Nationality', 'MaritalStatus'],\n",
    "    'Socioeconomic': ['IncomeRange', 'Income_Numeric', 'Education'],\n",
    "    'Financial': ['SavingsAccounts', 'HomeAsset', 'Total_Liquid_Assets', 'Estimated_Net_Worth'],\n",
    "    'Risk': ['RiskProfile', 'CKAProfile', 'CARProfile'],\n",
    "    'Derived': ['Age_Group', 'Income_Category', 'Investment_Ratio', 'Life_Stage']\n",
    "}\n",
    "\n",
    "feature_quality = []\n",
    "for category, features in ml_ready_features.items():\n",
    "    for feature in features:\n",
    "        if feature in deduplicated_df.columns:\n",
    "            missing_pct = (deduplicated_df[feature].isnull().sum() / len(deduplicated_df)) * 100\n",
    "            unique_count = deduplicated_df[feature].nunique()\n",
    "            \n",
    "            quality_score = 100 - missing_pct if unique_count > 1 else 0\n",
    "            status = \"READY\" if quality_score >= 70 else \"REVIEW\" if quality_score >= 40 else \"EXCLUDE\"\n",
    "            \n",
    "            feature_quality.append({\n",
    "                'Category': category,\n",
    "                'Feature': feature,\n",
    "                'Missing_%': round(missing_pct, 1),\n",
    "                'Unique_Values': unique_count,\n",
    "                'Status': status\n",
    "            })\n",
    "\n",
    "quality_df = pd.DataFrame(feature_quality)\n",
    "ready_features = quality_df[quality_df['Status'] == 'READY']\n",
    "\n",
    "print(f\"  ML-ready features: {len(ready_features)}\")\n",
    "print(f\"  High-quality categories: {ready_features['Category'].nunique()}\")\n",
    "\n",
    "# 3. Save ML-ready dataset\n",
    "deduplicated_df.to_excel('ML_READY_client_dataset.xlsx', index=False)\n",
    "print(f\"\\n💾 Saved: ML_READY_client_dataset.xlsx\")\n",
    "print(f\"   Records: {len(deduplicated_df):,} unique clients\")\n",
    "print(f\"   Features: {len(deduplicated_df.columns)} columns\")\n",
    "print(f\"   Status: Ready for Week 2 - Product Recommendation Analysis\")\n",
    "\n",
    "# 4. Quick preview of data distribution\n",
    "print(f\"\\n📊 FINAL DATA PREVIEW:\")\n",
    "print(f\"Age distribution: {deduplicated_df['ClientAge'].describe()[['25%', '50%', '75%']].to_dict()}\")\n",
    "print(f\"Income distribution: {deduplicated_df['Income_Category'].value_counts().to_dict()}\")\n",
    "print(f\"Asset holders: {(deduplicated_df['Total_Liquid_Assets'] > 0).sum():,} clients ({(deduplicated_df['Total_Liquid_Assets'] > 0).mean()*100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83430f9a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12959a92",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ac33829",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02acd340-5e8d-4c00-a0a7-6cfb93d4ca39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Layer 2 - Target Feature Analysis\n",
    "# Goal: Join client features with product recommendation data to create ML training labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3afda59a-e2e2-4e15-9b18-27579b241902",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== PHASE 2: TARGET VARIABLE ENGINEERING ===\n",
      "Goal: Join client features with product recommendation data to create ML training labels\n",
      "\n",
      "=== STEP 1: LOAD PRODUCT AND RECOMMENDATION TABLES ===\n",
      "✅ Loaded emfc2productsolution: 61,733 records\n",
      "✅ Loaded ProductMainPlan: 1,535 records\n",
      "✅ Loaded ProductType: 4 records\n",
      "✅ Loaded ProductCategory: 10 records\n",
      "✅ Loaded productsubcategory: 47 records\n",
      "\n",
      "=== STEP 2: BUILD COMPLETE PRODUCT HIERARCHY ===\n",
      "✅ Complete product hierarchy: 1,535 products\n",
      "   Product levels: ProductId → SubCategory → Category → Type\n",
      "\n",
      "📋 PRODUCT HIERARCHY SAMPLE:\n",
      "                           ProductId                                   MainPlan                       SubCategoryName                        CategoryName          TypeName\n",
      "004a70d6-dc07-4c28-a7bf-56fb03fe2204                                       Maid                                  Maid                            Personal General Insurance\n",
      "005859a3-af98-48bb-937b-b7d4c1b0a46c                                   Non-Wrap                                    UT                              Invest Wealth Management\n",
      "005a7dd4-c987-4bad-85a4-7a16b465ad75                       Etiqa Invest Plus SP Investment Linked Plan - Accumulation                 Wealth Accumulation    Life Insurance\n",
      "008f18c7-8095-43b7-a1f6-bf11e8165f08                                     PAStar                     Personal Accident                          Commercial General Insurance\n",
      "00acc921-e571-4070-9896-6c46bf2c4b61 Singlife ElderShield Standard (fka MyCare)                  Long Term Care Plans                   Wealth Protection    Life Insurance\n",
      "00d42002-1d2e-4bb2-a074-195eff06f67e                          RSP/Top Up to ILP   Policy Servicing (Non Shield Plans) Policy Servicing (Non Shield Plans)    Life Insurance\n",
      "01077cab-9be0-403b-9db0-1497c3985f15                                   VivoCash                     Whole Life Income                 Wealth Accumulation    Life Insurance\n",
      "011d57d9-2245-4076-8acd-0ae9e9112ef4                            Cyber Liability                                Others                          Commercial General Insurance\n",
      "0153b587-7f27-4069-a1f1-0ed668837c8d                           RHI-BUPA Company                         Packaged Plan                   Employee Benefits             Group\n",
      "015b4a97-be35-423e-9da6-ea5a88271196                            Star Secure Pro                            Whole Life                   Wealth Protection    Life Insurance\n",
      "\n",
      "=== STEP 3: ANALYZE TARGET VARIABLES ===\n",
      "📊 PRODUCT RECOMMENDATION ANALYSIS:\n",
      "   Total recommendations: 61,733\n",
      "   Recommendations with product info: 61,732\n",
      "\n",
      "🎯 TARGET OPTION 1: PRODUCT TYPE\n",
      "Product Type distribution:\n",
      "   Life Insurance: 52,272 (84.7%)\n",
      "   Wealth Management: 9,460 (15.3%)\n",
      "\n",
      "🎯 TARGET OPTION 2: PRODUCT CATEGORY\n",
      "Product Category distribution:\n",
      "   Wealth Protection: 43,941 (71.2%)\n",
      "   Invest: 9,460 (15.3%)\n",
      "   Wealth Accumulation: 7,563 (12.3%)\n",
      "   Policy Servicing (Non Shield Plans): 768 (1.2%)\n",
      "\n",
      "🎯 TARGET OPTION 3: PRODUCT SUBCATEGORY (SELECTED TARGET)\n",
      "Product SubCategory distribution (top 15):\n",
      "   Long Term Care Plans: 15,827 (25.6%)\n",
      "   SHIELD: 12,640 (20.5%)\n",
      "   UT: 9,239 (15.0%)\n",
      "   Term: 7,080 (11.5%)\n",
      "   Investment Linked Plan - Accumulation: 4,207 (6.8%)\n",
      "   Whole Life: 3,696 (6.0%)\n",
      "   Accident and Health Plans: 1,853 (3.0%)\n",
      "   Critical Illness Plans: 1,744 (2.8%)\n",
      "   ENDOW (NP): 1,373 (2.2%)\n",
      "   Retirement: 1,013 (1.6%)\n",
      "   Add Rider to existing shield plans: 787 (1.3%)\n",
      "   Policy Servicing (Non Shield Plans): 768 (1.2%)\n",
      "   Endowment: 664 (1.1%)\n",
      "   Whole Life Income: 299 (0.5%)\n",
      "   Universal Life (Protection): 172 (0.3%)\n",
      "\n",
      "Total subcategories available: 22\n",
      "\n",
      "=== STEP 4: CREATE ML TRAINING DATASET ===\n",
      "✅ Loaded client features: 65,089 unique clients\n",
      "🔗 JOINING CLIENTS WITH RECOMMENDATIONS:\n",
      "   Clients with recommendations: 44,189\n",
      "   Total training records: 369,054\n",
      "\n",
      "=== STEP 5: PREPARE TARGET VARIABLE (SUBCATEGORIES) ===\n",
      "   Records with product subcategory: 369,053\n",
      "\n",
      "📊 FINAL TARGET VARIABLE DISTRIBUTION (SUBCATEGORIES):\n",
      "   Long_Term_Care_Plans: 118,271 (32.0%)\n",
      "   SHIELD: 89,471 (24.2%)\n",
      "   Term: 58,301 (15.8%)\n",
      "   UT: 34,848 (9.4%)\n",
      "   Investment_Linked_Plan___Accumulation: 29,815 (8.1%)\n",
      "   Whole_Life: 16,500 (4.5%)\n",
      "   Critical_Illness_Plans: 14,550 (3.9%)\n",
      "   Accident_and_Health_Plans: 1,853 (0.5%)\n",
      "   ENDOW_NP: 1,373 (0.4%)\n",
      "   Retirement: 1,013 (0.3%)\n",
      "   Add_Rider_to_existing_shield_plans: 787 (0.2%)\n",
      "   Policy_Servicing_Non_Shield_Plans: 768 (0.2%)\n",
      "   Endowment: 663 (0.2%)\n",
      "   Whole_Life_Income: 299 (0.1%)\n",
      "   Universal_Life_Protection: 172 (0.0%)\n",
      "\n",
      "Total subcategories: 22\n",
      "\n",
      "⚖️  CLASS BALANCE ANALYSIS (SUBCATEGORIES):\n",
      "   Smallest class: 3 samples\n",
      "   Largest class: 118,271 samples\n",
      "   Imbalance ratio: 39423.7:1\n",
      "   Number of classes: 22\n",
      "\n",
      "📊 SMALL SUBCATEGORIES (<100 samples): 5\n",
      "Small subcategories:\n",
      "   Discretionary_Managed_Account: 74\n",
      "   Bond: 32\n",
      "   ILP: 5\n",
      "   Universal_Life_Accumulation: 4\n",
      "   Education_Funding: 3\n",
      "\n",
      "=== STEP 6: HANDLE SMALL SUBCATEGORIES ===\n",
      "🔧 SUBCATEGORIES WITH <50 SAMPLES:\n",
      "   Count: 4\n",
      "   Small subcategories to remove:\n",
      "     Bond: 32 samples\n",
      "     ILP: 5 samples\n",
      "     Universal_Life_Accumulation: 4 samples\n",
      "     Education_Funding: 3 samples\n",
      "\n",
      "   After removing small subcategories:\n",
      "     Original records: 369,053\n",
      "     Cleaned records: 369,009\n",
      "     Remaining subcategories: 18\n",
      "\n",
      "=== STEP 7: PREPARE FINAL ML FEATURES ===\n",
      "📊 FEATURE AVAILABILITY:\n",
      "   Requested features: 24\n",
      "   Available features: 24\n",
      "\n",
      "🔍 FEATURE QUALITY CHECK:\n",
      "   ClientAge: 83.3% missing, 77 unique - Poor\n",
      "   Age_Group: 83.3% missing, 6 unique - Poor\n",
      "   ClientGender: 75.1% missing, 4 unique - Poor\n",
      "   Nationality: 79.0% missing, 92 unique - Poor\n",
      "   MaritalStatus: 80.6% missing, 8 unique - Poor\n",
      "   IncomeRange: 81.1% missing, 5 unique - Poor\n",
      "   Income_Numeric: 81.1% missing, 5 unique - Poor\n",
      "   Income_Category: 81.1% missing, 3 unique - Poor\n",
      "   Education: 80.6% missing, 8 unique - Poor\n",
      "   EmploymentStatus: 83.4% missing, 6 unique - Poor\n",
      "   SavingsAccounts: 83.5% missing, 873 unique - Poor\n",
      "   HomeAsset: 83.5% missing, 570 unique - Poor\n",
      "   Total_Liquid_Assets: 0.0% missing, 975 unique - Good\n",
      "   Total_Investments: 0.0% missing, 1082 unique - Good\n",
      "   Estimated_Net_Worth: 0.0% missing, 4415 unique - Good\n",
      "   Investment_Ratio: 0.0% missing, 1397 unique - Good\n",
      "   RiskProfile: 89.8% missing, 5 unique - Poor\n",
      "   CKAProfile: 92.6% missing, 2 unique - Poor\n",
      "   CARProfile: 92.6% missing, 2 unique - Poor\n",
      "   Life_Stage: 0.0% missing, 6 unique - Good\n",
      "   Financial_Sophistication: 0.0% missing, 3 unique - Good\n",
      "   Target_SubCategory: 0.0% missing, 18 unique - Good\n",
      "   AIProduct: 0.0% missing, 2 unique - Good\n",
      "\n",
      "=== STEP 8: CREATE BALANCED SAMPLE FOR DEVELOPMENT ===\n",
      "🔧 CREATING BALANCED SAMPLE (max 1000 per subcategory):\n",
      "   Whole_Life_Income: 299 samples\n",
      "   UT: 1,000 samples\n",
      "   SHIELD: 1,000 samples\n",
      "   Term: 1,000 samples\n",
      "   Whole_Life: 1,000 samples\n",
      "   Add_Rider_to_existing_shield_plans: 787 samples\n",
      "   Universal_Life_Protection: 172 samples\n",
      "   ENDOW_NP: 1,000 samples\n",
      "   Investment_Linked_Plan___Accumulation: 1,000 samples\n",
      "   Endowment: 663 samples\n",
      "   Long_Term_Care_Plans: 1,000 samples\n",
      "   Policy_Servicing_Non_Shield_Plans: 768 samples\n",
      "   Retirement: 1,000 samples\n",
      "   Critical_Illness_Plans: 1,000 samples\n",
      "   Accident_and_Health_Plans: 1,000 samples\n",
      "   ETF: 115 samples\n",
      "   Investment_Linked_Policy___Protection: 136 samples\n",
      "   Discretionary_Managed_Account: 74 samples\n",
      "\n",
      "=== STEP 9: SAVE ML TRAINING DATASETS ===\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 292\u001b[39m\n\u001b[32m    289\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m=== STEP 9: SAVE ML TRAINING DATASETS ===\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    291\u001b[39m \u001b[38;5;66;03m# Dataset 1: Full subcategory dataset (real distribution)\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m292\u001b[39m \u001b[43mfinal_ml_dataset\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto_excel\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mML_TRAINING_SUBCATEGORIES_FULL.xlsx\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m    293\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m💾 SAVED: ML_TRAINING_SUBCATEGORIES_FULL.xlsx\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    294\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m   Records: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(final_ml_dataset)\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m,\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\_vico\\source\\repos\\synergysg\\OCC\\.venv\\Lib\\site-packages\\pandas\\util\\_decorators.py:333\u001b[39m, in \u001b[36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    327\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(args) > num_allow_args:\n\u001b[32m    328\u001b[39m     warnings.warn(\n\u001b[32m    329\u001b[39m         msg.format(arguments=_format_argument_list(allow_args)),\n\u001b[32m    330\u001b[39m         \u001b[38;5;167;01mFutureWarning\u001b[39;00m,\n\u001b[32m    331\u001b[39m         stacklevel=find_stack_level(),\n\u001b[32m    332\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m333\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\_vico\\source\\repos\\synergysg\\OCC\\.venv\\Lib\\site-packages\\pandas\\core\\generic.py:2436\u001b[39m, in \u001b[36mNDFrame.to_excel\u001b[39m\u001b[34m(self, excel_writer, sheet_name, na_rep, float_format, columns, header, index, index_label, startrow, startcol, engine, merge_cells, inf_rep, freeze_panes, storage_options, engine_kwargs)\u001b[39m\n\u001b[32m   2423\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mio\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mformats\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mexcel\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m ExcelFormatter\n\u001b[32m   2425\u001b[39m formatter = ExcelFormatter(\n\u001b[32m   2426\u001b[39m     df,\n\u001b[32m   2427\u001b[39m     na_rep=na_rep,\n\u001b[32m   (...)\u001b[39m\u001b[32m   2434\u001b[39m     inf_rep=inf_rep,\n\u001b[32m   2435\u001b[39m )\n\u001b[32m-> \u001b[39m\u001b[32m2436\u001b[39m \u001b[43mformatter\u001b[49m\u001b[43m.\u001b[49m\u001b[43mwrite\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2437\u001b[39m \u001b[43m    \u001b[49m\u001b[43mexcel_writer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2438\u001b[39m \u001b[43m    \u001b[49m\u001b[43msheet_name\u001b[49m\u001b[43m=\u001b[49m\u001b[43msheet_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2439\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstartrow\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstartrow\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2440\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstartcol\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstartcol\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2441\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfreeze_panes\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfreeze_panes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2442\u001b[39m \u001b[43m    \u001b[49m\u001b[43mengine\u001b[49m\u001b[43m=\u001b[49m\u001b[43mengine\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2443\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2444\u001b[39m \u001b[43m    \u001b[49m\u001b[43mengine_kwargs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mengine_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2445\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\_vico\\source\\repos\\synergysg\\OCC\\.venv\\Lib\\site-packages\\pandas\\io\\formats\\excel.py:962\u001b[39m, in \u001b[36mExcelFormatter.write\u001b[39m\u001b[34m(self, writer, sheet_name, startrow, startcol, freeze_panes, engine, storage_options, engine_kwargs)\u001b[39m\n\u001b[32m    959\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    960\u001b[39m     \u001b[38;5;66;03m# make sure to close opened file handles\u001b[39;00m\n\u001b[32m    961\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m need_save:\n\u001b[32m--> \u001b[39m\u001b[32m962\u001b[39m         \u001b[43mwriter\u001b[49m\u001b[43m.\u001b[49m\u001b[43mclose\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\_vico\\source\\repos\\synergysg\\OCC\\.venv\\Lib\\site-packages\\pandas\\io\\excel\\_base.py:1357\u001b[39m, in \u001b[36mExcelWriter.close\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1355\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mclose\u001b[39m(\u001b[38;5;28mself\u001b[39m) -> \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   1356\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"synonym for save, to make it more file-like\"\"\"\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1357\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_save\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1358\u001b[39m     \u001b[38;5;28mself\u001b[39m._handles.close()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\_vico\\source\\repos\\synergysg\\OCC\\.venv\\Lib\\site-packages\\pandas\\io\\excel\\_openpyxl.py:110\u001b[39m, in \u001b[36mOpenpyxlWriter._save\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    106\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_save\u001b[39m(\u001b[38;5;28mself\u001b[39m) -> \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    107\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    108\u001b[39m \u001b[33;03m    Save workbook to disk.\u001b[39;00m\n\u001b[32m    109\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m110\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbook\u001b[49m\u001b[43m.\u001b[49m\u001b[43msave\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_handles\u001b[49m\u001b[43m.\u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    111\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mr+\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m._mode \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m._handles.handle, mmap.mmap):\n\u001b[32m    112\u001b[39m         \u001b[38;5;66;03m# truncate file to the written content\u001b[39;00m\n\u001b[32m    113\u001b[39m         \u001b[38;5;28mself\u001b[39m._handles.handle.truncate()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\_vico\\source\\repos\\synergysg\\OCC\\.venv\\Lib\\site-packages\\openpyxl\\workbook\\workbook.py:386\u001b[39m, in \u001b[36mWorkbook.save\u001b[39m\u001b[34m(self, filename)\u001b[39m\n\u001b[32m    384\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.write_only \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m.worksheets:\n\u001b[32m    385\u001b[39m     \u001b[38;5;28mself\u001b[39m.create_sheet()\n\u001b[32m--> \u001b[39m\u001b[32m386\u001b[39m \u001b[43msave_workbook\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\_vico\\source\\repos\\synergysg\\OCC\\.venv\\Lib\\site-packages\\openpyxl\\writer\\excel.py:294\u001b[39m, in \u001b[36msave_workbook\u001b[39m\u001b[34m(workbook, filename)\u001b[39m\n\u001b[32m    292\u001b[39m workbook.properties.modified = datetime.datetime.now(tz=datetime.timezone.utc).replace(tzinfo=\u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[32m    293\u001b[39m writer = ExcelWriter(workbook, archive)\n\u001b[32m--> \u001b[39m\u001b[32m294\u001b[39m \u001b[43mwriter\u001b[49m\u001b[43m.\u001b[49m\u001b[43msave\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    295\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\_vico\\source\\repos\\synergysg\\OCC\\.venv\\Lib\\site-packages\\openpyxl\\writer\\excel.py:275\u001b[39m, in \u001b[36mExcelWriter.save\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    273\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34msave\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    274\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Write data into the archive.\"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m275\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mwrite_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    276\u001b[39m     \u001b[38;5;28mself\u001b[39m._archive.close()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\_vico\\source\\repos\\synergysg\\OCC\\.venv\\Lib\\site-packages\\openpyxl\\writer\\excel.py:77\u001b[39m, in \u001b[36mExcelWriter.write_data\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m     74\u001b[39m     custom_override = CustomOverride()\n\u001b[32m     75\u001b[39m     \u001b[38;5;28mself\u001b[39m.manifest.append(custom_override)\n\u001b[32m---> \u001b[39m\u001b[32m77\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_write_worksheets\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     78\u001b[39m \u001b[38;5;28mself\u001b[39m._write_chartsheets()\n\u001b[32m     79\u001b[39m \u001b[38;5;28mself\u001b[39m._write_images()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\_vico\\source\\repos\\synergysg\\OCC\\.venv\\Lib\\site-packages\\openpyxl\\writer\\excel.py:215\u001b[39m, in \u001b[36mExcelWriter._write_worksheets\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    212\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m idx, ws \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[38;5;28mself\u001b[39m.workbook.worksheets, \u001b[32m1\u001b[39m):\n\u001b[32m    214\u001b[39m     ws._id = idx\n\u001b[32m--> \u001b[39m\u001b[32m215\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mwrite_worksheet\u001b[49m\u001b[43m(\u001b[49m\u001b[43mws\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    217\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m ws._drawing:\n\u001b[32m    218\u001b[39m         \u001b[38;5;28mself\u001b[39m._write_drawing(ws._drawing)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\_vico\\source\\repos\\synergysg\\OCC\\.venv\\Lib\\site-packages\\openpyxl\\writer\\excel.py:200\u001b[39m, in \u001b[36mExcelWriter.write_worksheet\u001b[39m\u001b[34m(self, ws)\u001b[39m\n\u001b[32m    198\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    199\u001b[39m     writer = WorksheetWriter(ws)\n\u001b[32m--> \u001b[39m\u001b[32m200\u001b[39m     \u001b[43mwriter\u001b[49m\u001b[43m.\u001b[49m\u001b[43mwrite\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    202\u001b[39m ws._rels = writer._rels\n\u001b[32m    203\u001b[39m \u001b[38;5;28mself\u001b[39m._archive.write(writer.out, ws.path[\u001b[32m1\u001b[39m:])\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\_vico\\source\\repos\\synergysg\\OCC\\.venv\\Lib\\site-packages\\openpyxl\\worksheet\\_writer.py:359\u001b[39m, in \u001b[36mWorksheetWriter.write\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    355\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    356\u001b[39m \u001b[33;03mHigh level\u001b[39;00m\n\u001b[32m    357\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    358\u001b[39m \u001b[38;5;28mself\u001b[39m.write_top()\n\u001b[32m--> \u001b[39m\u001b[32m359\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mwrite_rows\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    360\u001b[39m \u001b[38;5;28mself\u001b[39m.write_tail()\n\u001b[32m    361\u001b[39m \u001b[38;5;28mself\u001b[39m.close()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\_vico\\source\\repos\\synergysg\\OCC\\.venv\\Lib\\site-packages\\openpyxl\\worksheet\\_writer.py:125\u001b[39m, in \u001b[36mWorksheetWriter.write_rows\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    123\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m xf.element(\u001b[33m\"\u001b[39m\u001b[33msheetData\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m    124\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m row_idx, row \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.rows():\n\u001b[32m--> \u001b[39m\u001b[32m125\u001b[39m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mwrite_row\u001b[49m\u001b[43m(\u001b[49m\u001b[43mxf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrow\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrow_idx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    127\u001b[39m \u001b[38;5;28mself\u001b[39m.xf.send(\u001b[38;5;28;01mNone\u001b[39;00m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\_vico\\source\\repos\\synergysg\\OCC\\.venv\\Lib\\site-packages\\openpyxl\\worksheet\\_writer.py:147\u001b[39m, in \u001b[36mWorksheetWriter.write_row\u001b[39m\u001b[34m(self, xf, row, row_idx)\u001b[39m\n\u001b[32m    141\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m    142\u001b[39m     cell._value \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    143\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m cell.has_style\n\u001b[32m    144\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m cell._comment\n\u001b[32m    145\u001b[39m     ):\n\u001b[32m    146\u001b[39m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m147\u001b[39m \u001b[43mwrite_cell\u001b[49m\u001b[43m(\u001b[49m\u001b[43mxf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mws\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcell\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcell\u001b[49m\u001b[43m.\u001b[49m\u001b[43mhas_style\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\_vico\\source\\repos\\synergysg\\OCC\\.venv\\Lib\\site-packages\\openpyxl\\cell\\_writer.py:51\u001b[39m, in \u001b[36metree_write_cell\u001b[39m\u001b[34m(xf, worksheet, cell, styled)\u001b[39m\n\u001b[32m     49\u001b[39m el = Element(\u001b[33m\"\u001b[39m\u001b[33mc\u001b[39m\u001b[33m\"\u001b[39m, attributes)\n\u001b[32m     50\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m value \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m value == \u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m---> \u001b[39m\u001b[32m51\u001b[39m     \u001b[43mxf\u001b[49m\u001b[43m.\u001b[49m\u001b[43mwrite\u001b[49m\u001b[43m(\u001b[49m\u001b[43mel\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     52\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[32m     54\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m cell.data_type == \u001b[33m'\u001b[39m\u001b[33mf\u001b[39m\u001b[33m'\u001b[39m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\_vico\\source\\repos\\synergysg\\OCC\\.venv\\Lib\\site-packages\\et_xmlfile\\xmlfile.py:120\u001b[39m, in \u001b[36m_IncrementalFileWriter.write\u001b[39m\u001b[34m(self, arg)\u001b[39m\n\u001b[32m    118\u001b[39m     default_ns_attr_prefix = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    119\u001b[39m     uri_to_prefix = {}\n\u001b[32m--> \u001b[39m\u001b[32m120\u001b[39m \u001b[43mincremental_tree\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_serialize_ns_xml\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    121\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    122\u001b[39m \u001b[43m    \u001b[49m\u001b[43marg\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    123\u001b[39m \u001b[43m    \u001b[49m\u001b[43mnsmap_scope\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnsmap_scope\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    124\u001b[39m \u001b[43m    \u001b[49m\u001b[43mglobal_nsmap\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mglobal_nsmap\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    125\u001b[39m \u001b[43m    \u001b[49m\u001b[43mshort_empty_elements\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    126\u001b[39m \u001b[43m    \u001b[49m\u001b[43mis_html\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mis_html\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    127\u001b[39m \u001b[43m    \u001b[49m\u001b[43mis_root\u001b[49m\u001b[43m=\u001b[49m\u001b[43mis_root\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    128\u001b[39m \u001b[43m    \u001b[49m\u001b[43muri_to_prefix\u001b[49m\u001b[43m=\u001b[49m\u001b[43muri_to_prefix\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    129\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdefault_ns_attr_prefix\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdefault_ns_attr_prefix\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    130\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\_vico\\source\\repos\\synergysg\\OCC\\.venv\\Lib\\site-packages\\et_xmlfile\\incremental_tree.py:613\u001b[39m, in \u001b[36m_serialize_ns_xml\u001b[39m\u001b[34m(write, elem, nsmap_scope, global_nsmap, short_empty_elements, is_html, is_root, uri_to_prefix, default_ns_attr_prefix, new_nsmap, **kwargs)\u001b[39m\n\u001b[32m    566\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_serialize_ns_xml\u001b[39m(\n\u001b[32m    567\u001b[39m     write,\n\u001b[32m    568\u001b[39m     elem,\n\u001b[32m   (...)\u001b[39m\u001b[32m    577\u001b[39m     **kwargs,\n\u001b[32m    578\u001b[39m ):\n\u001b[32m    579\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Serialize an element or tree using 'write' for output.\u001b[39;00m\n\u001b[32m    580\u001b[39m \n\u001b[32m    581\u001b[39m \u001b[33;03m    Args:\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    605\u001b[39m \u001b[33;03m            New prefix -> uri mapping to be applied to this element.\u001b[39;00m\n\u001b[32m    606\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m    607\u001b[39m     (\n\u001b[32m    608\u001b[39m         tag,\n\u001b[32m    609\u001b[39m         nsmap_scope,\n\u001b[32m    610\u001b[39m         default_ns_attr_prefix,\n\u001b[32m    611\u001b[39m         uri_to_prefix,\n\u001b[32m    612\u001b[39m         next_remains_root,\n\u001b[32m--> \u001b[39m\u001b[32m613\u001b[39m     ) = \u001b[43mwrite_elem_start\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    614\u001b[39m \u001b[43m        \u001b[49m\u001b[43mwrite\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    615\u001b[39m \u001b[43m        \u001b[49m\u001b[43melem\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    616\u001b[39m \u001b[43m        \u001b[49m\u001b[43mnsmap_scope\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    617\u001b[39m \u001b[43m        \u001b[49m\u001b[43mglobal_nsmap\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    618\u001b[39m \u001b[43m        \u001b[49m\u001b[43mshort_empty_elements\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    619\u001b[39m \u001b[43m        \u001b[49m\u001b[43mis_html\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    620\u001b[39m \u001b[43m        \u001b[49m\u001b[43mis_root\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    621\u001b[39m \u001b[43m        \u001b[49m\u001b[43muri_to_prefix\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    622\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdefault_ns_attr_prefix\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    623\u001b[39m \u001b[43m        \u001b[49m\u001b[43mnew_nsmap\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnew_nsmap\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    624\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    625\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m e \u001b[38;5;129;01min\u001b[39;00m elem:\n\u001b[32m    626\u001b[39m         _serialize_ns_xml(\n\u001b[32m    627\u001b[39m             write,\n\u001b[32m    628\u001b[39m             e,\n\u001b[32m   (...)\u001b[39m\u001b[32m    636\u001b[39m             new_nsmap=\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m    637\u001b[39m         )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\_vico\\source\\repos\\synergysg\\OCC\\.venv\\Lib\\site-packages\\et_xmlfile\\incremental_tree.py:539\u001b[39m, in \u001b[36mwrite_elem_start\u001b[39m\u001b[34m(write, elem, nsmap_scope, global_nsmap, short_empty_elements, is_html, is_root, uri_to_prefix, default_ns_attr_prefix, new_nsmap, **kwargs)\u001b[39m\n\u001b[32m    537\u001b[39m         write(\u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m.join([\u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[33m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mk\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m=\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mET._escape_attrib_html(v)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\u001b[33m'\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m item_parts]))\n\u001b[32m    538\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m539\u001b[39m         \u001b[43mwrite\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43mf\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[33;43m \u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mk\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[33;43m=\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mET\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_escape_attrib\u001b[49m\u001b[43m(\u001b[49m\u001b[43mv\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mitem_parts\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    540\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_html:\n\u001b[32m    541\u001b[39m     write(\u001b[33m\"\u001b[39m\u001b[33m>\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "print(\"=== PHASE 2: TARGET VARIABLE ENGINEERING ===\")\n",
    "print(\"Goal: Join client features with product recommendation data to create ML training labels\")\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "\n",
    "# Step 1: Load Product and Recommendation Tables\n",
    "print(\"\\n=== STEP 1: LOAD PRODUCT AND RECOMMENDATION TABLES ===\")\n",
    "\n",
    "# Load all product tables\n",
    "product_datasets = {}\n",
    "product_tables = {\n",
    "    'emfc2productsolution': 'mcZ4KQ$&d*?u',\n",
    "    'ProductMainPlan': ')XQ4ZDssowrA', \n",
    "    'ProductType': '#9zCw?^-xTO?',\n",
    "    'ProductCategory': '#F)cdAEOVJ@4',\n",
    "    'productsubcategory': 'y-^t$N9>%S%C'\n",
    "}\n",
    "\n",
    "for table_name, password in product_tables.items():\n",
    "    product_datasets[table_name] = load_encrypted_excel(f\"{table_name}.xlsx\", password)\n",
    "    print(f\"✅ Loaded {table_name}: {len(product_datasets[table_name]):,} records\")\n",
    "\n",
    "# Step 2: Build Complete Product Hierarchy\n",
    "print(\"\\n=== STEP 2: BUILD COMPLETE PRODUCT HIERARCHY ===\")\n",
    "\n",
    "# Step 2.1: ProductMainPlan → ProductSubCategory\n",
    "main_plans = product_datasets['ProductMainPlan']\n",
    "subcategories = product_datasets['productsubcategory']\n",
    "\n",
    "products_with_subcat = main_plans.merge(\n",
    "    subcategories[['ProductSubCategoryId', 'SubCategoryName', 'ProductCategoryId']], \n",
    "    on='ProductSubCategoryId', \n",
    "    how='left'\n",
    ")\n",
    "\n",
    "# Step 2.2: Add ProductCategory \n",
    "categories = product_datasets['ProductCategory']\n",
    "products_with_category = products_with_subcat.merge(\n",
    "    categories[['ProductCategoryId', 'CategoryName', 'ProductTypeId']], \n",
    "    on='ProductCategoryId', \n",
    "    how='left'\n",
    ")\n",
    "\n",
    "# Step 2.3: Add ProductType (final hierarchy level)\n",
    "product_types = product_datasets['ProductType']\n",
    "complete_product_hierarchy = products_with_category.merge(\n",
    "    product_types[['ProductTypeId', 'TypeName', 'InvestmentType']], \n",
    "    on='ProductTypeId', \n",
    "    how='left'\n",
    ")\n",
    "\n",
    "print(f\"✅ Complete product hierarchy: {len(complete_product_hierarchy):,} products\")\n",
    "print(f\"   Product levels: ProductId → SubCategory → Category → Type\")\n",
    "\n",
    "# Show the hierarchy structure\n",
    "hierarchy_sample = complete_product_hierarchy[\n",
    "    ['ProductId', 'MainPlan', 'SubCategoryName', 'CategoryName', 'TypeName']\n",
    "].head(10)\n",
    "\n",
    "print(f\"\\n📋 PRODUCT HIERARCHY SAMPLE:\")\n",
    "print(hierarchy_sample.to_string(index=False))\n",
    "\n",
    "# Step 3: Analyze Target Variables (Product Categories and SubCategories)\n",
    "print(\"\\n=== STEP 3: ANALYZE TARGET VARIABLES ===\")\n",
    "\n",
    "# Analyze what categories are being recommended\n",
    "recommendations = product_datasets['emfc2productsolution']\n",
    "\n",
    "# Join recommendations with complete product hierarchy\n",
    "recommendations_with_products = recommendations.merge(\n",
    "    complete_product_hierarchy[['ProductId', 'SubCategoryName', 'CategoryName', 'TypeName']], \n",
    "    on='ProductId', \n",
    "    how='left'\n",
    ")\n",
    "\n",
    "print(f\"📊 PRODUCT RECOMMENDATION ANALYSIS:\")\n",
    "print(f\"   Total recommendations: {len(recommendations_with_products):,}\")\n",
    "print(f\"   Recommendations with product info: {recommendations_with_products['CategoryName'].notna().sum():,}\")\n",
    "\n",
    "# Target Variable Option 1: Product Type (Highest level)\n",
    "print(f\"\\n🎯 TARGET OPTION 1: PRODUCT TYPE\")\n",
    "type_distribution = recommendations_with_products['TypeName'].value_counts()\n",
    "print(\"Product Type distribution:\")\n",
    "for prod_type, count in type_distribution.items():\n",
    "    pct = (count / len(recommendations_with_products)) * 100\n",
    "    print(f\"   {prod_type}: {count:,} ({pct:.1f}%)\")\n",
    "\n",
    "# Target Variable Option 2: Product Category (Mid level)\n",
    "print(f\"\\n🎯 TARGET OPTION 2: PRODUCT CATEGORY\")\n",
    "category_distribution = recommendations_with_products['CategoryName'].value_counts()\n",
    "print(\"Product Category distribution:\")\n",
    "for category, count in category_distribution.head(10).items():\n",
    "    pct = (count / len(recommendations_with_products)) * 100\n",
    "    print(f\"   {category}: {count:,} ({pct:.1f}%)\")\n",
    "\n",
    "# Target Variable Option 3: Product SubCategory (Detailed level - OUR TARGET)\n",
    "print(f\"\\n🎯 TARGET OPTION 3: PRODUCT SUBCATEGORY (SELECTED TARGET)\")\n",
    "subcat_distribution = recommendations_with_products['SubCategoryName'].value_counts()\n",
    "print(\"Product SubCategory distribution (top 15):\")\n",
    "for subcat, count in subcat_distribution.head(15).items():\n",
    "    pct = (count / len(recommendations_with_products)) * 100\n",
    "    print(f\"   {subcat}: {count:,} ({pct:.1f}%)\")\n",
    "\n",
    "print(f\"\\nTotal subcategories available: {subcat_distribution.nunique()}\")\n",
    "\n",
    "# Step 4: Create ML Training Dataset\n",
    "print(\"\\n=== STEP 4: CREATE ML TRAINING DATASET ===\")\n",
    "\n",
    "# Load client features from Week 1\n",
    "clients_df = pd.read_excel('ML_READY_client_dataset.xlsx')\n",
    "print(f\"✅ Loaded client features: {len(clients_df):,} unique clients\")\n",
    "\n",
    "# Join clients with recommendations and product SUBCATEGORIES\n",
    "ml_training_data = clients_df.merge(\n",
    "    recommendations_with_products, \n",
    "    on='PersonalInformationId', \n",
    "    how='inner'\n",
    ")\n",
    "\n",
    "print(f\"🔗 JOINING CLIENTS WITH RECOMMENDATIONS:\")\n",
    "print(f\"   Clients with recommendations: {ml_training_data['ClientId'].nunique():,}\")\n",
    "print(f\"   Total training records: {len(ml_training_data):,}\")\n",
    "\n",
    "# Step 5: Prepare Target Variable (SUBCATEGORIES)\n",
    "print(f\"\\n=== STEP 5: PREPARE TARGET VARIABLE (SUBCATEGORIES) ===\")\n",
    "\n",
    "# Remove records without subcategory information\n",
    "ml_training_data = ml_training_data[ml_training_data['SubCategoryName'].notna()]\n",
    "print(f\"   Records with product subcategory: {len(ml_training_data):,}\")\n",
    "\n",
    "# Create clean subcategory names for ML\n",
    "def clean_subcategory_name(subcategory):\n",
    "    if pd.isna(subcategory):\n",
    "        return 'Unknown'\n",
    "    \n",
    "    # Clean and standardize subcategory names\n",
    "    subcategory = str(subcategory).strip()\n",
    "    \n",
    "    # Replace spaces and special characters with underscores for ML compatibility\n",
    "    subcategory = (subcategory.replace(' ', '_')\n",
    "                             .replace('-', '_')\n",
    "                             .replace('(', '')\n",
    "                             .replace(')', '')\n",
    "                             .replace('/', '_')\n",
    "                             .replace('&', 'and')\n",
    "                             .replace(',', ''))\n",
    "    \n",
    "    return subcategory\n",
    "\n",
    "ml_training_data['Target_SubCategory'] = ml_training_data['SubCategoryName'].apply(clean_subcategory_name)\n",
    "\n",
    "# Show final target distribution (SUBCATEGORIES)\n",
    "final_target_dist = ml_training_data['Target_SubCategory'].value_counts()\n",
    "print(f\"\\n📊 FINAL TARGET VARIABLE DISTRIBUTION (SUBCATEGORIES):\")\n",
    "for subcategory, count in final_target_dist.head(15).items():  # Show top 15\n",
    "    pct = (count / len(ml_training_data)) * 100\n",
    "    print(f\"   {subcategory}: {count:,} ({pct:.1f}%)\")\n",
    "\n",
    "print(f\"\\nTotal subcategories: {final_target_dist.nunique()}\")\n",
    "\n",
    "# Check for class imbalance\n",
    "min_class_size = final_target_dist.min()\n",
    "max_class_size = final_target_dist.max()\n",
    "imbalance_ratio = max_class_size / min_class_size\n",
    "\n",
    "print(f\"\\n⚖️  CLASS BALANCE ANALYSIS (SUBCATEGORIES):\")\n",
    "print(f\"   Smallest class: {min_class_size:,} samples\")\n",
    "print(f\"   Largest class: {max_class_size:,} samples\") \n",
    "print(f\"   Imbalance ratio: {imbalance_ratio:.1f}:1\")\n",
    "print(f\"   Number of classes: {final_target_dist.nunique()}\")\n",
    "\n",
    "# Analyze small subcategories\n",
    "small_classes = final_target_dist[final_target_dist < 100]  # Classes with <100 samples\n",
    "print(f\"\\n📊 SMALL SUBCATEGORIES (<100 samples): {len(small_classes)}\")\n",
    "if len(small_classes) > 0:\n",
    "    print(\"Small subcategories:\")\n",
    "    for subcat, count in small_classes.items():\n",
    "        print(f\"   {subcat}: {count}\")\n",
    "\n",
    "# Step 6: Handle Small Subcategories\n",
    "print(f\"\\n=== STEP 6: HANDLE SMALL SUBCATEGORIES ===\")\n",
    "\n",
    "min_samples_threshold = 50  # Minimum samples per subcategory for stable ML training\n",
    "\n",
    "subcategory_counts = ml_training_data['Target_SubCategory'].value_counts()\n",
    "small_subcategories = subcategory_counts[subcategory_counts < min_samples_threshold]\n",
    "\n",
    "print(f\"🔧 SUBCATEGORIES WITH <{min_samples_threshold} SAMPLES:\")\n",
    "print(f\"   Count: {len(small_subcategories)}\")\n",
    "\n",
    "if len(small_subcategories) > 0:\n",
    "    print(\"   Small subcategories to remove:\")\n",
    "    for subcat, count in small_subcategories.items():\n",
    "        print(f\"     {subcat}: {count} samples\")\n",
    "    \n",
    "    # Remove small subcategories for model stability\n",
    "    ml_training_clean = ml_training_data[\n",
    "        ~ml_training_data['Target_SubCategory'].isin(small_subcategories.index)\n",
    "    ].copy()\n",
    "    \n",
    "    print(f\"\\n   After removing small subcategories:\")\n",
    "    print(f\"     Original records: {len(ml_training_data):,}\")\n",
    "    print(f\"     Cleaned records: {len(ml_training_clean):,}\")\n",
    "    print(f\"     Remaining subcategories: {ml_training_clean['Target_SubCategory'].nunique()}\")\n",
    "    \n",
    "    # Update distribution\n",
    "    final_target_dist = ml_training_clean['Target_SubCategory'].value_counts()\n",
    "else:\n",
    "    ml_training_clean = ml_training_data.copy()\n",
    "    print(f\"   ✅ All subcategories have sufficient samples\")\n",
    "\n",
    "# Step 7: Prepare Final ML Features\n",
    "print(f\"\\n=== STEP 7: PREPARE FINAL ML FEATURES ===\")\n",
    "\n",
    "ml_features = [\n",
    "    # Client demographics\n",
    "    'ClientAge', 'Age_Group', 'ClientGender', 'Nationality', 'MaritalStatus',\n",
    "    # Socioeconomic\n",
    "    'IncomeRange', 'Income_Numeric', 'Income_Category', 'Education', 'EmploymentStatus',\n",
    "    # Financial profile\n",
    "    'SavingsAccounts', 'HomeAsset', 'Total_Liquid_Assets', 'Total_Investments', \n",
    "    'Estimated_Net_Worth', 'Investment_Ratio',\n",
    "    # Risk assessment\n",
    "    'RiskProfile', 'CKAProfile', 'CARProfile',\n",
    "    # Derived features\n",
    "    'Life_Stage', 'Financial_Sophistication',\n",
    "    # Target variable (SUBCATEGORIES)\n",
    "    'Target_SubCategory',\n",
    "    # Additional context\n",
    "    'AIProduct', 'DateCreated'\n",
    "]\n",
    "\n",
    "# Filter to available features\n",
    "available_features = [col for col in ml_features if col in ml_training_clean.columns]\n",
    "final_ml_dataset = ml_training_clean[available_features].copy()\n",
    "\n",
    "print(f\"📊 FEATURE AVAILABILITY:\")\n",
    "print(f\"   Requested features: {len(ml_features)}\")\n",
    "print(f\"   Available features: {len(available_features)}\")\n",
    "\n",
    "missing_features = [col for col in ml_features if col not in ml_training_clean.columns]\n",
    "if missing_features:\n",
    "    print(f\"   Missing features: {missing_features}\")\n",
    "\n",
    "# Feature quality check\n",
    "print(f\"\\n🔍 FEATURE QUALITY CHECK:\")\n",
    "feature_quality_report = []\n",
    "\n",
    "for feature in available_features[:-1]:  # Exclude target variable\n",
    "    missing_pct = (final_ml_dataset[feature].isnull().sum() / len(final_ml_dataset)) * 100\n",
    "    unique_count = final_ml_dataset[feature].nunique()\n",
    "    \n",
    "    status = \"Good\" if missing_pct < 20 else \"Review\" if missing_pct < 50 else \"Poor\"\n",
    "    \n",
    "    feature_quality_report.append({\n",
    "        'Feature': feature,\n",
    "        'Missing_%': round(missing_pct, 1),\n",
    "        'Unique_Values': unique_count,\n",
    "        'Status': status\n",
    "    })\n",
    "    \n",
    "    print(f\"   {feature}: {missing_pct:.1f}% missing, {unique_count} unique - {status}\")\n",
    "\n",
    "# Step 8: Create Balanced Sample for Development\n",
    "print(f\"\\n=== STEP 8: CREATE BALANCED SAMPLE FOR DEVELOPMENT ===\")\n",
    "\n",
    "# Sample strategy: Take up to 1000 samples per subcategory for balanced training\n",
    "max_samples_per_class = 1000\n",
    "balanced_samples = []\n",
    "\n",
    "print(f\"🔧 CREATING BALANCED SAMPLE (max {max_samples_per_class} per subcategory):\")\n",
    "\n",
    "for subcategory in final_ml_dataset['Target_SubCategory'].unique():\n",
    "    subcat_data = final_ml_dataset[final_ml_dataset['Target_SubCategory'] == subcategory]\n",
    "    \n",
    "    if len(subcat_data) > max_samples_per_class:\n",
    "        sampled = subcat_data.sample(n=max_samples_per_class, random_state=42)\n",
    "    else:\n",
    "        sampled = subcat_data.copy()\n",
    "    \n",
    "    balanced_samples.append(sampled)\n",
    "    print(f\"   {subcategory}: {len(sampled):,} samples\")\n",
    "\n",
    "balanced_training_data = pd.concat(balanced_samples, ignore_index=True)\n",
    "\n",
    "# Step 9: Save ML Training Datasets\n",
    "print(f\"\\n=== STEP 9: SAVE ML TRAINING DATASETS ===\")\n",
    "\n",
    "# Dataset 1: Full subcategory dataset (real distribution)\n",
    "final_ml_dataset.to_excel('ML_TRAINING_SUBCATEGORIES_FULL.xlsx', index=False)\n",
    "print(f\"💾 SAVED: ML_TRAINING_SUBCATEGORIES_FULL.xlsx\")\n",
    "print(f\"   Records: {len(final_ml_dataset):,}\")\n",
    "print(f\"   Subcategories: {final_ml_dataset['Target_SubCategory'].nunique()}\")\n",
    "print(f\"   Features: {len(available_features) - 1}\")  # Exclude target\n",
    "\n",
    "# Dataset 2: Balanced subcategory dataset (for development)\n",
    "balanced_training_data.to_excel('ML_TRAINING_SUBCATEGORIES_BALANCED.xlsx', index=False)\n",
    "print(f\"💾 SAVED: ML_TRAINING_SUBCATEGORIES_BALANCED.xlsx\") \n",
    "print(f\"   Records: {len(balanced_training_data):,}\")\n",
    "print(f\"   Subcategories: {balanced_training_data['Target_SubCategory'].nunique()}\")\n",
    "print(f\"   Max samples per class: {max_samples_per_class}\")\n",
    "\n",
    "# Step 10: Create Summary Report\n",
    "print(f\"\\n=== STEP 10: CREATE SUMMARY REPORT ===\")\n",
    "\n",
    "subcategory_dist = final_ml_dataset['Target_SubCategory'].value_counts()\n",
    "\n",
    "summary_stats = {\n",
    "    'ML_Problem_Type': 'Multi-class Classification (Subcategory Prediction)',\n",
    "    'Business_Objective': 'Predict top 1-3 subcategories with confidence scores',\n",
    "    'Total_Records': len(final_ml_dataset),\n",
    "    'Unique_Clients': final_ml_dataset['ClientId'].nunique() if 'ClientId' in final_ml_dataset.columns else 'N/A',\n",
    "    'Subcategories_Count': subcategory_dist.nunique(),\n",
    "    'Top_Subcategory': f\"{subcategory_dist.index[0]} ({subcategory_dist.iloc[0]:,} samples, {subcategory_dist.iloc[0]/len(final_ml_dataset)*100:.1f}%)\",\n",
    "    'Features_Count': len(available_features) - 1,\n",
    "    'Imbalance_Ratio': f\"{subcategory_dist.max() / subcategory_dist.min():.0f}:1\",\n",
    "    'Model_Output': 'Top 1-3 subcategories with confidence scores',\n",
    "    'Business_Use_Case': 'Starting point for FNA discussions'\n",
    "}\n",
    "\n",
    "print(f\"📋 FINAL DATASET SUMMARY:\")\n",
    "for key, value in summary_stats.items():\n",
    "    print(f\"   {key}: {value}\")\n",
    "\n",
    "# Save comprehensive summary\n",
    "with open('ML_TRAINING_SUMMARY.txt', 'w') as f:\n",
    "    f.write(\"=== PHASE 2: ML TRAINING DATASET SUMMARY ===\\n\")\n",
    "    f.write(f\"Generated: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\\n\\n\")\n",
    "    \n",
    "    f.write(\"MODEL SPECIFICATION:\\n\")\n",
    "    f.write(\"Problem Type: Multi-class Classification\\n\")\n",
    "    f.write(\"Target Variable: Product Subcategories\\n\")\n",
    "    f.write(\"Model Output: Top 1-3 subcategories with confidence scores\\n\")\n",
    "    f.write(\"Business Use: Starting point for FNA discussions\\n\")\n",
    "    f.write(\"Prediction Behavior: Always give prediction (even if low confidence)\\n\\n\")\n",
    "    \n",
    "    f.write(\"DATASET STATISTICS:\\n\")\n",
    "    for key, value in summary_stats.items():\n",
    "        f.write(f\"{key}: {value}\\n\")\n",
    "    \n",
    "    f.write(f\"\\nSUBCATEGORY DISTRIBUTION (TOP 15):\\n\")\n",
    "    for i, (subcat, count) in enumerate(subcategory_dist.head(15).items(), 1):\n",
    "        pct = (count / len(final_ml_dataset)) * 100\n",
    "        f.write(f\"{i:2d}. {subcat}: {count:,} ({pct:.1f}%)\\n\")\n",
    "    \n",
    "    f.write(f\"\\nFEATURE QUALITY SUMMARY:\\n\")\n",
    "    good_features = [f['Feature'] for f in feature_quality_report if f['Status'] == 'Good']\n",
    "    review_features = [f['Feature'] for f in feature_quality_report if f['Status'] == 'Review']\n",
    "    poor_features = [f['Feature'] for f in feature_quality_report if f['Status'] == 'Poor']\n",
    "    \n",
    "    f.write(f\"Good Quality Features ({len(good_features)}): {', '.join(good_features)}\\n\")\n",
    "    if review_features:\n",
    "        f.write(f\"Review Quality Features ({len(review_features)}): {', '.join(review_features)}\\n\")\n",
    "    if poor_features:\n",
    "        f.write(f\"Poor Quality Features ({len(poor_features)}): {', '.join(poor_features)}\\n\")\n",
    "\n",
    "print(f\"💾 SAVED: ML_TRAINING_SUMMARY.txt\")\n",
    "\n",
    "print(f\"\\n🎉 PHASE 2 COMPLETE!\")\n",
    "print(f\"✅ Target variable: Product subcategories ({subcategory_dist.nunique()} classes)\")\n",
    "print(f\"✅ Training datasets: Full + Balanced versions created\")\n",
    "print(f\"✅ Feature candidates: {len(available_features) - 1} potential features identified\")\n",
    "print(f\"✅ Data quality: Assessed and documented\")\n",
    "print(f\"✅ Class imbalance: Analyzed and addressed with balanced sampling\")\n",
    "print(f\"✅ Ready for Phase 3: Feature selection and model development\")\n",
    "\n",
    "print(f\"\\n📁 FILES CREATED:\")\n",
    "print(f\"   • ML_TRAINING_SUBCATEGORIES_FULL.xlsx (real distribution)\")\n",
    "print(f\"   • ML_TRAINING_SUBCATEGORIES_BALANCED.xlsx (development)\")\n",
    "print(f\"   • ML_TRAINING_SUMMARY.txt (documentation)\")\n",
    "\n",
    "print(f\"\\n🎯 NEXT PHASE:\")\n",
    "print(f\"   Phase 3: Feature selection + Model training + Evaluation\")\n",
    "print(f\"   Goal: Build model that predicts top 1-3 subcategories with confidence\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b88668b-6b88-4698-8991-a7ddf847bab1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== ANALYZING TARGET VARIABLES (PRODUCT CATEGORIES) ===\n",
      "📊 PRODUCT RECOMMENDATION ANALYSIS:\n",
      "   Total recommendations: 61,733\n",
      "   Recommendations with product info: 61,732\n",
      "\n",
      "🎯 TARGET OPTION 1: PRODUCT TYPE\n",
      "Product Type distribution:\n",
      "   Life Insurance: 52,272 (84.7%)\n",
      "   Wealth Management: 9,460 (15.3%)\n",
      "\n",
      "🎯 TARGET OPTION 2: PRODUCT CATEGORY\n",
      "Product Category distribution:\n",
      "   Wealth Protection: 43,941 (71.2%)\n",
      "   Invest: 9,460 (15.3%)\n",
      "   Wealth Accumulation: 7,563 (12.3%)\n",
      "   Policy Servicing (Non Shield Plans): 768 (1.2%)\n",
      "\n",
      "🎯 TARGET OPTION 3: PRODUCT SUBCATEGORY\n",
      "Product SubCategory distribution (top 10):\n",
      "   Long Term Care Plans: 15,827 (25.6%)\n",
      "   SHIELD: 12,640 (20.5%)\n",
      "   UT: 9,239 (15.0%)\n",
      "   Term: 7,080 (11.5%)\n",
      "   Investment Linked Plan - Accumulation: 4,207 (6.8%)\n",
      "   Whole Life: 3,696 (6.0%)\n",
      "   Accident and Health Plans: 1,853 (3.0%)\n",
      "   Critical Illness Plans: 1,744 (2.8%)\n",
      "   ENDOW (NP): 1,373 (2.2%)\n",
      "   Retirement: 1,013 (1.6%)\n",
      "\n",
      "📈 RECOMMENDATION FOR TARGET VARIABLE:\n",
      "   - Use CategoryName (mid-level) for balanced granularity\n",
      "   - 4 categories vs 2 types vs 22 subcategories\n",
      "   - Categories provide meaningful business distinctions without being too granular\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "092cbe5a-53ad-479d-b546-62b34a4a0586",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== CREATING ML TRAINING DATASET (CORRECTED FOR SUBCATEGORIES) ===\n",
      "🔗 JOINING CLIENTS WITH RECOMMENDATIONS:\n",
      "   Clients with recommendations: 44,189\n",
      "   Total training records: 369,054\n",
      "\n",
      "🎯 PREPARING TARGET VARIABLE (SUBCATEGORIES):\n",
      "   Records with product subcategory: 369,053\n",
      "\n",
      "📊 FINAL TARGET VARIABLE DISTRIBUTION (SUBCATEGORIES):\n",
      "   Long_Term_Care_Plans: 118,271 (32.0%)\n",
      "   SHIELD: 89,471 (24.2%)\n",
      "   Term: 58,301 (15.8%)\n",
      "   UT: 34,848 (9.4%)\n",
      "   Investment_Linked_Plan___Accumulation: 29,815 (8.1%)\n",
      "   Whole_Life: 16,500 (4.5%)\n",
      "   Critical_Illness_Plans: 14,550 (3.9%)\n",
      "   Accident_and_Health_Plans: 1,853 (0.5%)\n",
      "   ENDOW_NP: 1,373 (0.4%)\n",
      "   Retirement: 1,013 (0.3%)\n",
      "   Add_Rider_to_existing_shield_plans: 787 (0.2%)\n",
      "   Policy_Servicing_Non_Shield_Plans: 768 (0.2%)\n",
      "   Endowment: 663 (0.2%)\n",
      "   Whole_Life_Income: 299 (0.1%)\n",
      "   Universal_Life_Protection: 172 (0.0%)\n",
      "\n",
      "Total subcategories: 22\n",
      "\n",
      "⚖️  CLASS BALANCE ANALYSIS (SUBCATEGORIES):\n",
      "   Smallest class: 3 samples\n",
      "   Largest class: 118,271 samples\n",
      "   Imbalance ratio: 39423.7:1\n",
      "   Number of classes: 22\n",
      "\n",
      "📊 SMALL SUBCATEGORIES (<100 samples): 5\n",
      "Small subcategories:\n",
      "   Discretionary_Managed_Account: 74\n",
      "   Bond: 32\n",
      "   ILP: 5\n",
      "   Universal_Life_Accumulation: 4\n",
      "   Education_Funding: 3\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "062a3489-ac8e-4177-9b79-ceb17d964542",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== ADDRESSING CLASS IMBALANCE ===\n",
      "Current class distribution:\n",
      "   Wealth_Protection: 288,084 (83.0%)\n",
      "   Wealth_Accumulation: 29,489 (8.5%)\n",
      "   Invest: 28,992 (8.4%)\n",
      "   Policy_Servicing_(Non_Shield_Plans): 423 (0.1%)\n",
      "\n",
      "🔧 STRATEGY 1: Remove extremely small class\n",
      "After removing Policy Servicing:\n",
      "   Wealth_Protection: 288,084 (83.1%)\n",
      "   Wealth_Accumulation: 29,489 (8.5%)\n",
      "   Invest: 28,992 (8.4%)\n",
      "New imbalance ratio: 9.9:1\n",
      "\n",
      "🔧 STRATEGY 2: Create balanced dataset for model development\n",
      "   Wealth_Protection: 50,000 samples\n",
      "   Wealth_Accumulation: 29,489 samples\n",
      "   Invest: 28,992 samples\n",
      "\n",
      "Balanced training dataset: 108,481 total samples\n",
      "Final imbalance ratio: 1.7:1 ✅\n",
      "\n",
      "🔧 STRATEGY 3: Keep full dataset for production testing\n",
      "   Full dataset: 346,565 records (real-world distribution)\n",
      "   Balanced dataset: 108,481 records (for training)\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "833771f4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
